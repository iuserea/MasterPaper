{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "prostate-wellington",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "designed-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package\n",
    "# --begin--\n",
    "# platform\n",
    "platform = 'group'\n",
    "# platform = 'old'\n",
    "\n",
    "# --Import--\n",
    "# Common\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "from math import log, e\n",
    "\n",
    "from datetime import date, timedelta\n",
    "\n",
    "# DataProcessing\n",
    "# file\n",
    "# import openpyxl\n",
    "import xlrd\n",
    "\n",
    "# pyspark\n",
    "# import pyspark\n",
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql import functions as pyspark_f\n",
    "# from pyspark.sql.functions import sum,expr,count,col,lit,isnull,when,countDistinct,rand,asc,array,udf,concat,concat_ws\n",
    "\n",
    "# from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "# from pyspark.sql.types import StringType,IntegerType,FloatType,LongType,ArrayType\n",
    "\n",
    "import shutil\n",
    "from multiprocessing import Pool\n",
    "import xlrd\n",
    "\n",
    "# feat\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder, MinMaxScaler, StandardScaler, MaxAbsScaler,Normalizer\n",
    "\n",
    "# time series anomaly detection\n",
    "# from pylab import rcParams\n",
    "# from PyAstronomy import pyasl\n",
    "# import pyculiarity\n",
    "# from pyculiarity import detect_ts\n",
    "# from pyculiarity.date_utils import format_timestamp\n",
    "# import sesd\n",
    "# from statsmodels.tsa.seasonal import seasonal_decompose,STL\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "# Model\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "# import torch.tensor\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "\n",
    "# model\n",
    "# ml\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,\\\n",
    "    BaggingClassifier,ExtraTreesClassifier,GradientBoostingClassifier,VotingClassifier\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier,LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.cluster import DBSCAN\n",
    "# dl\n",
    "from torch import nn, einsum\n",
    "from random import random,randint\n",
    "from numpy import array,argmax\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import LSTM,Dense,Conv2D,Conv1D,MaxPooling2D,MaxPooling1D,Flatten,TimeDistributed,Dropout\n",
    "# from keras.metrics import accuracy,binary_accuracy\n",
    "\n",
    "# import tensorflow as tf\n",
    "\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# if platform == 'group':\n",
    "#     from einops import rearrange, repeat\n",
    "#     from einops.layers.torch import Rearrange\n",
    "\n",
    "# hyperparameter\n",
    "# import optuna\n",
    "# from optuna.trial import TrialState\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,\\\n",
    "    mean_squared_error,log_loss,classification_report,confusion_matrix,\\\n",
    "    roc_curve,roc_auc_score,average_precision_score,precision_recall_curve\n",
    "\n",
    "#,plot_precision_recall_curve\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cbook as cbook\n",
    "import matplotlib\n",
    "font = { 'size'   : 24}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "import seaborn as sns\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "# import statsmodels.api as ssm\n",
    "# import scipy\n",
    "# from scipy.stats import f\n",
    "# import scipy.stats as stats\n",
    "# # additional packages\n",
    "# from statsmodels.stats.diagnostic import lilliefors\n",
    "\n",
    "# other\n",
    "import gc\n",
    "# import dill\n",
    "from pdb import set_trace as st\n",
    "import profile\n",
    "import random\n",
    "import joblib\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "# --Endings--"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-prince",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "processed-strike",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --Function--\n",
    "# Common\n",
    "def pkl2df(filename,path='default'):\n",
    "    '''\n",
    "    Pkl->DataFrame得分\n",
    "    '''\n",
    "    if path == 'default':\n",
    "        global save_path\n",
    "        path = save_path\n",
    "    file = path + filename + '.pkl'\n",
    "    # print('file'+file)\n",
    "    df = None\n",
    "    try:\n",
    "        df = pickle.load(open(file,'rb'))\n",
    "        print(f'pkl2df:{file} succeeded')\n",
    "    except:\n",
    "        print(f'pkl2df:{file} failed')\n",
    "    return df\n",
    "\n",
    "def xlsx2df(filename,path='default'):\n",
    "    '''\n",
    "    .xlsx->DataFrame\n",
    "    '''\n",
    "    if path == 'default':\n",
    "        global save_path\n",
    "        path = save_path\n",
    "    file = path + filename + '.xlsx'\n",
    "    print(f'filename:{file}')\n",
    "    df = None\n",
    "    try:\n",
    "        df = pd.read_excel(file, engine='openpyxl')\n",
    "        print(f'xlsx2df:{file} succeeded')\n",
    "    except Exception as e:\n",
    "        print(f'xlsx2df:{file} failed')\n",
    "        print(e)\n",
    "    return df    \n",
    "def csv2df(filename,path='default'):\n",
    "    '''\n",
    "    .csv->DataFrame\n",
    "    '''\n",
    "    if path == 'default':\n",
    "        global save_path\n",
    "        path = save_path\n",
    "    file = path + filename + '.csv'\n",
    "    print(f'filename:{file}')\n",
    "    df = None\n",
    "    try:\n",
    "        # df = pd.read_csv(file)\n",
    "        # df = pd.read_csv(file, encoding='utf-8')\n",
    "        # df=pd.read_csv(file,encoding='ISO-8859-1')\n",
    "        df=pd.read_csv(file,encoding='GBK')\n",
    "        # df = pd.read_csv(file, encoding=\"windows_1258\")\n",
    "        print(f'csv2df:{file} succeeded')\n",
    "    except Exception as e:\n",
    "        print(f'csv2df:{file} failed')\n",
    "        print(e)\n",
    "    return df\n",
    "    \n",
    "def check_file(filename=None,path='default',size_min=0):\n",
    "    '''\n",
    "    Check Attribute of File        \n",
    "    查看工作目录下各个文件及大小\n",
    "    '''\n",
    "    if path == 'default':\n",
    "        global save_path\n",
    "        path = save_path\n",
    "        # print(path)\n",
    "    cur_file_list = os.listdir(path)\n",
    "    cur_file_list = sorted(cur_file_list,key= lambda i:i[0])\n",
    "    if filename is None:\n",
    "        for file in cur_file_list:\n",
    "            size = os.path.getsize(path + str(file))\n",
    "            size = size / (2**20)\n",
    "            if size > size_min:\n",
    "                print(file,end ='--')\n",
    "                print(f'{size:.2f} MB')\n",
    "    else:\n",
    "        for file in cur_file_list:\n",
    "            if filename in file:\n",
    "                size = os.path.getsize(path + str(file))\n",
    "                size = size / (2**20)\n",
    "                if size > size_min:\n",
    "                    print(file,end ='--')\n",
    "                    print('%.2f'%size)\n",
    "                    # print(size // (2**20))\n",
    "\n",
    "def check_format(df,mode='detail',feat_list=None,filename='nothing'):\n",
    "    if feat_list is not None:\n",
    "        df = df[feat_list]\n",
    "    if df.empty:\n",
    "        print('the passing df is None')\n",
    "        df = pkl2df(filename)\n",
    "    pd.set_option('display.max_columns', None)#显示所有列\n",
    "    pd.set_option('display.max_rows', None)#显示所有行\n",
    "    pd.set_option('max_colwidth',500)#设置value的显示长度为100，默认为50\n",
    "    \n",
    "    df = pd.DataFrame(df)\n",
    "    print(df.shape) # mode:simple\n",
    "    # print(df.index)\n",
    "    \n",
    "    if mode=='cols':\n",
    "        cols = list(df.columns)\n",
    "        print(cols)\n",
    "    elif mode=='detail':\n",
    "        # print(df.columns)\n",
    "        print(df.info(verbose = True, null_counts = True))\n",
    "        print(df.describe())\n",
    "        # print(df.isnull().sum())\n",
    "    \n",
    "    print('**********************************')\n",
    "\n",
    "def check_detail(df,rows=10,feat_list=None,filename='nothing'):\n",
    "    if feat_list is not None:\n",
    "        df = df[feat_list]\n",
    "    if df.empty:\n",
    "        print('the passing df is None')\n",
    "        df = pkl2df(filename)\n",
    "    pd.set_option('display.max_columns', None)#显示所有列\n",
    "    pd.set_option('display.max_rows', None)#显示所有行\n",
    "    pd.set_option('max_colwidth',500)#设置value的显示长度为100，默认为50\n",
    "    df = pd.DataFrame(df)\n",
    "    print(df.head(rows))\n",
    "    \n",
    "    print('**********************************')\n",
    "\n",
    "def check_all(df,rows=10):\n",
    "    print(\"*******************start check_all**************************\")\n",
    "    check_format(df)\n",
    "    check_detail(df, rows=rows)\n",
    "    \n",
    "def check_module():\n",
    "    import pkgutil\n",
    "    # print(pkgutil.iter_modules())\n",
    "    for x in list(pkgutil.iter_modules()):\n",
    "        print(x[1])\n",
    "    \n",
    "def pandasdf2sparkdf(pandas_df):\n",
    "    '''\n",
    "    PandasDF2SparkDF\n",
    "    '''\n",
    "    # for col in pandas_df.columns:\n",
    "    #     if ((pandas_df[col].dtypes != np.int64) and (pandas_df[col].dtypes != np.float64)):\n",
    "    #         # pandas_df[col] = pandas_df[col].fillna('')\n",
    "    #         pandas_df[col].fillna('',inplace=True)\n",
    "    \n",
    "    # print(pandas_df.info(verbose=True))        \n",
    "    spark_df = spark.createDataFrame(pandas_df)\n",
    "    return spark_df,pandas_df\n",
    "\n",
    "def sparkdf2hive(spark_df,target_table):\n",
    "    spark_df.createOrReplaceTempView('TempTable')\n",
    "    sqlContext.sql(\"insert overwrite table \" + target_table + \" select * from TempTable\")\n",
    "    print('sparkdf2hive success')\n",
    "\n",
    "def simple_parquet2sparkdf(filename, mode='no_check'):\n",
    "    print(f'----------------start reading {filename}----------------')\n",
    "    spark_df = spark.read.format(\"parquet\").option(\"inferSchema\",\"true\").option(\"header\",'true').load(filename)\n",
    "    if mode == 'check':\n",
    "        check_spark_df(spark_df)\n",
    "    print('----------------read successfully-----------')    \n",
    "    return spark_df    \n",
    "    \n",
    "def parquet2sparkdf(user_type, table_type, date, version, mode='no_check'):\n",
    "    \n",
    "    # filename = filename + '.parquet'\n",
    "    filename = get_filename(user_type,table_type,date,version,'.parquet')\n",
    "\t# filename = './qk/' + filename\n",
    "    print(f'----------------start reading {filename}----------------')\n",
    "    spark_df = spark.read.format(\"parquet\").option(\"inferSchema\",\"true\").option(\"header\",'true').load(filename)\n",
    "    if mode == 'check':\n",
    "        check_spark_df(spark_df)\n",
    "    print('----------------read successfully-----------')    \n",
    "    return spark_df\n",
    "\n",
    "def sparkdf2parquet(df, user_type, table_type, date, version):\n",
    "    \n",
    "    # filename = filename + '.parquet'\n",
    "    filename = get_filename(user_type,table_type,date,version,'.parquet')\n",
    "\t# filename = './qk/' + filename\n",
    "    print(f'----------------start writing {filename}----------------')\n",
    "    df.write.format('parquet').mode('overwrite').save(filename)\n",
    "    print('----------------write successfully-----------')\n",
    "    \n",
    "def pandasdf_fillna_number(pandas_df):\n",
    "    for col in pandas_df.columns:\n",
    "        if ((pandas_df[col].dtypes == np.int64) or (pandas_df[col].dtypes == np.float64)):\n",
    "            pandas_df[col].fillna(0,inplace=True)\n",
    "    return pandas_df\n",
    "\n",
    "def set_deal_filename(file_selection,file_suffix='.parquet'):\n",
    "    deal_filename = file_selection + file_suffix\n",
    "    deal_filename_backup = deal_filename[:-len(file_suffix)] + '_backup' + file_suffix\n",
    "    return deal_filename,deal_filename_backup\n",
    "    \n",
    "# DataProcessing\n",
    "def feats2cols(feat_list):\n",
    "    '''\n",
    "    SQL\n",
    "    '''\n",
    "    key = feat_list[0] # key is the first feat by default\n",
    "    cols = \"\"\n",
    "    for feat in feat_list:\n",
    "        cols+=feat+','\n",
    "    cols = cols[:-1] # remove the last comma\n",
    "    return key,cols\n",
    "\n",
    "def unqiue_element_num(x):\n",
    "    a = len(np.unique(x))\n",
    "    return a    \n",
    "    \n",
    "def df2pkl(df,filename,mode='no_bak',path='default'):\n",
    "    '''\n",
    "    # DataFrame->Pkl\n",
    "    默认保存在save_path下\n",
    "    '''\n",
    "    if path == 'default':\n",
    "        global save_path\n",
    "        path = save_path\n",
    "    if mode == 'bak':\n",
    "        old_file = pkl2df(filename,path)\n",
    "        print('file shape before dealing:{}'.format(old_file.shape))\n",
    "        df2pkl(old_file,filename+'_bak','no_bak',path)\n",
    "    file = path + filename + '.pkl'\n",
    "    # print('file shape a-fter dealing:{}'.format(df.shape))\n",
    "    print(f'file:{file} has been saved in {save_path}')\n",
    "    pickle.dump(df,open(file,'wb'))\n",
    "    return None\n",
    "\n",
    "def remove_file(file,path='default',mode='rm'):\n",
    "    '''\n",
    "    check and clean space\n",
    "    '''\n",
    "    if path == 'default':\n",
    "        global save_path\n",
    "        path = save_path\n",
    "        # print(path)\n",
    "    cur_file_list = os.listdir(path)\n",
    "    for cur_file in cur_file_list:\n",
    "        if file == cur_file:\n",
    "            remove_file = path+cur_file\n",
    "            if mode == 'check':\n",
    "                print(remove_file)\n",
    "            elif mode == 'rm':\n",
    "                os.remove(remove_file)\n",
    "                # os.unlink(path)\n",
    "                print('success')\n",
    "                \n",
    "def pkl2pkl(src_file,tar_file,path='default',mode='no_bak'):\n",
    "    '''\n",
    "    Pkl->Pkl\n",
    "    mode:\n",
    "        del\n",
    "        no_bak\n",
    "    '''\n",
    "    if path == 'default':\n",
    "        global save_path\n",
    "        path = save_path\n",
    "    df = pkl2df(src_file,path)\n",
    "    df2pkl(df,tar_file,path,mode)\n",
    "    if mode == 'del':\n",
    "        remove_file(src_file,path)\n",
    "    return     \n",
    "\n",
    "    \n",
    "    \n",
    "def pkls2pkl(src_file_list,tar_file,path='default'):\n",
    "    '''\n",
    "    # Pkls->Pkl\n",
    "    '''\n",
    "    if path == 'default':\n",
    "        global save_path\n",
    "        path = save_path\n",
    "    df = pd.DataFrame()\n",
    "    for src_file in src_file_list:\n",
    "        tmp_df = pkl2df(src_file,path)\n",
    "        df = pd.concat([tmp_df,df],ignore_index=True)\n",
    "    df2pkl(df,tar_file,path)\n",
    "    return        \n",
    "\n",
    "def lists2list(lists):\n",
    "    extended_list = []\n",
    "    for single_list in lists:\n",
    "        extended_list.extend(single_list)\n",
    "    single_set = set(extended_list)\n",
    "    extended_list = list(single_set)\n",
    "    return extended_list\n",
    "\n",
    "def list2uniquelist(input_list):\n",
    "    single_set = set(input_list)\n",
    "    unique_list = list(single_set)\n",
    "    return unique_list\n",
    "    \n",
    "def cal_gb(num):\n",
    "    return num/1024/1024/1024\n",
    "    \n",
    "def check_memory():\n",
    "    '''\n",
    "    Check Memory\n",
    "    '''\n",
    "    mem = virtual_memory()\n",
    "    total = cal_gb(mem.total)# total physical memory available\n",
    "    print('total:'+str(total))\n",
    "    available = cal_gb(mem.available)# total physical memory available\n",
    "    print('available:'+str(available))\n",
    "\n",
    "\n",
    "def check_disk_usage(path='default'):\n",
    "    '''\n",
    "    Check Hard Disk\n",
    "    '''\n",
    "    if path == 'default':\n",
    "        global save_path\n",
    "        path = save_path\n",
    "    print(path)\n",
    "    total, used, free = shutil.disk_usage(path)\n",
    "    print(\"Total: %d GiB\" % (total // (2**30)))\n",
    "    print(\"Used: %d GiB\" % (used // (2**30)))\n",
    "    print(\"Free: %d GiB\" % (free // (2**30)))\n",
    "\n",
    "def show_file(file_dir='default'):\n",
    "    if file_dir == 'default':\n",
    "        global save_path\n",
    "        file_dir = save_path\n",
    "        # print(path)\n",
    "    for root, dirs, files in os.walk(file_dir):\n",
    "        print('root_dir:', root)  # 当前目录路径\n",
    "        print('sub_dirs:', dirs)  # 当前路径下所有子目录\n",
    "        print('files:', files)  # 当前路径下所有非目录子文件\n",
    "\n",
    "\n",
    "\n",
    "# feat\n",
    "def sub_binned_entropy(series,period=30,bin_num=7):\n",
    "    '''\n",
    "    计算某一特征的时间序列在等距分箱下的信息熵\n",
    "    '''\n",
    "    # pandas version\n",
    "    # Variable Statement\n",
    "    #  Args\n",
    "    base=2\n",
    "    # bin_num = 7\n",
    "    # period = 30\n",
    "    # minor_action = 1e-6 # 最小作用量\n",
    "    \n",
    "    #  Intermediate Variables\n",
    "    valid_val = 0\n",
    "    count_dict = {}\n",
    "    \n",
    "    # Output\n",
    "    entropy = 0\n",
    "    \n",
    "    # Calculate the binned entropy\n",
    "    bin_list = list(pd.cut(series,bin_num,labels=False)) #labels=False 返回分箱后的箱子序号序列\n",
    "    for bin in bin_list:\n",
    "        if bin in count_dict.keys():\n",
    "            count_dict[bin] += 1\n",
    "        else:\n",
    "            count_dict[bin] = 1\n",
    "    \n",
    "    for val in count_dict.values():\n",
    "        p = float(val/period)\n",
    "        entropy -= p*math.log(p,base)\n",
    "        valid_val += val\n",
    "    \n",
    "    # Validation\n",
    "    if valid_val != period:\n",
    "        print('valid_val:{} is not equal to period:{}'.format(valid_val,period))\n",
    "        #todo try-catch\n",
    "        return None\n",
    "    \n",
    "    # series['binned_entropy_'+str(bin_num)] = entropy # ValueError: Length of passed values is 30, index implies 31.\n",
    "    # print('series.values:{}'.format(series.values))\n",
    "    # print('bin_list:{}'.format(bin_list))\n",
    "    # print('entropy:{}'.format(entropy))\n",
    "\n",
    "    return entropy\n",
    "\n",
    "def apply_entropy(df,feat,month='202004',bin_num=7):\n",
    "    start = time.time()\n",
    "    \n",
    "    global month_period_dict\n",
    "    period=month_period_dict[int(month[-2:])]\n",
    "    # Feat Series collection\n",
    "    feat_list = []\n",
    "    for day in range(1,period+1,1):\n",
    "        if day <10:\n",
    "            day = '0' + str(day)\n",
    "        else:\n",
    "            day = str(day)\n",
    "        feat_list.append(month+day+'_'+feat)    \n",
    "    df = df[feat_list]\n",
    "    entropy = df.apply(sub_binned_entropy,args=(period,bin_num),axis='columns')\n",
    "    \n",
    "    print('time cost:{}'.format(time.time()-start))\n",
    "    # col = '202004_tgps_count_entropy'\n",
    "    # print(df.groupby(col)[col].describe())# 显示分组后的数据分布\n",
    "    # df = df[col]\n",
    "    # check_format(df)\n",
    "    return entropy\n",
    "\n",
    "def sub_extract_time(df,col):\n",
    "    # df['year'] = int(df[col][:4])\n",
    "    # df['month'] = int(df[col][4:6])\n",
    "    df['day'] = int(df[col][6:8])\n",
    "    # df['hour'] = int(df[col][8:10])\n",
    "    # df['min'] = int(df[col][10:12])\n",
    "    # df['sec'] = int(df[col][12:14])\n",
    "    return df\n",
    "\n",
    "def apply_extract_time(df,feat):\n",
    "    start = time.time()\n",
    "    df = df.apply(sub_extract_time,args=(feat,),axis='columns')\n",
    "    print(f'time cost of function apply_extract_time:{time.time()-start}')\n",
    "    return df\n",
    "\n",
    "def extract_time(df,col):\n",
    "    # df['year'] = int(df[col][:4])\n",
    "    # df['month'] = int(df[col][4:6])\n",
    "    # df['day'] = int(df[col][6:8])\n",
    "    # df['hour'] = int(df[col][8:10])\n",
    "    # df['min'] = int(df[col][10:12])\n",
    "    # df['sec'] = int(df[col][12:14])\n",
    "    # df['year'] = df[col][:4]\n",
    "    # df['month'] = df[col][4:6]\n",
    "    # df['day'] = df[col][6:8]\n",
    "    # df['hour'] = df[col][8:10]\n",
    "    # df['min'] = df[col][10:12]\n",
    "    # df['sec'] = df[col][12:14]\n",
    "    return df\n",
    "    \n",
    "def set_limit(record_num):\n",
    "    return f'limit {record_num}'\n",
    "\n",
    "# --EDA--\n",
    "# cdf\n",
    "def plot_cdf(bl,xlabel='Day tgps_count Entropy',max_val=float('inf'),ylabel='CDF'):\n",
    "    ecdf = ssm.distributions.ECDF(bl)\n",
    "    x =np.linspace(min(bl), min(max(bl),max_val))\n",
    "    y = ecdf(x)\n",
    "    fig, ax = plt.subplots(figsize=(20, 12))\n",
    "\n",
    "    ax.plot(x,y)\n",
    "    \n",
    "    ax.set_xlabel(xlabel)\n",
    "    # ax.set_ylabel(ylabel)\n",
    "    ax.set_ylabel(\"CDF\")\n",
    "    ax.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_hist(bl,xlabel='Day tgps_count Entropy',num_bins = 50,max_val=float('inf')):\n",
    "    fig, ax = plt.subplots(figsize=(20, 12))\n",
    "    ax.hist(bl, bins = num_bins, density=True, range=[min(bl), min(max(bl),max_val)],facecolor='gray',align='mid')\n",
    "    ax.set_xlabel(xlabel)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_cdf_hist(bl,xlabel='Day tgps_count Entropy',max_val=float('inf'),num_bins = 50,ylabel='CDF'):\n",
    "    ecdf = ssm.distributions.ECDF(bl)\n",
    "    x =np.linspace(min(bl), min(max(bl),max_val))\n",
    "    y = ecdf(x)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(20, 12))\n",
    "    ax_twinx = ax.twinx()\n",
    "\n",
    "    ax_twinx.hist(bl, bins = num_bins, density=True, range=[min(bl), min(max(bl),max_val)],facecolor='gray',align='mid',zorder=1)\n",
    "    ax.plot(x,y,zorder=2)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(\"CDF\")\n",
    "    ax_twinx.set_ylabel(\"Probability Density\")\n",
    "    ax.grid(True)\n",
    "    # ax.set_xlim(0,min(max(bl),max_val))\n",
    "    ax.set_ylim(0,1)\n",
    "    # ax_twinx.set_ylim(0,0.00175)\n",
    "    plt.show()    \n",
    "    \n",
    "def plot_cdf_two(df1, df2, col):\n",
    "    fst_ecdf = ssm.distributions.ECDF(df1[col])\n",
    "    fst_x = np.linspace(0, df1[col].max())\n",
    "    fst_y = fst_ecdf(fst_x)\n",
    "    plt.plot(fst_x, fst_y, linewidth='1', color='red')\n",
    "    print(col + '_mean: ', df1[col].mean())\n",
    "    \n",
    "    scd_ecdf = ssm.distributions.ECDF(df2[col])\n",
    "    scd_x = np.linspace(0, df2[col].max())\n",
    "    scd_y = scd_ecdf(scd_x)\n",
    "    plt.plot(scd_x, scd_y, linewidth='1', color='blue')\n",
    "    print(col + '_mean: ', df2[col].mean())\n",
    "    \n",
    "    x_max = max(df1[col].max(),df2[col].max())\n",
    "    plt.title(col+' CDF', fontsize=16)\n",
    "    plt.xlabel(col, fontsize=16)\n",
    "    plt.ylabel('Distributions', fontsize=16)\n",
    "    plt.legend(['normal_'+col,'lost_'+col], fontsize=16)\n",
    "    plt.xlim(0, x_max)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.show()\n",
    "# heatmap\n",
    "    \n",
    "# --Encoding--\n",
    "def onehot_encoding(df,feat_list,mode='save'):\n",
    "    '''\n",
    "    One-of-K encoding on an array of length K  ??\n",
    "    Basic method: Used with most linear algorithms  ??\n",
    "    Dropping first column avoids collinearity ??\n",
    "    Sparse format is memory-friendly  ??\n",
    "    Most current implementations don't gracefully treat missing, unseen variables how to avoid it\n",
    "    '''\n",
    "    func_suffix = ''\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    if mode == 'save':\n",
    "        func_suffix = '_onehot'\n",
    "    for feat in feat_list:\n",
    "        np_feat = df[feat].values\n",
    "        np_feat = np_feat.reshape(-1,1)\n",
    "        # print(np_feat)\n",
    "        feat_onehot_encoded=onehot_encoder.fit_transform(np_feat)\n",
    "        feat_onehot_encoded = pd.DataFrame(feat_onehot_encoded)\n",
    "        # print(feat_onehot_encoded)\n",
    "        for index,col in enumerate(feat_onehot_encoded.columns):\n",
    "            # print(feat_onehot_encoded[col])\n",
    "            df[feat+func_suffix+str(index)] = feat_onehot_encoded[col]\n",
    "    return df\n",
    "\n",
    "# def onehot_encoding_spark(df,feat_list):\n",
    "#     for feat in feat_list:\n",
    "#         string_indxr =  StringIndexer().setInputCol(feat).setOutputCol(feat+'_ind')\n",
    "#         # feat_ind = string_indxr.fit(df).transform(df.select(feat))\n",
    "#         feat_ind = string_indxr.fit(df.select(feat)).transform(df.select(feat))\n",
    "#         one_hot_encoder = OneHotEncoder().setInputCol(feat_ind).setOutputCol(feat+'_onehot')\n",
    "#         tmp = one_hot_encoder.transform(feat_ind)\n",
    "#         tmp.show()\n",
    "#     return df    \n",
    "\n",
    "def onehot_encoding_spark(df,cols):\n",
    "    '''\n",
    "    for non-tree-based model:logistic,svm,nn\n",
    "    '''\n",
    "    new_df = df\n",
    "    num = 0\n",
    "    total = len(cols)\n",
    "    print('onehot_encoding has started')\n",
    "    for col in cols:\n",
    "        num += 1\n",
    "        print(f\"{num}/{total} feature is string indexing:{time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        string_indexer = StringIndexer(inputCol = col, outputCol = col + \"_index\")\n",
    "        new_df = string_indexer.fit(new_df).transform(new_df).drop(col)\n",
    "        new_df = new_df.withColumnRenamed(col + '_onehot', col)\n",
    "        # new_df.show()\n",
    "        # print(f'col:{col}')\n",
    "        print(f\"{num}/{total} feature is onehot encoding:{time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        onehot_encoder = OneHotEncoder(inputCol = col, outputCol = col + \"_onehot\", dropLast = False)\n",
    "        new_df = onehot_encoder.transform(new_df)\n",
    "        # new_df = onehot_encoder.transform(new_df).drop(col)\n",
    "        # new_df = new_df.withColumnRenamed(col + '_onehot', col)\n",
    "        # new_df.withColumn('update_time',time.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "        # new_df.show()\n",
    "    print('onehot_encoding has finished')\n",
    "    return new_df\n",
    "    \n",
    "def label_encoding(df,feat_list,mode='save'):\n",
    "    '''\n",
    "    Give every categorical variable a unique numerical ID\n",
    "    Useful for non-linear tree-based algorithms\n",
    "    Does not increase dimensionality\n",
    "    Randomize the cat_var -> num_id mapping and  retrain, average, for small bump in accuracy\n",
    "    '''\n",
    "    func_suffix = ''\n",
    "    le = LabelEncoder()\n",
    "    if mode == 'save':\n",
    "        func_suffix = '_label'\n",
    "    for feat in feat_list:\n",
    "        enc_feat = feat+func_suffix\n",
    "        le.fit(df[feat])\n",
    "        df[enc_feat]=le.transform(df[feat])\n",
    "    return df\n",
    "\n",
    "# def label_encoding_spark(df,cols,mode='fit',encoder_list=[]):\n",
    "#     new_df = None\n",
    "#     total = len(cols) - 1\n",
    "#     print('label_encoding has started')\n",
    "#     encoder = None\n",
    "#     for i,col in enumerate(cols):\n",
    "#         if mode == 'fit':\n",
    "#             print(f\"{i}/{total} feature is string fitting:{time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "#             print(f'col:{col}')            \n",
    "#             encoder = StringIndexer(inputCol = col, outputCol = col + \"_index\")\n",
    "#             # encoder = StringIndexer.setInputCol(col).setOutputCol(col+\"_index\")\n",
    "#             encoder.fit(df)\n",
    "#             encoder_list.append(encoder)\n",
    "#         elif mode == 'transform':\n",
    "#             print(f\"{i}/{total} feature is string indexing:{time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "#             print(f'col:{col}')            \n",
    "#             encoder = encoder_list[i]\n",
    "#             print(f'type:{type(encoder)}')\n",
    "#             new_df = encoder.transform(df)\n",
    "#             print('start renaming')\n",
    "#             new_df = new_df.withColumnRenamed(col + '_index', col)\n",
    "#         # new_df.show()\n",
    "#         # print(f'col:{col}')\n",
    "#     print('label_encoding has finished')\n",
    "#     if mode == 'fit':\n",
    "#         return encoder_list\n",
    "#     elif mode == 'transform':\n",
    "#         return new_df    \n",
    "\n",
    "def label_encoding_spark(src_df,tar_df,cols):\n",
    "    # new_df = tar_df\n",
    "    total = len(cols) - 1\n",
    "    print('label_encoding has started')\n",
    "    for i,col in enumerate(cols):\n",
    "        print(f\"{i}/{total} feature is string fitting:{time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f'col:{col}')            \n",
    "        encoder = StringIndexer(inputCol = col, outputCol = col + \"_index\")\n",
    "        # encoder = StringIndexer.setInputCol(col).setOutputCol(col+\"_index\")\n",
    "        tar_df = encoder.fit(src_df).transform(tar_df)\n",
    "        # check_spark_df(tmp_df)\n",
    "        # new_df = new_df.withColumn(colName=col+'_index',col=tmp_df.select(col+'_index'))\n",
    "        # encoded_col = col + '_index'\n",
    "        # selected_col = tmp_df.select(tmp_df[encoded_col])\n",
    "        # # selected_col = tmp_df.select(tmp_df.columns[-1])\n",
    "        # print(f'type(selected_col):{type(selected_col)}')\n",
    "        \n",
    "        # # new_df = new_df.join()\n",
    "        # new_df = new_df.withColumn(colName=col+'_index',col=lit(selected_col))\n",
    "        # print('start renaming')\n",
    "        # new_df = new_df.withColumnRenamed(col + '_index', col)\n",
    "        # new_df.show()\n",
    "        # print(f'col:{col}')\n",
    "    print('label_encoding has finished')\n",
    "    return tar_df   \n",
    "    \n",
    "def sub_round_encoding(series,feat,enc_feat):\n",
    "    series[enc_feat]=int(round(series[feat]))\n",
    "    return series\n",
    "    \n",
    "def round_encoding(df,feat_list,mode='save'):\n",
    "    func_suffix = ''\n",
    "    if mode == 'save':\n",
    "        func_suffix = '_round'\n",
    "    for feat in feat_list:\n",
    "        enc_feat = feat+func_suffix\n",
    "        df= df.apply(sub_round_encoding,axis=1,args=(feat,enc_feat,))\n",
    "    return df\n",
    "\n",
    "def round_encoding_spark(df,cols):\n",
    "    total = len(cols) - 1\n",
    "    print('round_encoding has started')\n",
    "    for i,col in enumerate(cols):\n",
    "        print(f\"{i}/{total} feature is encoding:{time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f'col:{col}')\n",
    "        df = df.withColumn(col+'_round', df[col].cast(IntegerType()))\n",
    "\n",
    "    print('round_encoding has finished')\n",
    "    return df     \n",
    "    \n",
    "# def target_encoding(df,feat_list,mode='save'):\n",
    "#     '''\n",
    "#     Do in cross-validation manner\n",
    "#     Add smoothing to avoid setting variable encodings to 0.\n",
    "#     Add random noise to combat overfit\n",
    "#     '''\n",
    "#     func_suffix = ''\n",
    "#     tar_feat='is_halted_next_month'\n",
    "#     if mode == 'save':\n",
    "#         func_suffix = '_target'\n",
    "#     for feat in feat_list:\n",
    "#         enc_feat = feat+func_suffix\n",
    "#         df[enc_feat]=0\n",
    "#         uniq_vals = set(df[feat].values)\n",
    "#         # print(uniq_vals)\n",
    "#         for val in uniq_vals:\n",
    "#             # print('start cal val')\n",
    "#             # print(val)\n",
    "#             sample = df[df[feat]==val]\n",
    "#             # print(sample.shape)\n",
    "#             pos = sample[sample[tar_feat]==1]\n",
    "#             # print(pos.shape)\n",
    "#             tar_ratio = pos.shape[0]/sample.shape[0]\n",
    "#             # print(tar_ratio)\n",
    "#             # sample[enc_feat] = tar_ratio\n",
    "#             df.loc[df[feat]==val,[enc_feat]]=tar_ratio\n",
    "#     return df\n",
    "def target_encoding(df,feat_list,mode='save'):\n",
    "    '''\n",
    "    Do in cross-validation manner\n",
    "    Add smoothing to avoid setting variable encodings to 0.\n",
    "    Add random noise to combat overfit\n",
    "    '''\n",
    "    func_suffix = ''\n",
    "    tar_feat='is_halted_next_month'\n",
    "    if mode == 'save':\n",
    "        func_suffix = '_target'\n",
    "    for feat in feat_list:\n",
    "        enc_feat = feat+func_suffix\n",
    "        statistics_df = df.groupby(feat).agg(churner_num=pd.NamedAgg(column=tar_feat,aggfunc='sum'),group_user_num = pd.NamedAgg(column=feat,aggfunc='count'))\n",
    "        # check_format(statistics_df)\n",
    "        # check_detail(statistics_df)\n",
    "        df = df.merge(statistics_df,how='left',on=feat)\n",
    "        df.rename(columns={'churner_num':enc_feat+'_churner_num','group_user_num':enc_feat+'_group_user_num'},inplace=True)\n",
    "        # check_format(df,'cols')\n",
    "        df[enc_feat]=df[enc_feat+'_churner_num']/df[enc_feat+'_group_user_num']\n",
    "    return df\n",
    "\n",
    "def target_encoding_spark(df,feat_list,tar_feat,mode='save'):\n",
    "    func_suffix = ''\n",
    "    total = len(feat_list) - 1\n",
    "    print('target_encoding has started')    \n",
    "    if mode == 'save':\n",
    "        func_suffix = '_target'\n",
    "    for i,feat in enumerate(feat_list):\n",
    "        print(f\"{i}/{total} feature is encoding:{time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f'feat:{feat}')\n",
    "        enc_feat = feat+func_suffix\n",
    "        user_group_df = df.groupBy(feat).count().withColumnRenamed(\"count\",'tmp_group_user_num')\n",
    "\n",
    "        churner_group_df = df.groupBy(feat).sum(tar_feat).withColumnRenamed(f\"sum({tar_feat})\",'tmp_group_churner_num')\n",
    "        \n",
    "        check_spark_df(user_group_df)\n",
    "        check_spark_df(churner_group_df)\n",
    "        \n",
    "        join_expression = df[feat] == user_group_df[feat]\n",
    "        join_type=\"inner\"\n",
    "        \n",
    "        # df = df.join(user_group_df,join_expression,join_type)\n",
    "        df=df.join(user_group_df,[feat])\n",
    "        df=df.join(churner_group_df,[feat])\n",
    "        # df[enc_feat]=df.select(col('tmp_group_user_num')/col('tmp_group_churner_num'))\n",
    "        df=df.withColumn(enc_feat,col('tmp_group_churner_num')/col('tmp_group_user_num'))\n",
    "        check_spark_df(df,10,[enc_feat,feat,'tmp_group_user_num','tmp_group_churner_num'])\n",
    "        \n",
    "        df=df.drop('tmp_group_user_num','tmp_group_churner_num')\n",
    "        \n",
    "        # df = df.merge(statistics_df,how='left',on=feat)\n",
    "        # df.rename(columns={'churner_num':enc_feat+'_churner_num','group_user_num':enc_feat+'_group_user_num'},inplace=True)\n",
    "        # # check_format(df,'cols')\n",
    "        # df[enc_feat]=df[enc_feat+'_churner_num']/df[enc_feat+'_group_user_num']\n",
    "    print('target_encoding has finished')\n",
    "    return df    \n",
    "\n",
    "def period_generator(month='202004',period=None,start=None):\n",
    "    global month_period_dict\n",
    "    if period is None:\n",
    "        period = month_period_dict[int(month[-2:])]\n",
    "    period_list = []\n",
    "    if start is None:\n",
    "        start = 1\n",
    "    for day in range(start,start+period,1):\n",
    "        if day < 10:\n",
    "            day = '0' +str(day)\n",
    "        else:\n",
    "            day = str(day)\n",
    "        period_feat = month+day\n",
    "        period_list.append(period_feat)\n",
    "    return period_list\n",
    "\n",
    "        \n",
    "def period_feat(feat='byte_in',month='202004',period=30):\n",
    "    period_feat_list = []\n",
    "    for day in range(1,period+1,1):\n",
    "        if day < 10:\n",
    "            day = '0' +str(day)\n",
    "        else:\n",
    "            day = str(day)\n",
    "        period_feat = month+day+'_'+feat\n",
    "        period_feat_list.append(period_feat)\n",
    "    return period_feat_list\n",
    "\n",
    "def records2statistics(records,day,kind='cdr',gran='day'):\n",
    "    statistics = None\n",
    "    if kind == 'traffic':\n",
    "        \n",
    "        addition_sum = records.groupby(by=['prod_inst_id']).sum()\n",
    "        addition_sum.rename(columns={'byte_in':day+'_byte_in','byte_out':day+'_byte_out','duration':day+'_duration'},inplace=True)\n",
    "        # check_format(addition_sum)\n",
    "        # check_detail(addition_sum)\n",
    "    \n",
    "        record = pd.DataFrame(records.groupby(by=['prod_inst_id']).size())\n",
    "        record.rename(columns={0:day+'_record_num'},inplace=True)\n",
    "        # check_format(record)\n",
    "        # check_detail(record)\n",
    "        \n",
    "        statistics = addition_sum.join(record,how='left',on='prod_inst_id')\n",
    "        statistics.reset_index(inplace=True)\n",
    "        statistics.rename({'index':'prod_inst_id'},inplace=True)\n",
    "        # check_format(statistics)\n",
    "        # check_detail(statistics)\n",
    "        \n",
    "    elif kind == 'cdr':\n",
    "        # 'prod_inst_id','acc_nbr','calling_nbr','called_nbr','duration','day_id'\n",
    "        calling_records = records[records['acc_nbr']==records['calling_nbr']]\n",
    "        called_records = records[records['acc_nbr']==records['called_nbr']]\n",
    "        # check_format(calling_records,'simple')\n",
    "        # check_format(called_records,'simple')\n",
    "        # check_format(records,'simple')\n",
    "        \n",
    "        addition_sum1 = calling_records.groupby(by=['prod_inst_id']).sum()\n",
    "        addition_sum1.rename(columns={'duration':day+'_calling_duration'},inplace=True)\n",
    "        addition_sum2 = called_records.groupby(by=['prod_inst_id']).sum()\n",
    "        addition_sum2.rename(columns={'duration':day+'_called_duration'},inplace=True)\n",
    "        addition_sum = addition_sum1.merge(addition_sum2,how='outer',on='prod_inst_id')\n",
    "        # check_format(addition_sum1,'simple')\n",
    "        # check_format(addition_sum2,'simple')\n",
    "        # check_format(addition_sum,'simple')\n",
    "        # check_format(addition_sum)\n",
    "        # check_detail(addition_sum1)\n",
    "        # check_detail(addition_sum2)\n",
    "        # check_detail(addition_sum)\n",
    "    \n",
    "        record1 = pd.DataFrame(calling_records.groupby(by=['prod_inst_id']).size())\n",
    "        record1.rename(columns={0:day+'_calling_record_num'},inplace=True)\n",
    "        record2 = pd.DataFrame(called_records.groupby(by=['prod_inst_id']).size())\n",
    "        record2.rename(columns={0:day+'_called_record_num'},inplace=True)\n",
    "        record = record1.merge(record2,how='outer',on='prod_inst_id')\n",
    "        # check_format(record1,'simple')\n",
    "        # check_format(record2,'simple')\n",
    "        # check_format(record,'simple')\n",
    "        # # check_format(record)\n",
    "        # check_detail(record1)\n",
    "        # check_detail(record2)\n",
    "        # check_detail(record)\n",
    "        \n",
    "        statistics = addition_sum.join(record,how='outer',on='prod_inst_id')\n",
    "        statistics.reset_index(inplace=True)\n",
    "        statistics.rename({'index':'prod_inst_id'},inplace=True)\n",
    "        # check_format(statistics,'simple')\n",
    "        # check_detail(statistics)\n",
    "    return statistics\n",
    "\n",
    "def records2statistics_spark(records,day,kind='cdr',gran='day'):\n",
    "    '''\n",
    "    daily_traffic_col_list = ['prod_inst_id','byte_in','byte_out','p_day_id','duration']\n",
    "    daily_cdr_col_list =['prod_inst_id','acc_nbr','calling_nbr','called_nbr','duration']\n",
    "    '''\n",
    "    \n",
    "    statistics = None\n",
    "    if kind == 'traffic':\n",
    "        print('*****spark start:*********')\n",
    "        # records.printSchema()\n",
    "        # # records.columns()\n",
    "        # records.show(10)\n",
    "        \n",
    "        # records.filter(col(\"prod_inst_id\") == '920460094277').show(100)\n",
    "        statistics = records.groupBy('prod_inst_id').agg(sum(\"byte_in\").alias(day+'_byte_in'),count(\"byte_in\").alias(day+'_record_num'),sum(\"byte_out\").alias(day+'_byte_out'),sum(\"duration\").alias(day+'_duration'))\n",
    "        # statistics = records.groupBy('prod_inst_id').agg(sum(\"duration\").alias(day+'_duration'))                # print(type(group_sum)) # list of rows\n",
    "        # print(type(group_sum)) # list of rows\n",
    "        # group_sum.show(10)\n",
    "        # group_sum.filter(col(\"prod_inst_id\") == '920460094277').show()\n",
    "    elif kind == 'cdr':\n",
    "        # calling_records = records.filter(col('acc_nbr') == col('calling_nbr'))\n",
    "        calling_records = records.filter(col('acc_nbr') == col('calling_nbr_new'))\n",
    "        # calling_records.show(10)\n",
    "        # called_records = records.filter(col('acc_nbr') == col('called_nbr'))\n",
    "        called_records = records.filter(col('acc_nbr') == col('called_nbr_new'))\n",
    "        # called_records.show(10)\n",
    "        \n",
    "        calling_statistics = calling_records.groupBy('prod_inst_id').agg(sum(\"duration\").alias(day+'_calling_duration'),count(\"duration\").alias(day+'_calling_record_num'))\n",
    "        called_statistics = called_records.groupBy('prod_inst_id').agg(sum(\"duration\").alias(day+'_called_duration'),count(\"duration\").alias(day+'_called_record_num'))\n",
    "        statistics = calling_statistics.join(called_statistics,\"prod_inst_id\",\"outer\")\n",
    "        \n",
    "    \n",
    "    return statistics\n",
    "    \n",
    "def cal_sequence_feat(kind,table_name,col_list,month='202011'):\n",
    "    sequence_set = None\n",
    "    global month_period_dict\n",
    "    period = month_period_dict[int(month[-2:])]\n",
    "    start = time.time()\n",
    "    for i in range(1,period+1,1):\n",
    "    # for i in range(1,11,1):\n",
    "    # for i in range(1,3,1):\n",
    "        # Load raw table data\n",
    "        # Table Selection\n",
    "        # table_name = 'hlwk_raw_traffic_21_201101_201107' # 2020年11月1号到7号张家界互联网卡目标用户的原始流量表\n",
    "        # table_name = 'hlwk_simple_cdr_21_202011' # 2020年11月张家界互联网卡目标用户的简单cdr表\n",
    "        # table_name = 'hlwk_simple_cdr_2012_quick' # 2020年12月互联网卡目标用户的简单cdr表\n",
    "        # query_table = personal_database + table_name\n",
    "        query_table = table_name\n",
    "        \n",
    "        #  Columns Selection\n",
    "        # sql_feat_list = daily_traffic_feat_list\n",
    "        # sql_feat_list = daily_cdr_col_list\n",
    "        sql_feat_list = col_list\n",
    "        key,cols = feats2cols(sql_feat_list)\n",
    "        \n",
    "        if i < 10:\n",
    "            day = month + '0' + str(i)\n",
    "        else:\n",
    "            day = month + str(i)\n",
    "        \n",
    "        constraint_key = ''\n",
    "        if kind == 'cdr':\n",
    "            constraint_key = 'day_id' #fea\n",
    "        elif kind == 'traffic':\n",
    "            # constraint_key = 'p_day_id'\n",
    "            constraint_key = 'day_id'\n",
    "        constraint_value = day\n",
    "        \n",
    "        # limit_mode = 'limit 100'\n",
    "        query_sql = condition_sql.format(cols,query_table,constraint_key,constraint_value,limit_mode)\n",
    "        \n",
    "        #  Execute SQL\n",
    "        # start = time.time()\n",
    "        spark_df = spark.sql(query_sql) # type:pyspark.sql.dataframe.DataFrame\n",
    "        \n",
    "        # # pandas version fit to 10w\n",
    "        # #  SparkDF2PandasDF\n",
    "        # daily_set = spark_df.toPandas()\n",
    "        # # print('the time cost of function {}: {}s'.format('',time.time()-start))\n",
    "        # # start = time.time()\n",
    "        # #  Statistics processing\n",
    "        # daily_statistics = records2statistics(daily_set,day,kind)\n",
    "\n",
    "        # #  Feature Conjunction\n",
    "        # if sequence_set is None:\n",
    "        #     sequence_set = daily_statistics\n",
    "        # else:\n",
    "        #     sequence_set = sequence_set.merge(daily_statistics,how='outer',on='prod_inst_id')\n",
    "        # print('sequence_set.shape):{}'.format(sequence_set.shape))\n",
    "        \n",
    "        # spark version\n",
    "        daily_statistics = records2statistics_spark(spark_df,day,kind)\n",
    "        \n",
    "        if sequence_set is None:\n",
    "            sequence_set = daily_statistics\n",
    "        else:\n",
    "            join_type = 'outer'\n",
    "            join_expression = sequence_set[\"prod_inst_id\"] == daily_statistics['prod_inst_id']\n",
    "            # sequence_set = sequence_set.join(daily_statistics,join_expression,join_type)\n",
    "            sequence_set = sequence_set.join(daily_statistics,\"prod_inst_id\",join_type)\n",
    "                \n",
    "    # sequence_pd_set = sequence_set.toPandas()\n",
    "    \n",
    "    # check_format(sequence_pd_set)\n",
    "    # check_detail(sequence_pd_set)\n",
    "    # print('the time cost of function {}: {}s'.format('',time.time()-start))\n",
    "    # # sequence_set.fillna(value=0,inplace=True) # forbidden to deal in model\n",
    "    # print('sequence_set.shape):{}'.format(sequence_pd_set.shape))\n",
    "    # print('sys.getsizeof(sequence_set):{}'.format(sys.getsizeof(sequence_pd_set)/1024/1024))\n",
    "\n",
    "    # return sequence_set,sequence_pd_set\n",
    "    return sequence_set\n",
    "\n",
    "# sequential\n",
    "def cal_sequence_feat_spark(id_table,kind,seq_table,col_list,month='202011'):\n",
    "    '''\n",
    "    Input:\n",
    "        table of prod_inst_id;\n",
    "        kind of sequential data;\n",
    "        table of sequential data;\n",
    "        list of extracted feature;\n",
    "        month\n",
    "    Output:\n",
    "        spark dataframe of sequential statistics of some prod_inst_id set\n",
    "    '''\n",
    "    seq_df = None\n",
    "    period = month_period_dict[int(month[-2:])]\n",
    "    start = time.time()\n",
    "    print(f'id_table:{id_table}')\n",
    "    print(f'seq_table:{seq_table}')\n",
    "\n",
    "    constraint_key = ''\n",
    "    if kind == 'cdr':\n",
    "        constraint_key = 'day_id'\n",
    "    elif kind == 'traffic':\n",
    "        constraint_key = 'p_day_id'\n",
    "        constraint_key = 'day_id'\n",
    "       \n",
    "    for i in range(1,period+1,1):\n",
    "    # for i in range(1,3,1):\n",
    "        # Load raw table data\n",
    "        # Table Selection\n",
    "        query_table = seq_table\n",
    "        \n",
    "        #  Columns Selection\n",
    "        sql_feat_list = col_list\n",
    "        key,cols = feats2cols(sql_feat_list)\n",
    "        \n",
    "        if i < 10:\n",
    "            day = month + '0' + str(i)\n",
    "        else:\n",
    "            day = month + str(i)\n",
    "        constraint_value = day \n",
    "        # limit_mode = 'limit 100'\n",
    "        \n",
    "        query_sql = ''\n",
    "        if id_table == '':\n",
    "            query_sql = condition_sql.format(cols,query_table,constraint_key,constraint_value,limit_mode)\n",
    "        else:\n",
    "            # query_sql = \"SELECT {} FROM {} WHERE {} = {} {}\".format(cols,query_table,constraint_key,constraint_value,limit_mode)\n",
    "            # query_sql = 'SELECT a.'+cols+' FROM '+query_table+' a,'+id_table+' b WHERE a.'+constraint_key+' = '+constraint_value+' AND a.prod_inst_id=b.prod_inst_id'\n",
    "            # query_sql = 'SELECT a.'+cols+' FROM '+query_table+' a,'+id_table+' b WHERE a.'+constraint_key+' = '+constraint_value+' AND a.prod_inst_id=b.prod_inst_id and b.game_app_top1_name=1'\n",
    "            query_sql = \"SELECT a.prod_inst_id,a.day_id,duration FROM user_sjwj.view_tmp_app_exp_event_tgps_202110_apn_15303345281_20201124_001 a,user_sjwj.ic_2107 b WHERE a.day_id = \"+constraint_value+\" AND a.prod_inst_id=b.prod_inst_id and b.game_app_top1_name=1\"          \n",
    "            \n",
    "        print(f'query_sql:{query_sql}')    \n",
    "        \n",
    "        #  Execute SQL\n",
    "        spark_df = spark.sql(query_sql) # type:pyspark.sql.dataframe.DataFrame\n",
    "        \n",
    "        # spark version\n",
    "        daily_statistics = records2statistics_spark(spark_df,day,kind)\n",
    "        \n",
    "        if seq_df is None:\n",
    "            seq_df = daily_statistics\n",
    "        else:\n",
    "            join_type = 'outer'\n",
    "            join_expression = seq_df[\"prod_inst_id\"] == daily_statistics['prod_inst_id']\n",
    "            # sequence_set = sequence_set.join(daily_statistics,join_expression,join_type)\n",
    "            seq_df = seq_df.join(daily_statistics,\"prod_inst_id\",join_type)\n",
    "                \n",
    "    # check_spark_df(seq_df)\n",
    "    print(f'the time cost of function: {time.time()-start:.1f}s')\n",
    "    # print(f'sys.getsizeof(seq_df):{sys.getsizeof(seq_df)/1024/1024:.1f} MB') # not correct\n",
    "    \n",
    "    # sequence_set.fillna(value=0,inplace=True) # forbidden to deal in model\n",
    "\n",
    "    return seq_df\n",
    "#  SparkDF2Parquet\n",
    "# def sparkdf2parquet(sparkdf,dst_filename,backup_filename,if_backup='True',file_format='parquet',mode='overwrite'):\n",
    "#     if if_backup=='TRUE':\n",
    "#         try:\n",
    "#             old_sparkdf = spark.read.format(file_format).load(dst_filename)\n",
    "#         except IOError:\n",
    "#             print(file_format+'file load failed') # dst file not exists due to save the sparkdf firstly\n",
    "#             sparkdf.write.format(file_format).mode(mode).save(backup_filename)\n",
    "#         else:    \n",
    "#             old_sparkdf.format(file_format).mode(mode).save(backup_filename)\n",
    "#     sparkdf.write.format(file_format).mode(mode).save(dst_filename)\n",
    "    \n",
    "\n",
    "# Model\n",
    "def standardization(df,feat_list=None,scaler=StandardScaler()):\n",
    "    '''\n",
    "    标准化处理 统一函数接口\n",
    "    func可接受4种标准化函数:MinMaxScaler, StandardScaler, MaxAbsScaler,Normalizer\n",
    "    '''\n",
    "    if feat_list is None:\n",
    "        for feat in df.columns:\n",
    "            tmp = df[feat].values.reshape(-1,1)\n",
    "            df[feat] = scaler.fit_transform(tmp)\n",
    "            joblib.dump(scaler, 'scaler_'+feat+\".gz\")\n",
    "    else:\n",
    "        for feat in feat_list:\n",
    "            tmp = df[feat].values.reshape(-1,1)\n",
    "            df[feat] = scaler.fit_transform(tmp)\n",
    "            joblib.dump(scaler, 'scaler_'+feat+\".gz\")\n",
    "    df = pd.DataFrame(df)\n",
    "    # check_format(df)\n",
    "    return df\n",
    "    \n",
    "def inverse_standardization(df, feat_list):\n",
    "    # df = pd.DataFrame(np_array)\n",
    "    for feat in feat_list:\n",
    "        scaler = joblib.load('scaler_'+feat+\".gz\")\n",
    "        df[feat] = scaler.inverse_transform(df[feat].values)\n",
    "    return df\n",
    "    \n",
    "# Define Metrics\n",
    "def confusion_matrix_reset():\n",
    "    global tp\n",
    "    global tn\n",
    "    global fp\n",
    "    global fn\n",
    "    \n",
    "    tp=0\n",
    "    fn=0\n",
    "    tn=0\n",
    "    fp=0\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def confusion_matrix_count(series,threshold):\n",
    "    global tp\n",
    "    global tn\n",
    "    global fp\n",
    "    global fn\n",
    "    # print('tp:{}'.format(tp),end=' ')\n",
    "    # print('tn:{}'.format(tn),end=' ')\n",
    "    # print('fp:{}'.format(fp),end=' ')\n",
    "    # print('fn:{}'.format(fn))\n",
    "    ground_truth = series.iloc[0]\n",
    "    pred_truth = series.iloc[-1]\n",
    "    \n",
    "    if ground_truth == 1:\n",
    "        if pred_truth >= threshold:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "    else:\n",
    "        if pred_truth >= threshold:\n",
    "            fp += 1\n",
    "        else:\n",
    "            tn += 1\n",
    "    return         \n",
    "\n",
    "def confusion_matrix_plot():\n",
    "    global tp\n",
    "    global tn\n",
    "    global fp\n",
    "    global fn\n",
    "    # print('tp:{}'.format(tp))\n",
    "    # print('tn:{}'.format(tn))\n",
    "    # print('fp:{}'.format(fp))\n",
    "    # print('fn:{}'.format(fn))\n",
    "    data = pd.DataFrame([[tn,fp],[fn,tp]])\n",
    "    \n",
    "    data.rename(index={0:'Normal',1:'Churn'},columns={0:'Normal',1:'Churn'},inplace=True)\n",
    "\n",
    "    # print('data:{}'.format(data))\n",
    "    fig, ax= plt.subplots()\n",
    "    ax = sns.heatmap(data,annot=True,fmt='d', cmap=\"YlGnBu\", linewidths=1.0)\n",
    "    ax.set_xlabel('Ground Truth',fontsize=20)\n",
    "    ax.set_ylabel('Predicted Label',fontsize=20)\n",
    "    return\n",
    "\n",
    "\n",
    "def f_beta(precision,recall,beta=1):\n",
    "    ratio = math.pow(beta,2)\n",
    "    return (1+ratio)*precision*recall/(ratio*precision+recall)  \n",
    "\n",
    "def class_metrics():\n",
    "    global tp\n",
    "    global fn\n",
    "    global fp\n",
    "    \n",
    "    positive_precision = (tp)/(tp+fp)\n",
    "    positive_recall = (tp)/(tp+fn)\n",
    "    positive_f1 = f_beta(positive_precision,positive_recall,beta=1)\n",
    "    # positive_f2 = f_beta(positive_precision,positive_recall)\n",
    "    \n",
    "    # print('%.4f' % positive_precision)\n",
    "    # print('%.4f' % positive_recall)\n",
    "    # print('%.4f' % positive_f1)\n",
    "    # print('%.4f' % positive_f2)\n",
    "    \n",
    "    # positive_recall = round(positive_recall*100,2)\n",
    "    # positive_precision = round(positive_precision*100,2)\n",
    "    # positive_f1 = round(positive_f1*100,2)\n",
    "    \n",
    "    positive_recall = round(positive_recall,4)\n",
    "    positive_precision = round(positive_precision,4)\n",
    "    positive_f1 = round(positive_f1,4)    \n",
    "    \n",
    "    # print(f'recall:{positive_recall:.2f}')\n",
    "    # print(f'precision:{positive_precision:.2f}')\n",
    "    \n",
    "    return positive_f1,positive_recall,positive_precision\n",
    "    \n",
    "def auc(y_test,y_pre,mode='plot'):\n",
    "    # 增加别的roc_auc计算方法 看看这个计算正不正确\n",
    "    \n",
    "    # print(f'y_test.shape:{y_test.shape}')\n",
    "    # print(f'y_test:{y_test[:10]}')\n",
    "    # print(f'y_pre.shape:{y_pre.shape}')\n",
    "    # print(f'y_pre:{y_pre[:10]}')\n",
    "    # print('fpr:{}'.format(fpr))\n",
    "    # print('tpr:{}'.format(tpr))\n",
    "    # print('thresholds:{}'.format(thresholds))\n",
    "    auc = roc_auc_score(y_test, y_pre)\n",
    "    sota_auc = 0.97\n",
    "    # print('AUC:{}'.format('%.4f' % auc))\n",
    "    # auc *= 100\n",
    "    # print('{:.2f}'.format(auc))\n",
    "    # if(auc>sota_auc):\n",
    "    #     print('6666666666 Congratulation New SOTA 6666666666666666')\n",
    "\n",
    "    if mode=='plot':\n",
    "        fig, ax= plt.subplots()\n",
    "        lw = 2\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pre)\n",
    "        ax.plot(fpr, tpr, label=\"ROC(area ={:.3f})\".format(auc), color='darkorange', lw=lw)\n",
    "        ax.plot([0, 1], [0, 1], color='navy', linestyle='--',lw=lw)\n",
    "        ax.legend(loc=4, fontsize=10)\n",
    "        # ax.set_title('User Churn Prediction ROC with '+ model_name +' ',fontsize=14)\n",
    "        ax.set_xlabel('False Positive Rate',fontsize=20)\n",
    "        ax.set_ylabel('True Positive Rate',fontsize=20)\n",
    "    elif mode == 'cal':\n",
    "        pass\n",
    "        \n",
    "    return auc    \n",
    "\n",
    "def prauc(y_test,y_pre,mode='plot'):\n",
    "    average_precision = average_precision_score(y_test, y_pre)\n",
    "    # average_precision *= 100\n",
    "    if mode=='plot':\n",
    "        sota_auprc = 60\n",
    "        # print('{:.2f}'.format(average_precision))\n",
    "        # if(average_precision>sota_auprc):\n",
    "        #     print('666666666666666 Congratulation New SOTA 6666666666666666')\n",
    "        precision,recall,thresholds = precision_recall_curve(y_test,y_pre)\n",
    "        fig, ax= plt.subplots()\n",
    "        lw = 2\n",
    "        ax.plot(recall, precision, label=\"PRC(area ={:.3f})\".format(average_precision), color='darkblue', lw=lw)\n",
    "        ax.plot([0, 1], [1, 0], color='navy', linestyle='--',lw=lw)\n",
    "        ax.legend(loc=4, fontsize=10)\n",
    "        # ax.set_title('User Churn Prediction PRC with '+ model_name +' ',fontsize=14)\n",
    "        ax.set_xlabel('Recall',fontsize=20)\n",
    "        ax.set_ylabel('Precision',fontsize=20)\n",
    "    elif mode=='cal':\n",
    "        precision,recall,thresholds = precision_recall_curve(y_test,y_pre)\n",
    "      \n",
    "    return recall,precision,thresholds,average_precision    \n",
    "\n",
    "def cal_best_f1(recall,precision,thresholds):\n",
    "    theory_best_f1 = 0\n",
    "    # best_f1_rec = 0\n",
    "    # best_f1_pre =0\n",
    "    best_f1_threshold =0\n",
    "    for rec,pre,threshold in zip(recall,precision,thresholds):\n",
    "        f1 = f_beta(pre,rec)\n",
    "        if f1 > theory_best_f1:\n",
    "            theory_best_f1 = f1\n",
    "            # best_f1_pre = pre\n",
    "            # best_f1_rec = rec\n",
    "            best_f1_threshold = threshold\n",
    "    \n",
    "    # best_f1 *= 100\n",
    "    # best_f1_rec *= 100\n",
    "    # best_f1_pre *= 100\n",
    "    \n",
    "    # print('{}'.format('%.2f' % best_f1))\n",
    "    # print('{}'.format('%.2f' % best_f1_rec))\n",
    "    # print('{}'.format('%.2f' % best_f1_pre))\n",
    "    # print('{0:.2f}'.format(best_f1_threshold))\n",
    "    \n",
    "    theory_best_f1 = round(theory_best_f1,3)\n",
    "    best_f1_threshold = round(best_f1_threshold,2)# to avoid overfitting\n",
    "    \n",
    "    return theory_best_f1,best_f1_threshold     \n",
    "\n",
    "# def topu_metrics(y_test,y_pre,threshold,user_num_ratio):\n",
    "def topu_metrics(y_test,y_pre,threshold,part_gran):\n",
    "    # part_gran = 2000\n",
    "    # part_gran = 50\n",
    "    # part_gran = 100\n",
    "    # part_gran = 500*user_num_ratio # 10w user\n",
    "    \n",
    "    # part_gran = 100000\n",
    "    # part_gran = 1000\n",
    "    # part_gran = 2000\n",
    "    \n",
    "    # y_pre.rename(columns={0:'predict'},inplace=True)\n",
    "    # y_test.rename(columns={0:'truth'},inplace=True)\n",
    "    \n",
    "    # 0 static statistics\n",
    "    total_churner_num = y_test.loc[y_test.iloc[:,0] == 1].shape[0]\n",
    "    print('total_churner_num:{}'.format(total_churner_num))\n",
    "    \n",
    "    # part_gran = int(total_churner_num * user_num_ratio)\n",
    "    \n",
    "    # 1 concat the predcition and ground truth\n",
    "    y_pre.rename(columns={y_pre.columns[0]:\"predict\"}, inplace=True)\n",
    "    y_test.rename(columns={y_test.columns[0]:\"truth\"}, inplace=True)\n",
    "    result = pd.concat([y_pre,y_test],axis=1,join='inner')\n",
    "    \n",
    "    check_format(result)\n",
    "    check_detail(result)\n",
    "    \n",
    "    # result.rename(columns={result.columns[0]:\"predict\",result.columns[1]:\"truth\"}, inplace=True)\n",
    "    # check_format(result)\n",
    "    # check_detail(result)\n",
    "    \n",
    "    # 2 sort by score in descending order\n",
    "    result.sort_values(by=result.columns[0],ascending=False,inplace=True)\n",
    "    \n",
    "    # 4 filtered by thershlod\n",
    "    suspected_churner = result.loc[result.iloc[:,0] >= threshold] #?? whether > or >=\n",
    "    suspected_churner_num = suspected_churner.shape[0]\n",
    "#   suspected_churner_num += 300 # add more users under the threshold to increase the recall\n",
    "    \n",
    "    print('suspected_churner_num:{}'.format(suspected_churner_num))\n",
    "    #   reduce user to an integral multiple of part gran\n",
    "    \n",
    "    # top_churner_num = suspected_churner_num // part_gran * part_gran\n",
    "    top_churner_num = part_gran * 10\n",
    "    \n",
    "    # user_num = y_test.shape[0] // part_gran * part_gran\n",
    "    # print(f'user_num:{user_num}')\n",
    "    \n",
    "    top_churner = suspected_churner[:top_churner_num]\n",
    "    print('top_churner_num:{}'.format(top_churner_num))\n",
    "    \n",
    "    class_num = top_churner_num // part_gran\n",
    "    class_range = range(0,class_num)\n",
    "    metrics_matrix = np.zeros((class_num,2))\n",
    "#   print('metrics_matrix:{}'.format(metrics_matrix))\n",
    "    \n",
    "    true_churners_num = 0\n",
    "    total_user_num = 0\n",
    "    print('cur_true_churners_num:')\n",
    "    for top_churner_class in class_range:\n",
    "        churners = top_churner[top_churner_class*part_gran:(top_churner_class+1)*part_gran]\n",
    "        cur_true_churners_num = churners.loc[churners.iloc[:,1]==1].shape[0]\n",
    "       # print('cur_true_churners_num:{}'.format(cur_true_churners_num))\n",
    "        # print(cur_true_churners_num)\n",
    "        true_churners_num += cur_true_churners_num\n",
    "        print(true_churners_num)\n",
    "        total_user_num += part_gran\n",
    "        R_U = true_churners_num / total_churner_num # R@U = The number of true churners in top U / The total number of true churners\n",
    "        P_U = true_churners_num / total_user_num # P@U = The number of true churners in top U / U\n",
    "        metrics_matrix[top_churner_class,0] = R_U\n",
    "        metrics_matrix[top_churner_class,1] = P_U\n",
    "    \n",
    "    print('Top U')\n",
    "    for i,metrics in enumerate(metrics_matrix):\n",
    "        print((i+1)*part_gran)\n",
    "    \n",
    "    # print('Recall:')\n",
    "    # for i,metrics in enumerate(metrics_matrix):    \n",
    "    #     print('{:.4f}'.format(metrics[0]))\n",
    "    # print('Precision:')    \n",
    "    # for i,metrics in enumerate(metrics_matrix):    \n",
    "    #     print('{:.4f}'.format(metrics[1]))\n",
    "    \n",
    "    recall_list = []\n",
    "    precision_list = []\n",
    "    print('Recall:')\n",
    "    for i,metrics in enumerate(metrics_matrix):    \n",
    "        recall_list.append(metrics[0])\n",
    "    print(recall_list)\n",
    "    \n",
    "    print('Precision:')    \n",
    "    for i,metrics in enumerate(metrics_matrix):    \n",
    "        precision_list.append(metrics[1])\n",
    "    print(precision_list)\n",
    "    \n",
    "    return metrics_matrix\n",
    "    \n",
    "def adv_period_feat(feat='byte_in',period=[1,7],month='202004',frq=1):\n",
    "    '''advanced period feat extraction'''\n",
    "    period_feat_list = []\n",
    "    for day in range(period[0],period[1]+1):\n",
    "        if day % frq == 0:\n",
    "            if day < 10:\n",
    "                day = '0' +str(day)\n",
    "            else:\n",
    "                day = str(day)\n",
    "            period_feat = month+day+'_'+feat\n",
    "            period_feat_list.append(period_feat)\n",
    "        else:\n",
    "            period_feat_list.append('')\n",
    "    return period_feat_list\n",
    "\n",
    "def time_count(moduel_name,time_start,mode='count'):\n",
    "    global time_count_dict\n",
    "    if mode == 'count':\n",
    "        time_end = time.time()\n",
    "        time_cost = time_end - time_start\n",
    "        time_count_dict[moduel_name] = time_cost\n",
    "        return time_end\n",
    "    elif mode == 'print':\n",
    "        for key,val in time_count_dict.items():\n",
    "            print('Moduel : {}, Time Cost : {}'.format(key.ljust(15),'%.2f' % val))    \n",
    "\n",
    "def plot_loss(loss_dict):\n",
    "    fig,ax = plt.subplots()\n",
    "    style_list = ['bo','b','orange']\n",
    "    legend_list = []\n",
    "    for loss,style in zip(loss_dict.items(),style_list):\n",
    "        epochs = range(1,len(loss[1])+1)\n",
    "        ax.plot(epochs,loss[1],style,label=loss[0])\n",
    "        legend_list.append(loss[0])\n",
    "    ax.legend(legend_list)\n",
    "    ax.set_xlabel('Epochs',fontsize =20)\n",
    "    ax.set_ylabel('Loss',fontsize =20)\n",
    "\n",
    "def summary(model, input_size, batch_size=-1, device=torch.device('cuda:0'), dtypes=None):\n",
    "    result, params_info = summary_string(\n",
    "        model, input_size, batch_size, device, dtypes)\n",
    "    print(result)\n",
    "    return params_info\n",
    "\n",
    "def summary_string(model, input_size, batch_size=-1, device=torch.device('cuda:0'), dtypes=None):\n",
    "    if dtypes == None:\n",
    "        dtypes = [torch.FloatTensor]*len(input_size)\n",
    "\n",
    "    summary_str = ''\n",
    "    # device = torch.device(\"cpu\")\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    def register_hook(module):\n",
    "        def hook(module, input, outputs):\n",
    "            class_name = str(module.__class__).split(\".\")[-1].split(\"'\")[0]\n",
    "            module_idx = len(summary)\n",
    "\n",
    "            m_key = \"%s-%i\" % (class_name, module_idx + 1)\n",
    "            summary[m_key] = OrderedDict()\n",
    "            summary[m_key][\"input_shape\"] = list(input[0].size())\n",
    "            summary[m_key][\"input_shape\"][0] = batch_size\n",
    "            if isinstance(outputs, (list, tuple)):\n",
    "                summary[m_key][\"output_shape\"] = [\n",
    "                    [-1] + list(o.size())[1:] for o in outputs\n",
    "                ]\n",
    "            else:\n",
    "                summary[m_key][\"output_shape\"] = list(outputs.size())\n",
    "                summary[m_key][\"output_shape\"][0] = batch_size\n",
    "\n",
    "            params = 0\n",
    "            if hasattr(module, \"weight\") and hasattr(module.weight, \"size\"):\n",
    "                params += torch.prod(torch.LongTensor(list(module.weight.size())))\n",
    "                summary[m_key][\"trainable\"] = module.weight.requires_grad\n",
    "            if hasattr(module, \"bias\") and hasattr(module.bias, \"size\"):\n",
    "                params += torch.prod(torch.LongTensor(list(module.bias.size())))\n",
    "            summary[m_key][\"nb_params\"] = params\n",
    "\n",
    "        if (\n",
    "            not isinstance(module, nn.Sequential)\n",
    "            and not isinstance(module, nn.ModuleList)\n",
    "        ):\n",
    "            hooks.append(module.register_forward_hook(hook))\n",
    "\n",
    "    # multiple inputs to the network\n",
    "    if isinstance(input_size, tuple):\n",
    "        input_size = [input_size]\n",
    "\n",
    "    # batch_size of 2 for batchnorm\n",
    "    x = [torch.rand(2, *in_size).type(dtype).to(device=device)\n",
    "         for in_size, dtype in zip(input_size, dtypes)]\n",
    "\n",
    "    # create properties\n",
    "    summary = OrderedDict()\n",
    "    hooks = []\n",
    "\n",
    "    # register hook\n",
    "    model.apply(register_hook)\n",
    "\n",
    "    # make a forward pass\n",
    "    # print(x.shape)\n",
    "    model(*x)\n",
    "\n",
    "    # remove these hooks\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    summary_str += \"----------------------------------------------------------------\" + \"\\n\"\n",
    "    line_new = \"{:>20}  {:>25} {:>15}\".format(\n",
    "        \"Layer (type)\", \"Output Shape\", \"Param #\")\n",
    "    summary_str += line_new + \"\\n\"\n",
    "    summary_str += \"================================================================\" + \"\\n\"\n",
    "    total_params = 0\n",
    "    total_output = 0\n",
    "    trainable_params = 0\n",
    "    for layer in summary:\n",
    "        # input_shape, output_shape, trainable, nb_params\n",
    "        line_new = \"{:>20}  {:>25} {:>15}\".format(\n",
    "            layer,\n",
    "            str(summary[layer][\"output_shape\"]),\n",
    "            \"{0:,}\".format(summary[layer][\"nb_params\"]),\n",
    "        )\n",
    "        total_params += summary[layer][\"nb_params\"]\n",
    "\n",
    "        total_output += np.prod(summary[layer][\"output_shape\"])\n",
    "        if \"trainable\" in summary[layer]:\n",
    "            if summary[layer][\"trainable\"] == True:\n",
    "                trainable_params += summary[layer][\"nb_params\"]\n",
    "        summary_str += line_new + \"\\n\"\n",
    "\n",
    "    # assume 4 bytes/number (float on cuda).\n",
    "    total_input_size = abs(np.prod(sum(input_size, ()))\n",
    "                           * batch_size * 4. / (1024 ** 2.))\n",
    "    total_output_size = abs(2. * total_output * 4. /\n",
    "                            (1024 ** 2.))  # x2 for gradients\n",
    "    total_params_size = abs(total_params * 4. / (1024 ** 2.))\n",
    "    total_size = total_params_size + total_output_size + total_input_size\n",
    "\n",
    "    summary_str += \"================================================================\" + \"\\n\"\n",
    "    summary_str += \"Total params: {0:,}\".format(total_params) + \"\\n\"\n",
    "    summary_str += \"Trainable params: {0:,}\".format(trainable_params) + \"\\n\"\n",
    "    summary_str += \"Non-trainable params: {0:,}\".format(total_params -\n",
    "                                                        trainable_params) + \"\\n\"\n",
    "    summary_str += \"----------------------------------------------------------------\" + \"\\n\"\n",
    "    summary_str += \"Input size (MB): %0.2f\" % total_input_size + \"\\n\"\n",
    "    summary_str += \"Forward/backward pass size (MB): %0.2f\" % total_output_size + \"\\n\"\n",
    "    summary_str += \"Params size (MB): %0.2f\" % total_params_size + \"\\n\"\n",
    "    summary_str += \"Estimated Total Size (MB): %0.2f\" % total_size + \"\\n\"\n",
    "    summary_str += \"----------------------------------------------------------------\" + \"\\n\"\n",
    "    # return summary\n",
    "    return summary_str, (total_params, trainable_params)    \n",
    "\n",
    "def define_model(trial,in_features):\n",
    "    '''\n",
    "    Function:\n",
    "    Args:\n",
    "        trial:\n",
    "        in_features(int):the dim of input features\n",
    "    Return:\n",
    "        nn of torch    \n",
    "    Steps:\n",
    "    \n",
    "    '''\n",
    "    # We optimize\n",
    "    n_layers = trial.suggest_int(\"n_layers\",1,9)\n",
    "    layers = []\n",
    "    \n",
    "    in_features = in_features\n",
    "    for i in range(n_layers):\n",
    "        out_features = trial.suggest_int(\"n_units_l{}\".format(i),4,160)\n",
    "        layers.append(nn.Linear(in_features,out_features))\n",
    "        layers.append(nn.BatchNorm1d(out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "        # p = trial.suggest_float(\"dropout_l{}\".format(i),0.05,0.5)\n",
    "        p = trial.suggest_uniform(\"dropout_l{}\".format(i),0.05,0.5)\n",
    "        layers.append(nn.Dropout(p))\n",
    "        \n",
    "        in_features = out_features\n",
    "    # M1\n",
    "    # layers.append(nn.Linear(in_features,CLASSES))\n",
    "    # layers.append(nn.LogSoftmax(dim=1))\n",
    "    \n",
    "    # M2\n",
    "    layers.append(nn.Linear(in_features,1))\n",
    "    layers.append(nn.Sigmoid())\n",
    "    \n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def objective(trial):\n",
    "    '''\n",
    "    Function:\n",
    "    Args:\n",
    "    Return:\n",
    "    Steps:\n",
    "        # Generate the model.\n",
    "        # Generate the optimizers.\n",
    "        # Get the dataset.\n",
    "        # Training of the model.\n",
    "        # Validation of the model.\n",
    "        # Handle pruning based on the intermediate value.\n",
    "    '''\n",
    "    # Generate the model\n",
    "    in_features = 131\n",
    "    model = define_model(trial,in_features).to(DEVICE)\n",
    "    \n",
    "    # Generate the optimizers.\n",
    "    # optimizer_name = trial.suggest_categorical(\"optimizer\",[\"Adam\",\"RMSprop\",\"SGD\"])\n",
    "    lr = trial.suggest_float(\"lr\",1e-5,1e-1,log=True) #?? meaning of log=True\n",
    "    # optimizer = getattr(optim,optimizer_name)(model.parameters(),lr=lr) #??what does func getattr do\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr = lr)\n",
    "   \n",
    "    # Get the dataset.\n",
    "    # global train_loader\n",
    "    global valid_loader\n",
    "    global training_dataset\n",
    "    global Data\n",
    "    global BATCHSIZE\n",
    "    # BATCHSIZE = trial.suggest_int(\"batch\",64,512)\n",
    "    train_loader = Data.DataLoader(\n",
    "                dataset = training_dataset,\n",
    "                batch_size = BATCHSIZE,\n",
    "                shuffle = True\n",
    "            )\n",
    "    valid_loader = valid_loader\n",
    "    \n",
    "    # EPOCHS = trial.suggest_int(\"epoch\",1,40)\n",
    "    # Training of the model.\n",
    "    global EPOCHS\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        for batch_idx,(data,target) in enumerate(train_loader):\n",
    "            # Limiting training data for faster epochs.\n",
    "            if batch_idx * BATCHSIZE >= N_TRAIN_EXAMPLES:\n",
    "                break\n",
    "        \n",
    "            data, target = data.view(data.size(0),-1).to(DEVICE),target.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred = model(data)\n",
    "            # print(\"pred.squeeze() before:\")\n",
    "            # print(pred.squeeze())\n",
    "            \n",
    "            # pred = pred.argmax(dim=1,keepdim=True)\n",
    "            \n",
    "            # print(\"pred:\")\n",
    "            # print(pred)\n",
    "            # print(\"pred.squeeze():\")\n",
    "            # print(pred.squeeze())\n",
    "            # print(\"target:\")\n",
    "            # print(target)\n",
    "            \n",
    "            # print(\"pred.shape:\")\n",
    "            # print(pred.shape)\n",
    "            # print(\"pred.squeeze().shape:\")\n",
    "            # print(pred.squeeze().shape)\n",
    "            # print(\"target:\")\n",
    "            # print(target.shape)\n",
    "            \n",
    "            # loss = F.nll_loss(pred.squeeze(),target)\n",
    "            criterion = nn.BCELoss()\n",
    "            loss = criterion(pred.squeeze(),target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    # Validation of the model.\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        # M1 with batch\n",
    "        for batch_idx,(data,target) in enumerate(valid_loader):\n",
    "            # Limiting validation data.\n",
    "            if batch_idx * BATCHSIZE >= N_VALID_EXAMPLES:\n",
    "                break\n",
    "            data,target = data.view(data.size(0),-1).to(DEVICE),target.to(DEVICE)\n",
    "            pred = model(data)\n",
    "            # Get the index of the max log-probability.\n",
    "            # pred = pred.output.argmax(dim=1,keepdim=True)\n",
    "            # pred = pred.squeeze()\n",
    "            \n",
    "            # print(\"validation:\")\n",
    "            # print(\"pred:\")\n",
    "            # print(pred)\n",
    "            # print(\"target:\")\n",
    "            # print(target)\n",
    "            \n",
    "            # correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "            y_valid_df = pd.DataFrame(target.detach().numpy())\n",
    "\n",
    "            y_pred_df = pd.DataFrame(pred.detach().numpy())\n",
    "            y_result_df = pd.concat([y_valid_df,y_pred_df],axis=1)\n",
    "            # print('batch_idx:{}'.format(batch_idx))\n",
    "            # print('y_result_df:{}'.format(y_result_df))\n",
    "            prauc = prc(y_valid_df,y_pred_df,'MLP',\"cal\")\n",
    "        \n",
    "        # M2 without batch\n",
    "        \n",
    "        \n",
    "    # accuracy = correct/min(len(valid_loader.dataset),N_VALID_EXAMPLES)\n",
    "    \n",
    "    trial.report(prauc,epoch)\n",
    "            \n",
    "    # Handle pruning based on the intermediate value.\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "    \n",
    "    return prauc  \n",
    "\n",
    "def output_str(content,output_mode='file',filename=None,filemode='a'):\n",
    "    if output_mode == 'file':\n",
    "        with open(filename,filemode) as f:\n",
    "            f.write(content)\n",
    "            f.close()\n",
    "    elif output_mode == 'print':\n",
    "        print(content)\n",
    "    return     \n",
    "        \n",
    "def print_file(filename):\n",
    "    with open(filename,'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            # if line != '\\n':\n",
    "            print(line[:-1])\n",
    "            # print('end of line')\n",
    "        f.close()    \n",
    "    return        \n",
    "\n",
    "def clear_file(filename):\n",
    "    with open(filename,'w') as f:\n",
    "        f.write(\"\")\n",
    "        f.close()\n",
    "    return\n",
    "\n",
    "#  anomaly detection\n",
    "def timestamp_generator(year=2020,month=4,period=[1,30]):\n",
    "    if month < 10:\n",
    "        base_date = str(year)+'-0'+str(month)+'-'\n",
    "    else:\n",
    "        base_date = str(year)+'-'+str(month)+'-'\n",
    "    date_list = []\n",
    "    for i in range(period[0],period[1]+1):\n",
    "        date=''\n",
    "        if i < 10:\n",
    "            date =  base_date + '0' + str(i)\n",
    "        else:\n",
    "            date =  base_date + str(i)\n",
    "        date += ' 00:00:00'\n",
    "        date_list.append(date)\n",
    "    return date_list\n",
    "\n",
    "def anomaly_detection_shesd(series):\n",
    "    global time\n",
    "    # start_0 = time.time()\n",
    "    \n",
    "    global date_array\n",
    "    global src_feat_list\n",
    "    global tar_feat_list\n",
    "    # global user_set\n",
    "    feat_series = series[src_feat_list].copy()\n",
    "    ts = pd.DataFrame(feat_series).T.values[0]\n",
    "    ts_data = pd.DataFrame(np.vstack((date_array,ts))).T\n",
    "    ts_data.iloc[:,1] = ts_data.iloc[:,1].astype('float')\n",
    "    \n",
    "    # start_1 = time.time()\n",
    "    results = detect_ts(ts_data, max_anoms=0.49, alpha=0.001, direction='neg', only_last=None)\n",
    "    # time_cost_1 = time.time() - start_1\n",
    "    # print('the time cost of data processing 1: {:.2f}s'.format(time_cost_1))\n",
    "    \n",
    "    for timestamp in results['anoms'].index:\n",
    "        # print(type(time))\n",
    "        timestamp = str(timestamp)\n",
    "        index = int(timestamp[8:10])-1 # -1 cause index start from 0 while day start from 1\n",
    "        # print(index)\n",
    "        tar_feat = tar_feat_list[index]\n",
    "        series[tar_feat] = 1\n",
    "    \n",
    "    # all_time_cost = time.time() - start_0\n",
    "    # print('the time cost of all data processing: {:.2f}s'.format(all_time_cost))\n",
    "    # print('the time percentage of 1: {:.2f}/% '.format(time_cost_1/all_time_cost))\n",
    "    return series    \n",
    "       \n",
    "    # print('***************')\n",
    "\n",
    "# Mutiple Processing\n",
    "def sub_process(args):  # multiple parameters (arguments)\n",
    "    print(f'| inputs:  {args.shape}')\n",
    "    args = args.apply(anomaly_detection_shesd,axis=1)\n",
    "    # time.sleep(1)  # pretend it is a time-consuming operation\n",
    "    \n",
    "    return args\n",
    "\n",
    "\n",
    "def run_pool(process_args):  # main process\n",
    "    # from multiprocessing import Pool\n",
    "    global cpu_worker_num\n",
    "    \n",
    "    # process_args = [(1, 1), (9, 9), (4, 4), (3, 3), ]\n",
    "\n",
    "    # print(f'| inputs:  {process_args}')\n",
    "    # print(f'| inputs:  {process_args}')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with Pool(cpu_worker_num) as p:\n",
    "        outputs = p.map(sub_process, process_args)\n",
    "        \n",
    "    # print(f'| outputs: {outputs}    TimeUsed: {time.time() - start_time:.1f}s    \\n')\n",
    "    print(f'|TimeUsed: {time.time() - start_time:.1f}s    \\n')\n",
    "    result = pd.concat(outputs,axis='index')\n",
    "    # check_format(result)\n",
    "    # check_detail(result)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def sequence_reshape(np_arr,timesteps,mode='Normal'):\n",
    "    # mode='Normal'\n",
    "    print(f'np_arr.shape:{np_arr.shape}')\n",
    "    print(f'timesteps:{timesteps}')\n",
    "    print(f'np_arr.shape[1]//timesteps:{np_arr.shape[1]//timesteps}')\n",
    "    if mode == 'Normal':\n",
    "        np_arr = np_arr.reshape(np_arr.shape[0],timesteps,np_arr.shape[1]//timesteps)    \n",
    "    elif mode == 'Transposition':\n",
    "        np_arr = np_arr.reshape(np_arr.shape[0],np_arr.shape[1]//timesteps,timesteps)\n",
    "    return np_arr\n",
    "\n",
    "# def sequence_reshape_T(np_arr,timesteps):\n",
    "#     print(f'np_arr.shape:{np_arr.shape}')\n",
    "#     print(f'timesteps:{timesteps}')\n",
    "#     print(f'np_arr.shape[1]//timesteps:{np_arr.shape[1]//timesteps}')\n",
    "#     # np_arr = np_arr.reshape(np_arr.shape[0],timesteps,np_arr.shape[1]//timesteps)\n",
    "#     np_arr = np_arr.reshape(np_arr.shape[0],np_arr.shape[1]//timesteps,timesteps)\n",
    "#     return np_arr    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "def feat2featlist(series,feat):\n",
    "    global record_num_list\n",
    "    global counter\n",
    "    feat_list = feat + '_list'\n",
    "    series[feat_list] = []\n",
    "    # if counter<=2:\n",
    "    #     print(series[feat_list])\n",
    "    \n",
    "    for feat_idx in record_num_list:\n",
    "        # if counter<=2:\n",
    "        #     print(series[feat_idx])\n",
    "        series[feat_list].append(series[feat_idx])\n",
    "        \n",
    "    # if counter<=2:\n",
    "    #     print(series[feat_list])    \n",
    "    # counter+=1\n",
    "    return series    \n",
    "\n",
    "def simple_hiveql2df(query_sql):\n",
    "    return spark.sql(query_sql)\n",
    "\n",
    "def hiveql2df(personal_database, table_name, sql_col_list, limit_mode, where_key='', where_val=''):\n",
    "    #HiveQL2SparkDF\n",
    "    #  Table Selection\n",
    "    query_table = personal_database + table_name\n",
    "    query_sql = \"\"\n",
    "\n",
    "    #  Columns Selection\n",
    "    # if sql_col_list != []:\n",
    "    key,cols = feats2cols(sql_col_list)\n",
    "        #  SQL Combination\n",
    "    if where_key == '' and where_val == '':\n",
    "        query_sql = simple_sql.format(cols,query_table,limit_mode)\n",
    "    else:    \n",
    "        query_sql = condition_sql.format(cols,query_table,where_key,where_val,limit_mode)\n",
    "    \n",
    "    print(f'query_sql:{query_sql}')\n",
    "    \n",
    "    spark_df = spark.sql(query_sql) #  Execute SQL/HiveQL2SparkDF\n",
    "    \n",
    "    # pandas_df = spark_df.toPandas() #SparkDF2PandasDF\n",
    "    \n",
    "    # return spark_df,pandas_df\n",
    "    \n",
    "    # global spark_qk\n",
    "    # spark_df = spark_qk.sql(query_sql) #  Execute SQL/HiveQL2SparkDF\n",
    "    \n",
    "    return spark_df\n",
    "    \n",
    "def data_labeling(user_set,src_col,tar_col):\n",
    "    # user_set['is_halted_next_month'] = 0\n",
    "    # user_set['eff_date'] = user_set['eff_date'].fillna('')\n",
    "    # rule = (user_set['eff_date'] != '')\n",
    "    # index = np.arange(user_set.shape[0])[rule]\n",
    "    # user_set.loc[index,'is_halted_next_month'] = 1\n",
    "    # return user_set\n",
    "    user_set[tar_col] = 0\n",
    "    user_set[src_col] = user_set[src_col].fillna('')\n",
    "    rule = (user_set[src_col] != '')\n",
    "    index = np.arange(user_set.shape[0])[rule]\n",
    "    user_set.loc[index,tar_col] = 1\n",
    "    return user_set    \n",
    "    \n",
    "def data_labeling_spark(df,src_col,tar_col):\n",
    "    #1'\n",
    "    # non_churners = df.where(col(\"eff_date\") != '')\n",
    "    # non_churners = non_churners.withColumn(\"is_halted_next_month\", lit(0))\n",
    "    # print('non_churners:*******')\n",
    "    # check_spark_df(non_churners)\n",
    "    \n",
    "    # churners = df.where(col(\"eff_date\") == '')\n",
    "    # churners = churners.withColumn(\"is_halted_next_month\", lit(1))\n",
    "    # print('churners:*******')\n",
    "    # check_spark_df(churners)\n",
    "    \n",
    "    # df = non_churners.union(churners)\n",
    "    \n",
    "    #2'\n",
    "    # non_churners = df.filter(isnull('effdate'))\n",
    "    # non_churners = non_churners.withColumn(\"is_halted_next_month\", lit(0))\n",
    "    # print('non_churners:*******')\n",
    "    # check_spark_df(non_churners)\n",
    "    \n",
    "    # churners = df.filter(isNotNull('effdate'))\n",
    "    # churners = churners.withColumn(\"is_halted_next_month\", lit(1))\n",
    "    # print('churners:*******')\n",
    "    # check_spark_df(churners)\n",
    "    \n",
    "    # df = non_churners.union(churners)\n",
    "    \n",
    "    #3'\n",
    "    # df = df.withColumn(\"is_halted_next_month\", lit(0))\n",
    "    df = df.select('*',when(isnull(src_col),0).otherwise(1).alias(tar_col))\n",
    "    print('ic:*******')\n",
    "    # check_spark_df(df)    \n",
    "    return df\n",
    "\n",
    "def scale4features(X_train,X_valid):\n",
    "    '''\n",
    "    X_train&X_valid:pandas.df\n",
    "    \n",
    "    '''\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    \n",
    "    X_train = pd.DataFrame(scaler.transform(X_train))\n",
    "    X_valid = pd.DataFrame(scaler.transform(X_valid))\n",
    "    \n",
    "    # fill after scale equals mean #totest\n",
    "    X_train.fillna(0,inplace=True)\n",
    "    X_valid.fillna(0,inplace=True)\n",
    "    return X_train,X_valid\n",
    "\n",
    "def pca4features(X_train,X_valid):\n",
    "    pca = PCA(n_components = pca_ratio,svd_solver='auto')\n",
    "    pca.fit(X_train)\n",
    "    X_train = pd.DataFrame(pca.transform(X_train))\n",
    "    \n",
    "    # PCA-X_test\n",
    "    X_valid = pd.DataFrame(pca.transform(X_valid))\n",
    "    return X_train, X_valid\n",
    "    \n",
    "def data_feat_filter(data,feat_list,label_feat):\n",
    "    '''\n",
    "    data is a pandas dataframe\n",
    "    feat_list is a list of useless feat\n",
    "    '''\n",
    "    print(\"data_feat_filter:\")\n",
    "    data.drop(columns=feat_list,axis='columns',inplace=True)\n",
    "    \n",
    "    # check_all(data)\n",
    "    print(f\"label_feat:{label_feat}\")\n",
    "    \n",
    "    label = data.pop(label_feat)\n",
    "    feature = data\n",
    "    \n",
    "    # check_detail(feature)\n",
    "    \n",
    "    print(f'feature.shape:{feature.shape}')\n",
    "    print(f'label.shape:{label.shape}')\n",
    "    check_detail(label)\n",
    "    \n",
    "    return feature,label\n",
    "\n",
    "def data_feat_selecter(data,feat_list,label_feat):\n",
    "    '''\n",
    "    data is a pandas dataframe\n",
    "    feat_list is a list of useless feat\n",
    "    '''\n",
    "    feat_list.append(label_feat)\n",
    "    data = data[feat_list]\n",
    "    \n",
    "    label = data.pop(label_feat)\n",
    "    feature = data\n",
    "    \n",
    "    print(f'feature.shape:{feature.shape}')\n",
    "    print(f'label.shape:{label.shape}')\n",
    "#     check_all(label)\n",
    "    \n",
    "    return feature,label\n",
    "\n",
    "def data_feat_extraction(user_set,feat_list):\n",
    "    global label_feat\n",
    "    feat_list.append(label_feat)\n",
    "\n",
    "    dataset = user_set[feat_list]\n",
    "    label = dataset.pop(label_feat)\n",
    "    feature = dataset\n",
    "    print(f'feature.shape:{feature.shape}')\n",
    "    print(f'label.shape:{label.shape}')\n",
    "\n",
    "    return feature,label\n",
    "\n",
    "def metrics_output(metrics,mode='numerical'):\n",
    "    mean_metric, roc_auc, pr_auc, empirical_best_f1, best_f1_rec, best_f1_pre ,tn ,fp ,fn ,tp =metrics\n",
    "    start_content = \"*******************************\\n\"\n",
    "    end_content = \"*******************************\\n\"\n",
    "    metrics_content = None\n",
    "    if mode == 'numerical':\n",
    "        mean_metric_content = '{:.4f}\\n'.format(mean_metric)\n",
    "        auc_content = '{:.4f}\\n'.format(roc_auc)\n",
    "        prauc_content = '{:.4f}\\n'.format(pr_auc)\n",
    "        best_content = '{:.4f}\\n'.format(empirical_best_f1)+'{:.4f}\\n'.format(best_f1_rec)+'{:.4f}\\n'.format(best_f1_pre)\n",
    "        # print(tn+fp+fn+tp)\n",
    "        sample_content = str(tn)+'   '+str(fp)+'\\n'+str(fn)+'   '+str(tp)+'\\n'\n",
    "        # time_content =\n",
    "        metrics_content = start_content+mean_metric_content+auc_content+prauc_content+best_content+sample_content+end_content\n",
    "        # output_str(metrics_content,'print')\n",
    "    elif mode == 'detail':\n",
    "        mean_metric_content = '{:.2f}\\n'.format(mean_metric)\n",
    "        auc_content = 'ROC-AUC'.ljust(17)+':{:.2f}\\n'.format(roc_auc)\n",
    "        prauc_content = 'PR-AUC'.ljust(17)+':{:.2f}\\n'.format(pr_auc)\n",
    "        best_content = 'EMPIRICAL-BEST-F1'.ljust(17)+':{:.2f}\\n'.format(empirical_best_f1)+'BEST-F1-RECALL'.ljust(17)+':{:.2f}\\n'.format(best_f1_rec)+'BEST-F1-PRECISION:{:.2f}\\n'.format(best_f1_pre)+'BEST-F1-THRESHOLD:{:.3f}\\n'.format(best_f1_threshold)\n",
    "        print('validation_sample_num:{}'.format(tn+fp+fn+tp))\n",
    "        sample_content = 'TN     FP\\n'+str(tn).ljust(7)+str(fp).ljust(7)+'\\n'+str(fn).ljust(7)+str(tp).ljust(7)+'\\n'+'FN     TP\\n'\n",
    "        # time_content =\n",
    "        metrics_content = start_content+auc_content+prauc_content+best_content+sample_content+end_content\n",
    "        # output_str(metrics_content,output_mode,output_filename,'a')\n",
    "    elif mode == 'group':\n",
    "        best_content = '{:.2f}\\n'.format(best_f1_rec)+'{:.2f}\\n'.format(best_f1_pre)\n",
    "        metrics_content = best_content\n",
    "    output_str(metrics_content,'print')    \n",
    "    return    \n",
    "\n",
    "def feature_std_np(feat):\n",
    "    mu = np.mean(feat, axis=0)\n",
    "    sigma = np.std(feat, axis=0)\n",
    "    return (feat - mu) / sigma\n",
    "\n",
    "def data_transformation(dataset,mode='all'):\n",
    "    print(f'*******start of function:data_transformation mode:{mode}*******')\n",
    "    dataset_df = None\n",
    "    dataset_np = None\n",
    "    dataset_tensor = None\n",
    "    \n",
    "    X_train,y_train,X_valid,y_valid = dataset\n",
    "    \n",
    "    y_train = pd.DataFrame(y_train).astype('int')\n",
    "    y_valid = pd.DataFrame(y_valid).astype('int')\n",
    "    \n",
    "    X_train_df = X_train.copy()\n",
    "    y_train_df = y_train.copy()\n",
    "    X_valid_df = X_valid.copy()\n",
    "    y_valid_df = y_valid.copy()\n",
    "    # print(f'X_train_df.shape:{X_train_df.shape}')\n",
    "    \n",
    "    X_train_np = X_train_df.values\n",
    "    X_valid_np = X_valid_df.values\n",
    "    y_train_np = y_train_df.values\n",
    "    y_valid_np = y_valid_df.values\n",
    "    print('X_train_np.shape:{}'.format(X_train_np.shape))\n",
    "    print('X_valid_np.shape:{}'.format(X_valid_np.shape))\n",
    "    print('y_train_np.shape:{}'.format(y_train_np.shape))\n",
    "    print('y_valid_np.shape:{}'.format(y_valid_np.shape))    \n",
    "    \n",
    "    if mode == 'all':\n",
    "        # standardization\n",
    "        X_train_df,X_valid_df = scale4features(X_train_df,X_valid_df)\n",
    "        print(\"scaling:\")\n",
    "        print(f'X_train_df.shape:{X_train_df.shape}')\n",
    "        print(f'X_valid_df.shape:{X_valid_df.shape}')\n",
    "        \n",
    "        # PCA\n",
    "        X_train_df,X_valid_df = pca4features(X_train_df,X_valid_df)\n",
    "        print(\"pca:\")\n",
    "        print(f'X_train_df.shape:{X_train_df.shape}')\n",
    "        print(f'X_valid_df.shape:{X_valid_df.shape}')\n",
    "        \n",
    "        # for sklearn\n",
    "        dataset_df = (X_train_df,y_train_df,X_valid_df,y_valid_df)\n",
    "        \n",
    "        X_train_np = X_train_df.values\n",
    "        X_valid_np = X_valid_df.values\n",
    "        y_train_np = y_train_df.values\n",
    "        y_valid_np = y_valid_df.values\n",
    "        \n",
    "    elif mode == 'sequence':\n",
    "        global month_days\n",
    "        print('start transforming sequential data')\n",
    "        # print(f'month_days:{month_days}')\n",
    "        \n",
    "        print('before X_train_np.shape:{}'.format(X_train_np.shape))\n",
    "        print('before X_valid_np.shape:{}'.format(X_valid_np.shape))\n",
    "        \n",
    "        X_train_np = sequence_reshape(X_train_np,month_days)\n",
    "        X_valid_np = sequence_reshape(X_valid_np,month_days)\n",
    "        \n",
    "        # X_train_np = sequence_reshape(X_train_np, month_days, 'Normal')\n",
    "        # X_valid_np = sequence_reshape(X_valid_np, month_days, 'Normal')\n",
    "        \n",
    "        print('after X_train_np.shape:{}'.format(X_train_np.shape))\n",
    "        print('after X_valid_np.shape:{}'.format(X_valid_np.shape))\n",
    "        \n",
    "        # std\n",
    "        seq_num = X_train_np.shape[2]\n",
    "        for i in range(seq_num):\n",
    "            X_train_np[:,:,i] = feature_std_np(X_train_np[:,:,i])\n",
    "            X_valid_np[:,:,i] = feature_std_np(X_valid_np[:,:,i])\n",
    "        \n",
    "        print(f\"X_train_np[:,:,0]:{X_train_np[:,:,0]}\")\n",
    "        \n",
    "    elif mode == 'plain':\n",
    "        # standardization\n",
    "        X_train_df,X_valid_df = scale4features(X_train_df,X_valid_df)\n",
    "        print(\"scaling:\")\n",
    "        print(f'X_train_df.shape:{X_train_df.shape}')\n",
    "        print(f'X_valid_df.shape:{X_valid_df.shape}')\n",
    "        \n",
    "        # for sklearn\n",
    "        dataset_df = (X_train_df,y_train_df,X_valid_df,y_valid_df)\n",
    "        \n",
    "        X_train_np = X_train_df.values\n",
    "        X_valid_np = X_valid_df.values\n",
    "        y_train_np = y_train_df.values\n",
    "        y_valid_np = y_valid_df.values      \n",
    "        \n",
    "    elif mode == 'none':\n",
    "        dataset_df = (X_train_df,y_train_df,X_valid_df,y_valid_df)\n",
    "        \n",
    "        X_train_np = X_train_df.values\n",
    "        X_valid_np = X_valid_df.values\n",
    "        y_train_np = y_train_df.values\n",
    "        y_valid_np = y_valid_df.values           \n",
    "        \n",
    "    \n",
    "    \n",
    "    # for keras\n",
    "    dataset_np = (X_train_np,y_train_np,X_valid_np,y_valid_np)\n",
    "    \n",
    "    # for pytorch\n",
    "    X_train_tensor = torch.FloatTensor(X_train_np)\n",
    "    y_train_tensor = torch.FloatTensor(np.reshape(y_train_np,(-1,)))\n",
    "    X_valid_tensor = torch.FloatTensor(X_valid_np)\n",
    "    y_valid_tensor = torch.FloatTensor(np.reshape(y_valid_np,(-1,)))\n",
    "    dataset_tensor = (X_train_tensor,y_train_tensor,X_valid_tensor,y_valid_tensor)\n",
    "    \n",
    "    print(f'*******end of function:data_transformation mode:{mode}*******')\n",
    "    # data_time = time_count(\"Data Prepare\",start_time)\n",
    "    return dataset_df,dataset_np,dataset_tensor\n",
    "\n",
    "def performance_evaluation(y_valid_df,y_hat_df,y_result_df, best_threshold=None, part_gran = 2000):\n",
    "    # y_valid = y_valid_df.values.flatten()\n",
    "    # y_hat = y_valid_df.values.flatten()\n",
    "    # print(f\"performance_evaluation-y_valid:{y_valid[:10]}\")\n",
    "    # print(f\"performance_evaluation-y_hat:{y_hat[:10]}\")\n",
    "    # roc_auc = auc(y_valid,y_hat,'plot')\n",
    "    # roc_auc = auc(y_valid_df,y_hat_df,'plot')\n",
    "    roc_auc = auc(y_valid_df,y_hat_df,'cal')\n",
    "    auc_content = '{:.4f}\\n'.format(roc_auc)\n",
    "    \n",
    "    recall,precision,thresholds,pr_auc = prauc(y_valid_df,y_hat_df,'cal')\n",
    "    # recall,precision,thresholds,pr_auc = prauc(y_valid_df,y_hat_df,'plot')\n",
    "    prauc_content = '{:.4f}\\n'.format(pr_auc)\n",
    "    \n",
    "    theory_best_f1,best_f1_threshold = cal_best_f1(recall,precision,thresholds)\n",
    "    print(f'best_f1_threshold:{best_f1_threshold}')\n",
    "    \n",
    "    # topu_metrics(y_valid_df,y_hat_df,best_f1_threshold,user_num_ratio) #todo: write to the file\n",
    "    # topu_metrics(y_valid_df,y_hat_df,best_f1_threshold,2000) #todo: write to the file\n",
    "    if best_threshold is not None:\n",
    "        topu_metrics(y_valid_df,y_hat_df,best_threshold, part_gran) #todo: write to the file\n",
    "        y_result_df.apply(confusion_matrix_count,args=(best_threshold,),axis=1)\n",
    "    else:\n",
    "        topu_metrics(y_valid_df,y_hat_df,best_f1_threshold, part_gran) #todo: write to the file\n",
    "        y_result_df.apply(confusion_matrix_count,args=(best_f1_threshold,),axis=1)\n",
    "    \n",
    "    \n",
    "    empirical_best_f1,best_f1_rec,best_f1_pre = class_metrics()\n",
    "    # print(f'best_f1_rec:{best_f1_rec}')\n",
    "    # print(f'best_f1_pre:{best_f1_pre}')\n",
    "    \n",
    "    mean_metric = (roc_auc+pr_auc+empirical_best_f1)/3\n",
    "    metrics = (mean_metric, roc_auc, pr_auc, empirical_best_f1, best_f1_rec, best_f1_pre ,tn ,fp ,fn ,tp)\n",
    "    \n",
    "    confusion_matrix_reset()\n",
    "    return metrics, best_f1_threshold\n",
    "\n",
    "def bar_plot(ylabel_name,data_list,label_list):\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    x = range(len(data_list))\n",
    "    ax.bar(x,data_list)\n",
    "    # ax.legend(model_list)\n",
    "    \n",
    "    ax.set_xlabel('models')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(label_list)\n",
    "    \n",
    "    ax.set_ylabel(ylabel_name)\n",
    "    plt.savefig('./'+ylabel_name)\n",
    "    # fig.tight_layout()\n",
    "\n",
    "    plt.show()    \n",
    "  \n",
    "def check_spark_df(spark_df,show_rows=10,col_list=['*'],mode='detail'):\n",
    "    n_rows = spark_df.count()\n",
    "    cols = spark_df.columns\n",
    "    n_cols = len(cols)\n",
    "    print(f'shape:{n_rows,n_cols}')\n",
    "    # print(cols)\n",
    "    \n",
    "    if mode == 'detail':\n",
    "        spark_df.printSchema()\n",
    "        spark_df.select(col_list).show(show_rows)\n",
    "\n",
    "def print_spark_df(spark_df,mode = 'spark'):\n",
    "    if mode == \"spark\":\n",
    "        n_rows = spark_df.count()\n",
    "        spark_df.show(n_rows)\n",
    "    elif mode == \"list\":\n",
    "        for col in spark_df.columns:\n",
    "            print(spark_df.select(col).rdd.map(lambda x : x[0]).collect())\n",
    "\n",
    "def env_test(test_path='./'):\n",
    "    from multiprocessing import cpu_count\n",
    "     \n",
    "     \n",
    "    print(\"CPU的核数为：{}\".format(cpu_count()))\n",
    "    # print(type(cpu_count()))\n",
    "    \n",
    "    import os\n",
    "    \n",
    "    # 获取/目录的磁盘信息\n",
    "    info = os.statvfs(test_path)\n",
    "    \n",
    "    free_size = info.f_bsize * info.f_bavail / 1024 / 1024 /1024\n",
    "    # print(f'可用磁盘空间:{free_size}MB')\n",
    "    print(f'可用磁盘空间:{free_size:.1f}GB')\n",
    "    \n",
    "    #常用的：\n",
    "    import psutil\n",
    "    import os\n",
    "    info = psutil.virtual_memory()\n",
    "    # print(f'内存使用：{psutil.Process(os.getpid()).memory_info().rss/ 1024 / 1024 /1024}GB')\n",
    "    print(f'总内存：{info.total/ 1024 / 1024 /1024:.1f}GB')\n",
    "    print(f'使用内存占比：{info.percent}%',)\n",
    "    # print(f'cpu个数：{psutil.cpu_count()}')    \n",
    "\n",
    "\n",
    "def mkdir(path):\n",
    "    isExisted = os.path.exists(path)\n",
    "    if not isExisted:\n",
    "        os.makedirs(path)\n",
    "        print(f'{path} has been made')\n",
    "    else:\n",
    "        print(f'{path} has has already existed')\n",
    "\n",
    "def get_filename(user_type,table_type,date,version,file_type='.parquet'):\n",
    "    return user_type+'_'+table_type+'_'+date+'_'+version+file_type\n",
    "    \n",
    "# def get_database(platform,if_self_built,table_type):\n",
    "#     global group_project_database\n",
    "    \n",
    "#     if platform == 'group':\n",
    "#         if if_self_built == True:\n",
    "#             return group_project_database\n",
    "#         else:\n",
    "#             return group_project_database\n",
    "#     else:\n",
    "#         if table_type == 'halt':\n",
    "#             return None\n",
    "        \n",
    "#     return None\n",
    "    \n",
    "def get_table(user,table_type,date):\n",
    "    global table_dict\n",
    "    return table_dict[user+'_'+table_type+'_'+date]\n",
    "\n",
    "def get_database(user,table_type,date):\n",
    "    global database_dict\n",
    "    return database_dict[user+'_'+table_type+'_'+date]\n",
    "    \n",
    "def get_full_table(user,table_type,date):\n",
    "    table = get_database(user,table_type,date)+get_table(user,table_type,date)\n",
    "    return table\n",
    "\n",
    "def hive2info(month):\n",
    "    '''\n",
    "    table = table_ic_info_2102\n",
    "    '''\n",
    "    global no_limit\n",
    "    \n",
    "    table = get_table('ic','info',month)\n",
    "    database = get_database('ic','info',month)\n",
    "    \n",
    "    where_key = 'is_hlwk_offer'\n",
    "    where_val = \"'1' AND prod_status_name='正常' AND std_stgy_sale_dept_cd='12002'\"\n",
    "    # where_val = \"'1' AND std_stgy_sale_dept_cd='12002'\"\n",
    "    sql_col_list = ['*']\n",
    "    \n",
    "    \n",
    "    ic_df = hiveql2df(database,table,sql_col_list,no_limit,where_key,where_val)\n",
    "    # change col name\n",
    "    ic_df = ic_df.withColumnRenamed(\"prd_inst_id\",\"prod_inst_id\")\n",
    "    \n",
    "    check_spark_df(ic_df)\n",
    "    return ic_df\n",
    "\n",
    "def hive2halt(month):\n",
    "    '''\n",
    "    table = table_ic_halted_2101\n",
    "    '''\n",
    "    global no_limit,platform\n",
    "    \n",
    "    table = get_table('ic','halt',month)\n",
    "    database = get_database('ic','halt',month)\n",
    "    \n",
    "    where_key = ''\n",
    "    where_val = \"\"\n",
    "    if platform == 'group':\n",
    "        # where_key = 'is_hlwk_offer'\n",
    "        # where_val = \"'1' AND standard_code='A05' AND cur_status='00A' AND month_id='202011'\" # 202011/2012/2101/2102 for group\n",
    "        # where_val = \"'00A' AND exp_date='A05' AND month_id='202106'\" # 202106/07/08 for group\n",
    "    \n",
    "        where_key = 'standard_code'\n",
    "        where_val = \"'A05' AND cur_status='00A' AND month_id='\" +month +\"'\" # 202011/2012/2101/2102 for group    \n",
    "    else:\n",
    "        where_key = 'is_hlwk_offer'\n",
    "        where_val = \"'1' AND standard_code='A05' and status_cd='00A'\"\n",
    "        # where_val = \"'1' AND standard_code='A05' AND status_cd='00A'\" # 202106/07/08 for old\n",
    "    sql_col_list = ['prod_inst_id','eff_date']\n",
    "    \n",
    "    halted_df = hiveql2df(database,table,sql_col_list,no_limit,where_key,where_val)\n",
    "    halted_df = halted_df.withColumnRenamed(\"eff_date\",\"eff_date_\"+month[2:])\n",
    "    \n",
    "    check_spark_df(halted_df)\n",
    "    \n",
    "    return halted_df\n",
    "\n",
    "# sequential\n",
    "# def cal_sequence_feat_spark(id_table,kind,seq_table,col_list,month='202011'):\n",
    "#     '''\n",
    "#     Input:\n",
    "#         table of prod_inst_id;\n",
    "#         kind of sequential data;\n",
    "#         table of sequential data;\n",
    "#         list of extracted feature;\n",
    "#         month\n",
    "#     Output:\n",
    "#         spark dataframe of sequential statistics of some prod_inst_id set\n",
    "#     '''\n",
    "#     seq_df = None\n",
    "#     period = month_period_dict[int(month[-2:])]\n",
    "#     start = time.time()\n",
    "\n",
    "#     constraint_key = ''\n",
    "#     if kind == 'cdr':\n",
    "#         constraint_key = 'day_id'\n",
    "#     elif kind == 'traffic':\n",
    "#         # constraint_key = 'p_day_id'\n",
    "#         constraint_key = 'day_id'\n",
    "       \n",
    "#     for i in range(1,period+1,1):\n",
    "#     # for i in range(1,3,1):\n",
    "#         # Load raw table data\n",
    "#         # Table Selection\n",
    "#         query_table = seq_table\n",
    "        \n",
    "#         #  Columns Selection\n",
    "#         sql_feat_list = col_list\n",
    "#         key,cols = feats2cols(sql_feat_list)\n",
    "        \n",
    "#         if i < 10:\n",
    "#             day = month + '0' + str(i)\n",
    "#         else:\n",
    "#             day = month + str(i)\n",
    "#         constraint_value = day\n",
    "#         # limit_mode = 'limit 100'\n",
    "        \n",
    "#         query_sql = ''\n",
    "#         if id_table == '':\n",
    "#             query_sql = condition_sql.format(cols,query_table,constraint_key,constraint_value,limit_mode)\n",
    "#         else:\n",
    "#             # query_sql = \"SELECT {} FROM {} WHERE {} = {} {}\".format(cols,query_table,constraint_key,constraint_value,limit_mode)\n",
    "#             query_sql = 'SELECT a.'+cols+' FROM '+query_table+' a,'+id_table+' b WHERE a.'+constraint_key+' = '+constraint_value+' AND a.prod_inst_id=b.prod_inst_id'\n",
    "#         # print(f'query_sql:{query_sql}')    \n",
    "        \n",
    "#         #  Execute SQL\n",
    "#         spark_df = spark.sql(query_sql) # type:pyspark.sql.dataframe.DataFrame\n",
    "        \n",
    "#         # spark version\n",
    "#         daily_statistics = records2statistics_spark(spark_df,day,kind)\n",
    "        \n",
    "#         if seq_df is None:\n",
    "#             seq_df = daily_statistics\n",
    "#         else:\n",
    "#             join_type = 'outer'\n",
    "#             join_expression = seq_df[\"prod_inst_id\"] == daily_statistics['prod_inst_id']\n",
    "#             # sequence_set = sequence_set.join(daily_statistics,join_expression,join_type)\n",
    "#             seq_df = seq_df.join(daily_statistics,\"prod_inst_id\",join_type)\n",
    "                \n",
    "#     # check_spark_df(seq_df)\n",
    "#     print(f'the time cost of function: {time.time()-start:.1f}s')\n",
    "#     # print(f'sys.getsizeof(seq_df):{sys.getsizeof(seq_df)/1024/1024:.1f} MB') # not correct\n",
    "    \n",
    "#     # sequence_set.fillna(value=0,inplace=True) # forbidden to deal in model\n",
    "\n",
    "#     return seq_df\n",
    "\n",
    "def sequential_statistics(seq_df,stats='all'):\n",
    "    '''\n",
    "    Input:\n",
    "        (seq_df,spark dataframe)\n",
    "        (stats,string)\n",
    "    Output:\n",
    "        (stats_df,pandas df)\n",
    "    '''\n",
    "    stats_df = seq_df.describe().toPandas()\n",
    "    return stats_df\n",
    "\n",
    "def print_sequential_statistics(ic_df_list,month_list):\n",
    "    for month,ic_df in zip(month_list,ic_df_list):\n",
    "        print('------------------'+month+'------------------')\n",
    "        stats_df = sequential_statistics(ic_df)\n",
    "        month_days = month_period_dict[int(month[-2:])]\n",
    "        \n",
    "        byte_in_list = period_feat('byte_in',month,month_days)\n",
    "        new_li = []\n",
    "        li = list(stats_df[byte_in_list].values)\n",
    "        for i in range(len(li)):\n",
    "            new_li.append(list(li[i]))\n",
    "        # print(new_li)\n",
    "        # byte_out_list = period_feat('byte_out',month,month_days)\n",
    "        # print(stats_df[byte_out_list].values)\n",
    "        \n",
    "        # duration_list = period_feat('duration',month,month_days)\n",
    "        # print(stats_df[duration_list].values)\n",
    "        \n",
    "        # record_num_list = period_feat('record_num',month,month_days)\n",
    "        # print(stats_df[record_num_list].values)\n",
    "        \n",
    "        # return stats_df[byte_in_list].values\n",
    "\n",
    "def num2decimal_str(num):\n",
    "    if num < 10:\n",
    "        return '0'+str(num)\n",
    "    elif num>=10 and num<100:\n",
    "        return str(num)\n",
    "    else:\n",
    "        print('num2decimal_str error')\n",
    "    return     \n",
    "\n",
    "def test_param_generation(year,month,incre=1):\n",
    "    test_year=0\n",
    "    test_month = 0\n",
    "    if month == 12:\n",
    "        test_month = incre\n",
    "        test_year = year + incre\n",
    "    else:\n",
    "        test_month = month + incre\n",
    "        test_year = year\n",
    "    return test_year, test_month\n",
    "\n",
    "def month_generation(year,month,incre=1):\n",
    "    test_year=0\n",
    "    test_month = 0\n",
    "    if month == 12:\n",
    "        test_month = incre\n",
    "        test_year = year + incre\n",
    "    else:\n",
    "        test_month = month + incre\n",
    "        test_year = year\n",
    "    \n",
    "    train_month = str(year)+num2decimal_str(month)\n",
    "    test_month = str(test_year)+num2decimal_str(test_month)\n",
    "    return train_month, test_month\n",
    "\n",
    "def boxplot_sequential_data(seq):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 9))\n",
    "    labels = ['01','','','','','06','','','','','11','','','','','16','','','','','21','','','','','26','','','','30']\n",
    "    \n",
    "    seq = seq.values\n",
    "    stats = cbook.boxplot_stats(seq, labels=labels, bootstrap=10000)\n",
    "    print(f'list(stats[0]):{list(stats[0])}')\n",
    "    bplot1 = ax.bxp(stats, showmeans=True, meanline=True)\n",
    "    \n",
    "    ticks = [i for i in range(1,31,1)]\n",
    "    ax.set_xticks(ticks)\n",
    "    labels = ['01','','','','','06','','','','','11','','','','','16','','','','','21','','','','','26','','','','30']\n",
    "    ax.set_xticklabels(labels=labels)\n",
    "    fig.autofmt_xdate()\n",
    "    \n",
    "    ax.set_xlabel('Dates of a month')\n",
    "    ax.set_ylabel('#. traffic records')\n",
    "    plt.show()\n",
    "\n",
    "def weeknum(dayname):\n",
    "    if dayname == 'Monday':   return 0\n",
    "    if dayname == 'Tuesday':  return 1\n",
    "    if dayname == 'Wednesday':return 2\n",
    "    if dayname == 'Thursday': return 3\n",
    "    if dayname == 'Friday':   return 4\n",
    "    if dayname == 'Saturday': return 5\n",
    "    if dayname == 'Sunday':   return 6\n",
    "\n",
    "def generate_date(year, month, week, whichDayYouWant):\n",
    "    d = date(year, month, 1 + (week - 1)*7)\n",
    "    # print(d)\n",
    "\n",
    "    delta = timedelta(days = (weeknum(whichDayYouWant) - d.weekday()) % 7)\n",
    "    # print(f\"delta:{delta}\")\n",
    "    d += delta\n",
    "    \n",
    "    s = d.strftime('%Y%m%d')\n",
    "\n",
    "    return s\n",
    "\n",
    "  \n",
    "    \n",
    "def extract_sequential_list(ic_df_list,month_list,col):\n",
    "    '''\n",
    "    Function:\n",
    "    Args:\n",
    "    Return:\n",
    "    Steps:\n",
    "    '''\n",
    "    li = []\n",
    "    for ic_df,month in zip(ic_df_list,month_list):\n",
    "        month_days = month_period_dict[int(month[-2:])]\n",
    "        period_feat_list = period_feat(col,month,month_days)\n",
    "        tmp_pd_df = ic_df[period_feat_list].toPandas()\n",
    "        tmp_pd_df.fillna(0,inplace=True)\n",
    "        li.append(tmp_pd_df)\n",
    "    return li\n",
    "\n",
    "def get_train_feat(month,test_month):\n",
    "    # constant feat\n",
    "    base_feat = [ 'lan_id',   'innet_dur', 'balance', 'night_latitude', 'night_longitude',  'day_latitude', 'day_longitude', 'offer_grade', 'user_terminal_dur', 'reg_term_price',   'exceed_flow_l1m', 'surplus_flow_l1m', 'total_offer_flow_l1m', 'use_dura_l1m', 'use_offer_percent', 'use_flow_l1m', 'exceed_flow_l2m', 'surplus_flow_l2m', 'use_dura_l2m', 'use_flow_l2m', 'l1_use_offer_percent', 'use_dura_avg_3m', 'use_flow_avg_3m', 'surplus_flow_cm', 'use_dura_cm', 'use_flow_cm',  'games_app_days_l1m', 'music_app_days_l1m', 'video_app_days_l1m', 'shopping_app_days_l1m', 'own_age', 'avg_over_flow', 's_stop_cnt_sum_3m', 'd_stop_cnt_sum_3m', 'hlwk_offer_aim_flow_cm', 'std_stgy_sale_dept_cd', 'partition_nf_amount_1m', 'l2_use_offer_percent', 'rel_amount']\n",
    "    \n",
    "    # base_feat.remove(\"balance\")\n",
    "    \n",
    "    magic_feat_list =['rel_amount','hlwk_offer_aim_flow_cm']\n",
    "    magic_feat_list_v2 =['rel_amount_round','hlwk_offer_aim_flow_cm_round']\n",
    "    magic_feat_list_v3 =['rel_amount_round_target','hlwk_offer_aim_flow_cm_round_target']\n",
    "    \n",
    "    month_days = month_period_dict[int(month[-2:])]\n",
    "    # print(f\"month:{month},month_days:{month_days}\")\n",
    "    # sequential feat\n",
    "    traffic_feat_list = period_feat('byte_in',month,month_days)\n",
    "    traffic_feat_list.extend(period_feat('byte_out',month,month_days))\n",
    "    traffic_feat_list.extend(period_feat('duration',month,month_days))\n",
    "    traffic_feat_list.extend(period_feat('record_num',month,month_days))\n",
    "    \n",
    "   \n",
    "    \n",
    "    cdr_feat_list = period_feat('calling_duration',month,month_days)\n",
    "    cdr_feat_list.extend(period_feat('calling_record_num',month,month_days))\n",
    "    cdr_feat_list.extend(period_feat('called_duration',month,month_days))\n",
    "    cdr_feat_list.extend(period_feat('called_record_num',month,month_days))\n",
    "    \n",
    "    sequence_feat_list = []\n",
    "    sequence_feat_list.extend(traffic_feat_list)\n",
    "    sequence_feat_list.extend(cdr_feat_list)\n",
    "    \n",
    "    anomaly_feat_list = period_feat('byte_in_anomaly',month,month_days)\n",
    "    sequence_feat_list.extend(anomaly_feat_list)\n",
    "    \n",
    "    traffic_statistics_feat = ['byte_in', 'byte_out', 'duration', 'record_num']\n",
    "    entropy_feat_list = []\n",
    "    anomaly_feat_list = []\n",
    "    \n",
    "    for ele in traffic_statistics_feat:\n",
    "        entropy_feat_list.append(month[2:]+\"_\"+ele+\"_binned_entropy_7\")\n",
    "    # for ele in traffic_statistics_feat:\n",
    "    #     entropy_feat_list.append(month+\"_\"+ele+\"_list_entropy\")\n",
    "    #     anomaly_feat_list.append(month+\"_\"+ele+\"_list_anomaly\")\n",
    "    \n",
    "    train_feat=[]\n",
    "    train_feat.extend(base_feat)\n",
    "    \n",
    "    # train_feat.extend(iccp_feat_list)\n",
    "    \n",
    "    train_feat.extend(sequence_feat_list)\n",
    "    train_feat.extend(magic_feat_list_v3)\n",
    "    \n",
    "    train_feat.extend(entropy_feat_list)\n",
    "    train_feat.extend(anomaly_feat_list)\n",
    "    \n",
    "    # train_feat.append('is_halted_next_2months')\n",
    "    \n",
    "    label_feat = 'halt_' + test_month[2:]\n",
    "    print(f\"label_feat:{label_feat}\")\n",
    "    \n",
    "    train_feat.append(label_feat)\n",
    "    \n",
    "    id_feat = \"prod_inst_id\"\n",
    "    train_feat.append(id_feat)\n",
    "    \n",
    "    train_feat.append(\"all_app_use_times\")\n",
    "    return train_feat\n",
    "\n",
    "def merge_metrics(metrics_list):\n",
    "    auc_list = []\n",
    "    pr_auc_list = []\n",
    "    f1_list = []\n",
    "    rec_list = []\n",
    "    pre_list = []\n",
    "    for metrics in metrics_list:\n",
    "        roc_auc = metrics[1]\n",
    "        pr_auc = metrics[2]\n",
    "        f1 = metrics[3]\n",
    "        rec = metrics[4]\n",
    "        pre = metrics[5]\n",
    "        \n",
    "        auc_list.append(roc_auc)\n",
    "        pr_auc_list.append(pr_auc)\n",
    "        f1_list.append(f1)\n",
    "        rec_list.append(rec)\n",
    "        pre_list.append(pre)\n",
    "    \n",
    "    print(f\"auc_list:{auc_list}\")\n",
    "    print(f\"pr_auc_list:{pr_auc_list}\")\n",
    "    print(f\"f1_list:{f1_list}\")\n",
    "    print(f\"rec_list:{rec_list}\")\n",
    "    print(f\"pre_list:{pre_list}\")\n",
    "    return (auc_list, pr_auc_list, f1_list, rec_list, pre_list)\n",
    "\n",
    "\n",
    "def churner_greedy_algorithm(pre_churner,budget):\n",
    "    pre_churner.sort_values(by=[\"churn_prob\"],ascending=False,inplace=True)\n",
    "    pre_churner.drop(columns=['level_0',\"index\"],inplace=True,errors='ignore')\n",
    "    pre_churner.reset_index(inplace=True)\n",
    "    # check_all(pre_churner)\n",
    "    \n",
    "    income = 0\n",
    "    ci = 0\n",
    "    cni = 0\n",
    "    nci = 0\n",
    "    ncni = 0      \n",
    "    for i in range(pre_churner.shape[0]):\n",
    "        stg_ind = -1\n",
    "        max_single_profit = -1\n",
    "        for j in range(len(cost)):\n",
    "            cur_single_profit = pre_churner.loc[i,\"rel_amount\"]*suc_rate[j]*rent_month[j]-cost[j]\n",
    "            if(cur_single_profit>max_single_profit):\n",
    "                max_single_profit = cur_single_profit\n",
    "                stg_ind = j\n",
    "        if(budget<cost[stg_ind]):\n",
    "            stg_ind = 0\n",
    "        budget -= cost[stg_ind]\n",
    "        if(pre_churner.at[i,\"is_churn\"]==1):\n",
    "            income += pre_churner.loc[i,\"rel_amount\"]*suc_rate[stg_ind]*rent_month[stg_ind]\n",
    "            if(stg_ind == 0):\n",
    "                cni += 1\n",
    "            else:\n",
    "                ci += 1\n",
    "        else:\n",
    "            if(stg_ind == 0):\n",
    "                ncni += 1\n",
    "            else:\n",
    "                nci += 1             \n",
    "        \n",
    "    profit = income - (500000 - budget)\n",
    "    print(\"*******************strategy:churner_greedy_algorithm************************\")\n",
    "    print(f\"income:{income:.0f}\")\n",
    "    print(f\"cost:{500000 - budget}\")\n",
    "    print(f\"profit:{profit:.0f}\")\n",
    "    print(f\"ci:{ci}\")\n",
    "    print(f\"cni:{cni}\")\n",
    "    print(f\"nci:{nci}\")\n",
    "    print(f\"ncni:{ncni}\")\n",
    "    acc = (ci+ncni)/(ci+nci+cni+ncni)\n",
    "    print(f\"acc:{acc:.4f}\")\n",
    "    return profit    \n",
    "\n",
    "def profit_greedy_algorithm(pre_churner,budget):\n",
    "    pre_churner.sort_values(by=['rel_amount'],ascending=False,inplace=True)\n",
    "    pre_churner.drop(columns=['level_0',\"index\"],inplace=True,errors='ignore')\n",
    "    pre_churner.reset_index(inplace=True)\n",
    "    # check_all(pre_churner)\n",
    "    \n",
    "    income = 0\n",
    "    ci = 0\n",
    "    cni = 0\n",
    "    nci = 0\n",
    "    ncni = 0      \n",
    "    for i in range(pre_churner.shape[0]):\n",
    "        stg_ind = -1\n",
    "        max_single_profit = -1\n",
    "        for j in range(len(cost)):\n",
    "            cur_single_profit = pre_churner.loc[i,\"rel_amount\"]*suc_rate[j]*rent_month[j]-cost[j]\n",
    "            if(cur_single_profit>max_single_profit):\n",
    "                max_single_profit = cur_single_profit\n",
    "                stg_ind = j\n",
    "        if(budget<cost[stg_ind]):\n",
    "            stg_ind = 0\n",
    "        budget -= cost[stg_ind]\n",
    "        if(pre_churner.at[i,\"is_churn\"]==1):\n",
    "            income += pre_churner.loc[i,\"rel_amount\"]*suc_rate[stg_ind]*rent_month[stg_ind]\n",
    "            if(stg_ind == 0):\n",
    "                cni += 1\n",
    "            else:\n",
    "                ci += 1\n",
    "        else:\n",
    "            if(stg_ind == 0):\n",
    "                ncni += 1\n",
    "            else:\n",
    "                nci += 1      \n",
    "        \n",
    "    profit = income - (500000 - budget)\n",
    "    print(\"*******************strategy:profit_greedy_algorithm************************\")\n",
    "    print(f\"income:{income:.0f}\")\n",
    "    print(f\"cost:{500000 - budget}\")\n",
    "    print(f\"profit:{profit:.0f}\")\n",
    "    print(f\"ci:{ci}\")\n",
    "    print(f\"cni:{cni}\")\n",
    "    print(f\"nci:{nci}\")\n",
    "    print(f\"ncni:{ncni}\")\n",
    "    acc = (ci+ncni)/(ci+nci+cni+ncni)\n",
    "    print(f\"acc:{acc:.4f}\")    \n",
    "    return profit\n",
    "\n",
    "def random_selection(pre_churner,budget):\n",
    "    income = 0\n",
    "    ci = 0\n",
    "    cni = 0\n",
    "    nci = 0\n",
    "    ncni = 0    \n",
    "    for i in range(pre_churner.shape[0]):\n",
    "        # print(f\"i:{i}\")\n",
    "        stg_ind = -1\n",
    "        if(budget >= 100):\n",
    "            stg_ind = random.randint(0,2)\n",
    "        elif(budget >= 15):\n",
    "            stg_ind = random.randint(0,1)\n",
    "        else:\n",
    "            stg_ind = 0\n",
    "        budget -= cost[stg_ind]\n",
    "        # if(pre_churner.loc[i,\"is_churner\"]>0):\n",
    "        # obj = pre_churner.at[i,\"churn_prob\"]\n",
    "        obj = pre_churner.at[i,\"is_churn\"]\n",
    "        # print(f\"obj:{obj}\")\n",
    "        if(obj==1):\n",
    "            income += pre_churner.loc[i,\"rel_amount\"]*suc_rate[stg_ind]*rent_month[stg_ind]\n",
    "            if(stg_ind == 0):\n",
    "                cni += 1\n",
    "            else:\n",
    "                ci += 1\n",
    "        else:\n",
    "            if(stg_ind == 0):\n",
    "                ncni += 1\n",
    "            else:\n",
    "                nci += 1              \n",
    "        \n",
    "    profit = income - (500000 - budget)\n",
    "    print(\"*******************strategy:random_selection************************\")\n",
    "    print(f\"income:{income:.0f}\")\n",
    "    print(f\"cost:{500000 - budget}\")\n",
    "    print(f\"profit:{profit:.0f}\")\n",
    "    print(f\"ci:{ci}\")\n",
    "    print(f\"cni:{cni}\")\n",
    "    print(f\"nci:{nci}\")\n",
    "    print(f\"ncni:{ncni}\")\n",
    "    acc = (ci+ncni)/(ci+nci+cni+ncni)\n",
    "    print(f\"acc:{acc:.4f}\")    \n",
    "    return profit\n",
    "\n",
    "def naive_alg(pre_churner, budget, int_stg_li, kind = \"do_not_intervention\"):\n",
    "    income = 0\n",
    "    profit = 0\n",
    "    \n",
    "    ci = 0\n",
    "    cni = 0\n",
    "    nci = 0\n",
    "    ncni = 0\n",
    "    \n",
    "    if kind == \"do_not_intervention\":\n",
    "        for i in range(pre_churner.shape[0]):\n",
    "            stg_ind = 0\n",
    "            single_income, single_cost = int_stg_li[stg_ind].cal_income_cost(pre_churner.at[i,\"rel_amount\"])\n",
    "            budget -= single_cost\n",
    "            \n",
    "            if(pre_churner.at[i,\"is_churn\"]==1):\n",
    "                income += single_income\n",
    "                if(stg_ind == 0):\n",
    "                    cni += 1\n",
    "                else:\n",
    "                    ci += 1\n",
    "            else:\n",
    "                if(stg_ind == 0):\n",
    "                    ncni += 1\n",
    "                else:\n",
    "                    nci += 1            \n",
    "                \n",
    "        profit = income - (500000 - budget)\n",
    "        \n",
    "    print(\"*******************strategy:do_not_intervention************************\")\n",
    "    print(f\"income:{income:.0f}\")\n",
    "    print(f\"cost:{500000 - budget}\")\n",
    "    print(f\"profit:{profit:.0f}\")\n",
    "    \n",
    "    print(f\"ci:{ci}\")\n",
    "    print(f\"cni:{cni}\")\n",
    "    print(f\"nci:{nci}\")\n",
    "    print(f\"ncni:{ncni}\")\n",
    "    acc = (ci+ncni)/(ci+nci+cni+ncni)\n",
    "    print(f\"acc:{acc:.4f}\")    \n",
    "    return profit    \n",
    "\n",
    "def check_sp_pd_li(spark_df,cols,rows=50):\n",
    "    tmp_df = ex_ic_df.select(*(cols))\n",
    "    check_spark_df(tmp_df)\n",
    "    \n",
    "    tmp_df_sample = tmp_df.sample(withReplacement=False,fraction=0.00001)\n",
    "    # check_spark_df(dt_series_df_sample)\n",
    "    \n",
    "    tmp_df_sample_pd_df = tmp_df_sample.toPandas()\n",
    "    check_all(tmp_df_sample_pd_df)\n",
    "    \n",
    "    print(cols)\n",
    "    for col in cols:\n",
    "        print(list(tmp_df_sample_pd_df.loc[:rows,col].values))\n",
    "    \n",
    "    return\n",
    "\n",
    "# def data_preprocessing(ic_spark_df,label_feat):\n",
    "#     '''\n",
    "#     user selection\n",
    "#     '''\n",
    "#     ic_spark_df = ic_spark_df.filter(ic_spark_df.src_offer_id.isin(cur_src_offer_id_li))\n",
    "#     # ic_spark_df = ic_spark_df.sample(fraction = 0.1, withReplacement = False)\n",
    "#     # check_spark_df(ic_spark_df)\n",
    "\n",
    "#     import pyspark.sql.functions as F\n",
    "#     ic_spark_df = ic_spark_df.withColumn('user_obj',F.when((ic_spark_df.lan_id == 20) | (ic_spark_df.lan_id == 11), 1).otherwise(0))\n",
    "#     ic_spark_df = ic_spark_df.where(ic_spark_df.user_obj == 1)\n",
    "#     # check_spark_df(ic_spark_df)\n",
    "#     '''\n",
    "#     feature selection\n",
    "#     '''\n",
    "#     # manual\n",
    "#     id_feat = [\"prod_inst_id\",\"lan_id\"]\n",
    "    \n",
    "#     # basic feat\n",
    "#     # bio_feat = [\"own_gender_cd\",\"own_age\"]\n",
    "#     bio_feat = [\"own_age\"]\n",
    "#     # money_feat = [\"partition_nf_amount_1m\",\"balance\",\"rel_amount\"]\n",
    "#     money_feat = [\"balance\",\"rel_amount\"]\n",
    "#     # offer_feat = [\"src_offer_id\",\"offer_grade\"]\n",
    "#     # offer_feat = [\"offer_grade\"] # 套餐不合适/套餐贵\n",
    "#     offer_feat = [\"rel_amount\"] # 套餐不合适/套餐贵\n",
    "#     cm_tfc_feat = [\"cm_tfc_sum\"] # 通用流量不够用\n",
    "#     dt_tfc_feat = [\"in_offer_tfc_sum\",\"out_offer_tfc_sum\"] # 定向流量不满足需求\n",
    "#     halt_feat = [\"d_stop_cnt_sum_3m\",\"s_stop_cnt_sum_3m\"]\n",
    "#     app_feat = [\"all_app_use_times\"] \n",
    "    \n",
    "#     # churn feat\n",
    "#     net_feat = [\"max_net_speed\",\"avg_net_speed\"] # 网速不行\n",
    "#     remote_feat = [\"monthly_trf_remote_record_num\"] # 不愿使用异地号卡\n",
    "#     card_feat = [\"card_weight\"] # 号卡太多\n",
    "    \n",
    "#     # label_feat = [\"halt_2012\"]\n",
    "    \n",
    "#     user_feat = []\n",
    "#     user_feat.extend(id_feat)\n",
    "#     # user_feat.extend(bio_feat)\n",
    "#     # user_feat.extend(money_feat)\n",
    "#     user_feat.extend(offer_feat)\n",
    "#     # user_feat.extend(halt_feat)\n",
    "#     # user_feat.extend(app_feat)\n",
    "#     user_feat.extend(cm_tfc_feat)\n",
    "#     user_feat.extend(dt_tfc_feat)\n",
    "#     user_feat.extend(net_feat)\n",
    "#     user_feat.extend(remote_feat)\n",
    "#     user_feat.extend(card_feat)\n",
    "#     user_feat.extend(label_feat)\n",
    "    \n",
    "#     print(user_feat)\n",
    "    \n",
    "#     part_ic_spark_df = ic_spark_df.select([col for col in user_feat])\n",
    "#     # check_spark_df(part_ic_spark_df)\n",
    "    \n",
    "#     # spark->pandas\n",
    "#     part_ic_pd_df = part_ic_spark_df.toPandas()\n",
    "#     part_ic_pd_df.rename(columns={\"monthly_trf_remote_record_num\":\"monthly_tfc_remote_record_num\"},inplace=True)\n",
    "#     # check_all(part_ic_pd_df,100)\n",
    "    \n",
    "#     '''\n",
    "#     # DataWashing\n",
    "#     '''\n",
    "#     # TypeConverting\n",
    "#     for col in part_ic_pd_df.columns:\n",
    "#         part_ic_pd_df[col] = part_ic_pd_df[col].astype(\"double\")\n",
    "    \n",
    "#     # MissingValue\n",
    "#     part_ic_pd_df.fillna(0,inplace=True)\n",
    "    \n",
    "#     # # AnomalyValue\n",
    "#     # part_ic_pd_df = part_ic_pd_df.loc[(part_ic_pd_df['own_age']>=16)&(part_ic_pd_df['own_age']<=85)]\n",
    "#     part_ic_pd_df = part_ic_pd_df.loc[(part_ic_pd_df['rel_amount']>=0)]\n",
    "#     # # check_all(part_ic_pd_df)\n",
    "#     part_ic_pd_df['rel_amount'] = part_ic_pd_df['rel_amount'] / 100\n",
    "#     # part_ic_pd_df.loc[part_ic_pd_df['d_stop_cnt_sum_3m'] == -1, 'd_stop_cnt_sum_3m'] = 0\n",
    "#     # part_ic_pd_df.loc[part_ic_pd_df['s_stop_cnt_sum_3m'] == -1, 's_stop_cnt_sum_3m'] = 0\n",
    "    \n",
    "#     # ID Column\n",
    "#     part_ic_pd_df.set_index(id_feat[0],inplace=True)\n",
    "    \n",
    "#     check_all(part_ic_pd_df,10)\n",
    "    \n",
    "#     return part_ic_pd_df\n",
    "\n",
    "\n",
    "def data_preprocessing_churn(ic_spark_df,label_feat):\n",
    "    '''\n",
    "    user selection\n",
    "    '''\n",
    "    ic_spark_df = ic_spark_df.filter(ic_spark_df.src_offer_id.isin(cur_src_offer_id_li))\n",
    "    # ic_spark_df = ic_spark_df.sample(fraction = 0.1, withReplacement = False)\n",
    "    # check_spark_df(ic_spark_df)\n",
    "\n",
    "    # import pyspark.sql.functions as F\n",
    "    # ic_spark_df = ic_spark_df.withColumn('user_obj',F.when((ic_spark_df.lan_id == 20) | (ic_spark_df.lan_id == 11), 1).otherwise(0))\n",
    "    # ic_spark_df = ic_spark_df.where(ic_spark_df.user_obj == 1)\n",
    "    # check_spark_df(ic_spark_df)\n",
    "    '''\n",
    "    feature selection\n",
    "    '''\n",
    "    # manual\n",
    "    id_feat = [\"prod_inst_id\"]\n",
    "    \n",
    "    # basic feat\n",
    "    # bio_feat = [\"own_gender_cd\",\"own_age\"]\n",
    "    bio_feat = [\"own_age\"]\n",
    "    # money_feat = [\"partition_nf_amount_1m\",\"balance\",\"rel_amount\"]\n",
    "    money_feat = [\"balance\",\"rel_amount\"]\n",
    "    # offer_feat = [\"src_offer_id\",\"offer_grade\"]\n",
    "    # offer_feat = [\"offer_grade\"] # 套餐不合适/套餐贵\n",
    "    offer_feat = [\"rel_amount\"] # 套餐不合适/套餐贵\n",
    "    cm_tfc_feat = [\"cm_tfc_sum\"] # 通用流量不够用\n",
    "    dt_tfc_feat = [\"in_offer_tfc_sum\",\"out_offer_tfc_sum\"] # 定向流量不满足需求\n",
    "    halt_feat = [\"d_stop_cnt_sum_3m\",\"s_stop_cnt_sum_3m\"]\n",
    "    app_feat = [\"all_app_use_times\"] \n",
    "    \n",
    "    # churn feat\n",
    "    net_feat = [\"max_net_speed\",\"avg_net_speed\"] # 网速不行\n",
    "    remote_feat = [\"monthly_tfc_remote_record_num\"] # 不愿使用异地号卡\n",
    "    card_feat = [\"card_weight\"] # 号卡太多\n",
    "    \n",
    "    # label_feat = [\"halt_2012\"]\n",
    "    \n",
    "    user_feat = []\n",
    "    user_feat.extend(id_feat)\n",
    "    # user_feat.extend(bio_feat)\n",
    "    # user_feat.extend(money_feat)\n",
    "    user_feat.extend(offer_feat)\n",
    "    # user_feat.extend(halt_feat)\n",
    "    # user_feat.extend(app_feat)\n",
    "    user_feat.extend(cm_tfc_feat)\n",
    "    user_feat.extend(dt_tfc_feat)\n",
    "    user_feat.extend(net_feat)\n",
    "    user_feat.extend(remote_feat)\n",
    "    user_feat.extend(card_feat)\n",
    "    user_feat.extend(label_feat)\n",
    "    \n",
    "    print(user_feat)\n",
    "    \n",
    "    part_ic_spark_df = ic_spark_df.select([col for col in user_feat])\n",
    "    # check_spark_df(part_ic_spark_df)\n",
    "    \n",
    "    # spark->pandas\n",
    "    part_ic_pd_df = part_ic_spark_df.toPandas()\n",
    "    # check_all(part_ic_pd_df,100)\n",
    "    \n",
    "    '''\n",
    "    # DataWashing\n",
    "    '''\n",
    "    # TypeConverting\n",
    "    for col in part_ic_pd_df.columns:\n",
    "        part_ic_pd_df[col] = part_ic_pd_df[col].astype(\"double\")\n",
    "    \n",
    "    # MissingValue\n",
    "    part_ic_pd_df.fillna(0,inplace=True)\n",
    "    \n",
    "    # # AnomalyValue\n",
    "    # part_ic_pd_df = part_ic_pd_df.loc[(part_ic_pd_df['own_age']>=16)&(part_ic_pd_df['own_age']<=85)]\n",
    "    part_ic_pd_df = part_ic_pd_df.loc[(part_ic_pd_df['rel_amount']>=0)]\n",
    "    # # check_all(part_ic_pd_df)\n",
    "    part_ic_pd_df['rel_amount'] = part_ic_pd_df['rel_amount'] / 100\n",
    "    # part_ic_pd_df.loc[part_ic_pd_df['d_stop_cnt_sum_3m'] == -1, 'd_stop_cnt_sum_3m'] = 0\n",
    "    # part_ic_pd_df.loc[part_ic_pd_df['s_stop_cnt_sum_3m'] == -1, 's_stop_cnt_sum_3m'] = 0\n",
    "    \n",
    "    # ID Column\n",
    "    part_ic_pd_df.set_index(id_feat[0],inplace=True)\n",
    "    \n",
    "    check_all(part_ic_pd_df,10)\n",
    "    \n",
    "    return part_ic_pd_df\n",
    "    \n",
    "def plotImp(model, X, num = 20, fig_size = (40, 20)):\n",
    "    feature_imp = pd.DataFrame({'Value':model.feature_importances_,'Feature':X.columns})\n",
    "    plt.figure(figsize=fig_size)\n",
    "    sns.set(font_scale = 5)\n",
    "    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", \n",
    "                                                        ascending=False)[0:num])\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('lgbm_importances-01.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return feature_imp  \n",
    "    \n",
    "def check_label_distribution(df, df_name, label_feat):\n",
    "    pos_df = df.loc[df[label_feat]==1]\n",
    "    print(f\"{df_name}:positive sample number:{pos_df.shape[0]}\")\n",
    "    neg_df = df.loc[df[label_feat]==0]\n",
    "    print(f\"{df_name}:negative sample number:{neg_df.shape[0]}\") \n",
    "    print(f\"{df_name}:neg-pos ratio:{neg_df.shape[0]/pos_df.shape[0]:.4f}\")    \n",
    "\n",
    "\n",
    "'''\n",
    "check degree of repetition of History data\n",
    "'''\n",
    "def check_spark_feature_intersection(spark_df1, spark_df2):\n",
    "    print(\"the feature of spark_df1:\")\n",
    "    col1 = spark_df1.columns\n",
    "    print(col1)\n",
    "    \n",
    "    print(\"the feature of spark_df2:\")\n",
    "    col2 = spark_df2.columns\n",
    "    print(col2)    \n",
    "    \n",
    "    print(\"the common feature of spark_df1 and spark_df2:\")\n",
    "    common_col = list(set(col1)&set(col2))\n",
    "    print(common_col)\n",
    "    \n",
    "    return common_col\n",
    "    \n",
    "def check_spark_sample_intersection(spark_df1, spark_df2, id_feat):\n",
    "    print(\"the #. of sample of spark_df1:\")\n",
    "    rows1 = spark_df1.count()\n",
    "    print(rows1)\n",
    "    id_df_1= spark_df1.select(id_feat)\n",
    "    id_pd_df_1 = id_df_1.toPandas()\n",
    "    id_li_1= list(id_pd_df_1[id_feat])\n",
    "    \n",
    "    print(\"the #. of sample of spark_df2:\")\n",
    "    rows2 = spark_df2.count()\n",
    "    print(rows2)\n",
    "    id_df_2= spark_df2.select(id_feat)\n",
    "    id_pd_df_2 = id_df_2.toPandas()\n",
    "    id_li_2= list(id_pd_df_2[id_feat])    \n",
    "    \n",
    "    print(\"the common sample of spark_df1 and spark_df2:\")\n",
    "    common_id_li = list(set(id_li_1)&set(id_li_2))\n",
    "    print(len(common_id_li))\n",
    "    \n",
    "    join_flag = False\n",
    "    if (len(id_li_1) == len(common_id_li)) or (len(id_li_2) == len(common_id_li)):\n",
    "        join_flag = True    \n",
    "    \n",
    "    return id_li_1, id_li_2, common_id_li, join_flag   \n",
    "\n",
    "def check_pandas_sample_intersection(id_pd_df_1, id_pd_df_2, id_feat):\n",
    "    id_li_1= list(id_pd_df_1[id_feat])\n",
    "    print(f\"len(id_li_1):{len(id_li_1)}\")\n",
    "#     print(f\":{}\")\n",
    "    \n",
    "    id_li_2= list(id_pd_df_2[id_feat])    \n",
    "    print(f\"len(id_li_2):{len(id_li_2)}\")    \n",
    "    \n",
    "    print(\"the common sample of spark_df1 and spark_df2:\")\n",
    "    common_id_li = list(set(id_li_1)&set(id_li_2))\n",
    "    print(len(common_id_li))\n",
    "    \n",
    "    join_flag = False\n",
    "#     if (len(id_li_1) == len(common_id_li)) or (len(id_li_2) == len(common_id_li)):\n",
    "#         join_flag = True    \n",
    "    \n",
    "    return id_li_1, id_li_2, common_id_li, join_flag  \n",
    "\n",
    "# ic_churn_ex_wide_df = parquet2sparkdf(\"ic_churn\",\"ex_wide\",\"202011\",\"1\")\n",
    "# ic_churn_ex_wide_pd_df = ic_churn_ex_wide_df.toPandas()\n",
    "# check_all(ic_churn_ex_wide_pd_df)\n",
    "# #  51  in_offer_tfc_sum               363281 non-null  float64\n",
    "# #  52  out_offer_tfc_sum              363281 non-null  float64\n",
    "# #  53  avg_net_speed                  0 non-null       object \n",
    "# #  54  max_net_speed                  0 non-null       object \n",
    "# #  55  monthly_trf_remote_record_num  0 non-null       object \n",
    "# #  56  second_card_slot_l1m           0 non-null       object \n",
    "# #  57  card_weight                    0 non-null       object \n",
    "# #  58  avg_net_speed2                 272751 non-null  float64\n",
    "# #  59  max_net_speed2                 123797 non-null  float64\n",
    "\n",
    "# ic_ex_wide_df = parquet2sparkdf(\"ic\",\"ex_wide\",\"202011\",\"7\")\n",
    "# ic_ex_wide_df = ic_ex_wide_df.where(ic_ex_wide_df.halt_2012 == 0) \n",
    "# ic_ex_wide_df = ic_ex_wide_df.sample(fraction = 0.1, withReplacement = False).limit(20000)\n",
    "# ic_ex_wide_pd_df = ic_ex_wide_df.toPandas()\n",
    "# check_all(ic_ex_wide_pd_df)\n",
    "\n",
    "# churner:\n",
    "#  52  out_offer_tfc_sum              20000 non-null  float64\n",
    "#  53  avg_net_speed                  0 non-null      object \n",
    "#  54  max_net_speed                  0 non-null      object \n",
    "#  55  monthly_trf_remote_record_num  0 non-null      object \n",
    "#  56  second_card_slot_l1m           0 non-null      object \n",
    "#  57  card_weight                    0 non-null      object \n",
    "#  58  lan_id                         20000 non-null  object \n",
    "\n",
    "# non-churner:\n",
    "#  52  out_offer_tfc_sum              20000 non-null  float64\n",
    "#  53  avg_net_speed                  15130 non-null  float64\n",
    "#  54  max_net_speed                  11278 non-null  float64\n",
    "#  55  monthly_trf_remote_record_num  13768 non-null  float64\n",
    "#  56  second_card_slot_l1m           16310 non-null  object \n",
    "#  57  card_weight                    16310 non-null  float64\n",
    "#  58  lan_id                         20000 non-null  object \n",
    "\n",
    "# ic_ex_wide_df = parquet2sparkdf(\"ic\",\"wide\",\"202011\",\"5\")\n",
    "# ic_ex_wide_df = ic_ex_wide_df.where(ic_ex_wide_df.halt_2012 == 0) \n",
    "# ic_ex_wide_df = ic_ex_wide_df.sample(fraction = 0.1, withReplacement = False).limit(10000)\n",
    "# ic_ex_wide_pd_df = ic_ex_wide_df.toPandas()\n",
    "# # check_all(ic_ex_wide_pd_df)\n",
    "# # print(ic_ex_wide_pd_df.info(verbose=True, null_counts=True))\n",
    "\n",
    "# common_feat_li = check_spark_feature_intersection(ic_churn_ex_wide_df, ic_ex_wide_df)\n",
    "# id_li_1, id_li_2, common_id_li = check_spark_sample_intersection(ic_churn_ex_wide_df, ic_ex_wide_df, \"prod_inst_id\")\n",
    "# # print(id_li_1[:10])\n",
    "# # print(id_li_2[:10])\n",
    "\n",
    "def generate_iccp_dataset(X_train,X_valid,y_train,y_valid,y_valid_hat_df):\n",
    "#     print(f\":{}\")\n",
    "#     print(f\"X_train.shape:{X_train.shape}\")\n",
    "#     print(f\"X_valid.shape:{X_valid.shape}\")\n",
    "#     print(f\"y_train.shape:{y_train.shape}\")\n",
    "#     print(f\"y_valid.shape:{y_valid.shape}\")\n",
    "#     print(f\"y_valid_hat_df.shape:{y_valid_hat_df.shape}\") # true\n",
    "    \n",
    "#     print('start y_train_li')\n",
    "#     y_train_start = pd.DataFrame(y_train)\n",
    "#     y_train_li = y_train_start.values.tolist()  \n",
    "#     print(y_train_li) # not nan\n",
    "    \n",
    "#     print('start y_valid_li')\n",
    "#     y_valid_li = pd.DataFrame(y_valid).values.tolist()  \n",
    "#     print(y_valid_li) # not nan    \n",
    "    \n",
    "    # X_all = np.r_[X_train,X_valid]\n",
    "    # y_hat_all_np = model.predict(X_all)\n",
    "    # y_hat_all_df = pd.DataFrame(y_hat_all_np)\n",
    "    X_train['is_train'] = 1  \n",
    "    X_valid['is_train'] = 0\n",
    "    \n",
    "    X_all = pd.concat([X_train,X_valid])\n",
    "#     print(\"check_format(X_all,'simple')\")\n",
    "#     check_format(X_all,'simple')\n",
    "    # check_detail(X_all)\n",
    "    \n",
    "    # X_all_fakelabel = pd.concat([X_all,y_valid_hat_df],axis=1,join='inner')\n",
    "    \n",
    "    X_all.reset_index(drop=True, inplace=True)\n",
    "    X_all_fakelabel = pd.concat([X_all,y_valid_hat_df],axis=1,ignore_index=True,join='inner')\n",
    "    print(f\"X_all_fakelabel.shape:{X_all_fakelabel.shape}\")\n",
    "    \n",
    "#     check_format(X_all_fakelabel,'simple')\n",
    "#     check_detail(X_all_fakelabel)\n",
    "        \n",
    "    y_all = pd.concat([y_train,y_valid])\n",
    "    y_all.reset_index(drop=True, inplace=True)\n",
    "    print(f\"y_all.shape:{y_all.shape}\")\n",
    "#     check_format(pd.DataFrame(y_all))\n",
    "#     check_detail(y_all)\n",
    "#     print('y_all_li')\n",
    "#     y_all_li = pd.DataFrame(y_all).values.tolist()  \n",
    "#     print(y_all_li)# not nan \n",
    "    \n",
    "    X_all['label'] = y_all\n",
    "#     print(f'check_format(X_all):{check_format(X_all)}')\n",
    "#     print(f'check_detail(X_all):{check_detail(X_all)}')\n",
    "    print('check_format(X_all):')\n",
    "    check_format(X_all)\n",
    "#     print('check_detail(X_all):')\n",
    "#     check_detail(X_all)\n",
    "    \n",
    "    \n",
    "    X_train = X_all[X_all['is_train'] == 1]\n",
    "    \n",
    "    y_train = X_train.pop('label')\n",
    "#     print(f'check_format(pd.DataFrame(y_train)):{check_format(pd.DataFrame(y_train))}')\n",
    "    print(f'check_format(pd.DataFrame(y_train)):')\n",
    "    check_format(pd.DataFrame(y_train))\n",
    "    \n",
    "#     print('middle y_train_li')\n",
    "#     y_train_mid = pd.DataFrame(y_train)\n",
    "#     y_train_li = y_train_mid.values.tolist()\n",
    "#     print(y_train_li)\n",
    "     \n",
    "    X_train.pop('is_train')\n",
    "#     print(f'check_format(X_train):{check_format(X_train)}')\n",
    "#     print(f'check_detail(X_train):{check_detail(X_train)}')   \n",
    "    print(f'check_format(X_train):')\n",
    "    check_format(X_train)\n",
    "#     print(f'check_detail(X_train):')   \n",
    "#     check_detail(X_train)\n",
    "   \n",
    "    \n",
    "    X_valid = X_all[X_all['is_train'] == 0]\n",
    "    y_valid = X_valid.pop('label')\n",
    "    print(f'check_format(pd.DataFrame(y_valid)):')\n",
    "    check_format(pd.DataFrame(y_valid))    \n",
    "    X_valid.pop('is_train')\n",
    "    print(f'check_format(X_valid):')\n",
    "    check_format(X_valid)    \n",
    "    \n",
    "#     X_train,X_valid,y_train,y_valid = train_test_split(X_all_fakelabel,y_all,random_state=random_state,test_size=0.2)\n",
    "#     print(f'X_train.shape:{X_train.shape}')\n",
    "#     print(f'y_train.shape:{y_train.shape}')\n",
    "#     print(f'X_valid.shape:{X_valid.shape}')\n",
    "#     print(f'y_valid.shape:{y_valid.shape}')\n",
    "\n",
    "    iccp_dataset = (X_train, y_train, X_valid, y_valid)\n",
    " \n",
    "    iccp_dataset_df, iccp_dataset_np, iccp_dataset_tensor = data_transformation(iccp_dataset, 'none')\n",
    "    return iccp_dataset_df, iccp_dataset_np, iccp_dataset_tensor  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-reducing",
   "metadata": {},
   "source": [
    "## Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "technological-patio",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --Class\n",
    "\n",
    "            \n",
    "\n",
    "# Model\n",
    "# class ML_Model:\n",
    "#     def __init__(self,dataset,model,model_name,mode='data'):\n",
    "#         self._x_train,self._y_train,self._x_test,self._y_test = dataset\n",
    "#         self._y_pre = None\n",
    "#         self._model = model\n",
    "#         self._model_name = model_name\n",
    "#         self._mode = mode\n",
    "#         print('--------------'+self._model_name+'--------------------------')\n",
    "    \n",
    "#     def train(self):\n",
    "#         self._model.fit(self._x_train, self._y_train)\n",
    "    \n",
    "#     def predict(self):   \n",
    "#         self._y_pre = pd.DataFrame(self._model.predict_proba(self._x_test)[:,1])\n",
    "    \n",
    "#     def run(self):\n",
    "#         mode = self._mode\n",
    "#         if mode == 'data':\n",
    "#             self.train()\n",
    "#             self.predict()\n",
    "#             self._y_test.reset_index(inplace=True,drop=True)\n",
    "#             y_result_df = pd.concat([self._y_test, self._y_pre],axis=1,ignore_index=True)\n",
    "#             return self._y_pre, self._y_test, y_result_df\n",
    "\n",
    "# Model\n",
    "class ML_Model:\n",
    "    def __init__(self,dataset,model,model_name,mode='data'):\n",
    "        self._x_train,self._y_train,self._x_test,self._y_test = dataset\n",
    "        self._y_pre = None\n",
    "        self._model = model\n",
    "        self._model_name = model_name\n",
    "        self._mode = mode\n",
    "        print('--------------'+self._model_name+'--------------------------')\n",
    "    \n",
    "    def get_model(self):\n",
    "        return self._model\n",
    "    \n",
    "    def change_dataset(self,data_set):\n",
    "        self._x_train,self._y_train,self._x_test,self._y_test = data_set\n",
    "    \n",
    "    def train(self):\n",
    "        self._model.fit(self._x_train, self._y_train)\n",
    "    \n",
    "    def predict(self):\n",
    "        self._y_pre = pd.DataFrame(self._model.predict_proba(self._x_test)[:,1])\n",
    "        # self._x_all = np.r_[self._x_train,self._x_test]\n",
    "        # self._y_pre = pd.DataFrame(self._model.predict_proba(self._x_all)[:,1])\n",
    "        \n",
    "        self._y_test.reset_index(inplace=True,drop=True)\n",
    "        y_result_df = pd.concat([self._y_test, self._y_pre],axis=1,ignore_index=True)\n",
    "        return self._y_pre, self._y_test, y_result_df            \n",
    "        # return self._y_pre\n",
    "        \n",
    "    def run(self):\n",
    "        mode = self._mode\n",
    "        if mode == 'data':\n",
    "            self.train()\n",
    "            self.predict()\n",
    "            \n",
    "            self._y_test.reset_index(inplace=True,drop=True)\n",
    "            y_result_df = pd.concat([self._y_test, self._y_pre],axis=1,ignore_index=True)\n",
    "            return self._y_pre, self._y_test, y_result_df\n",
    "            \n",
    "            # self._y_test.reset_index(inplace=True,drop=True)\n",
    "            # self._y_train.reset_index(inplace=True,drop=True)\n",
    "            # self._y_all = pd.DataFrame(np.r_[self._y_train,self._y_test])\n",
    "            # y_result_df = pd.concat([self._y_all, self._y_pre],axis=1,ignore_index=True)\n",
    "            # print(f\"type(self._y_pre):{type(self._y_pre)}\")\n",
    "            # print(f\"type(self._y_all):{type(self._y_all)}\")\n",
    "            # print(f\"type(y_result_df):{type(y_result_df)}\")\n",
    "            # return self._y_pre, self._y_all, y_result_df        \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_layers, input_dim, hidden_dim, output_dim, drop_out=0.1):\n",
    "        super(MLP, self).__init__()\n",
    "        module_list = []\n",
    "        for i in range(num_layers-1):\n",
    "            module_list.append(nn.Linear(input_dim,hidden_dim))\n",
    "            \n",
    "            module_list.append(nn.BatchNorm1d(hidden_dim))\n",
    "            # module_list.append(nn.BatchNorm1d(input_dim))\n",
    "            \n",
    "            module_list.append(nn.ReLU())\n",
    "            \n",
    "            input_dim = hidden_dim\n",
    "            \n",
    "            module_list.append(nn.Dropout(p=drop_out))\n",
    "        module_list.append(nn.Linear(hidden_dim,output_dim))\n",
    "        module_list.append(nn.Sigmoid())\n",
    "        self.models = nn.Sequential(*module_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.models(x)\n",
    "    \n",
    "    # def initialize(self):\n",
    "    #     a = np.sqrt(3/self.neurals)\n",
    "    #     print('a:{}'.format(a))\n",
    "    #     for m in self.modules():\n",
    "    #         if isinstance(m,nn.Linear):\n",
    "    #             a = np.sqrt(6 / self.neurals)\n",
    "    #             nn.init.uniform_(m.weight.data,-a,a)\n",
    "    \n",
    "    def initialize(self):#totest\n",
    "        a = np.sqrt(3/self.neurals)\n",
    "        print('a:{}'.format(a))\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(m.weight.data)    \n",
    "\n",
    "class ResMLP(nn.Module):\n",
    "    def __init__(self, num_layers, input_dim, hidden_dim, output_dim, drop_out=0.1):\n",
    "        super(ResMLP, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.in1 = nn.Linear(input_dim,hidden_dim)\n",
    "            \n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "            \n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "            \n",
    "        self.drop1 = nn.Dropout(p=drop_out)\n",
    "\n",
    "        self.in2 = nn.Linear(hidden_dim,hidden_dim)\n",
    "            \n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "            \n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "            \n",
    "        self.drop2 = nn.Dropout(p=drop_out)\n",
    "\n",
    "        self.in3 = nn.Linear(hidden_dim,hidden_dim)\n",
    "            \n",
    "        self.bn3 = nn.BatchNorm1d(hidden_dim)\n",
    "            \n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "            \n",
    "        self.drop3 = nn.Dropout(p=drop_out) \n",
    "        \n",
    "        self.in4 = nn.Linear(hidden_dim,hidden_dim)\n",
    "            \n",
    "        self.bn4 = nn.BatchNorm1d(hidden_dim)\n",
    "            \n",
    "        self.relu4 = nn.ReLU(inplace=True)\n",
    "            \n",
    "        self.drop4 = nn.Dropout(p=drop_out)\n",
    "        \n",
    "        self.in5 = nn.Linear(hidden_dim,hidden_dim)\n",
    "            \n",
    "        self.bn5 = nn.BatchNorm1d(hidden_dim)\n",
    "            \n",
    "        self.relu5 = nn.ReLU(inplace=True)\n",
    "            \n",
    "        self.drop5 = nn.Dropout(p=drop_out)        \n",
    "\n",
    "        self.out1 = nn.Linear(hidden_dim,output_dim)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.in1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.drop1(out)\n",
    "        \n",
    "        identity = out\n",
    "        \n",
    "        out = self.in2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.drop2(out) \n",
    "        \n",
    "        out += identity\n",
    "        \n",
    "        identity = out\n",
    "        \n",
    "        out = self.in3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.relu3(out)\n",
    "        out = self.drop3(out) \n",
    "        \n",
    "        out += identity \n",
    "        \n",
    "        identity = out\n",
    "        \n",
    "        out = self.in4(out)\n",
    "        out = self.bn4(out)\n",
    "        out = self.relu4(out)\n",
    "        out = self.drop4(out) \n",
    "        \n",
    "        out += identity\n",
    "        \n",
    "        out = self.in5(out)\n",
    "        out = self.bn5(out)\n",
    "        out = self.relu5(out)\n",
    "        out = self.drop5(out) \n",
    "        \n",
    "        out += identity         \n",
    "        \n",
    "        out = self.out1(out)\n",
    "        \n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "       \n",
    "    \n",
    "    def initialize(self):#totest\n",
    "        a = np.sqrt(3/self.neurals)\n",
    "        print('a:{}'.format(a))\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(m.weight.data)                           \n",
    "\n",
    "# Define LSTM Neural Networks\n",
    "class LSTMRNN(nn.Module):\n",
    "    \"\"\"\n",
    "        Parameters：\n",
    "        - input_size: feature size\n",
    "        - hidden_size: number of hidden units\n",
    "        - output_size: number of output\n",
    "        - num_layers: layers of LSTM to stack\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers,input_dim,hidden_dim,output_dim):\n",
    "        super().__init__()\n",
    "        # 1'\n",
    "        # module_list = []\n",
    "        # for i in range(num_layers):\n",
    "        #     module_list.append(nn.LSTM(input_dim, hidden_dim, num_layers)) # utilize the LSTM model in torch.nn\n",
    "        #     # module_list.append(nn.Linear(input_dim,hidden_dim))\n",
    "        #     # module_list.append(nn.BatchNorm1d(hidden_dim))\n",
    "        #     # module_list.append(nn.BatchNorm1d(input_dim))\n",
    "        #     # module_list.append(nn.ReLU())\n",
    "        #     # input_dim = hidden_dim\n",
    "        #     # module_list.append(nn.Dropout(p=drop_out))\n",
    "        # module_list.append(nn.Linear(hidden_dim,output_dim))\n",
    "        # module_list.append(nn.Sigmoid())\n",
    "        # self.models = nn.Sequential(*module_list)\n",
    "    \n",
    "        # 2'\n",
    "        # self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers) # utilize the LSTM model in torch.nn\n",
    "        # self.forwardCalculation = nn.Linear(hidden_dim, output_dim)\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # 3'\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "        self.mid = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bn = nn.BatchNorm1d(hidden_dim)\n",
    "        self.activa = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.classifier = nn.Linear(hidden_dim, output_dim)\n",
    "        self.final = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        # 1'\n",
    "        # def forward(self, x):\n",
    "        #     return self.models(x)\n",
    "        \n",
    "        # 2'\n",
    "        # x, _ = self.lstm(x)  # _x is input, size (seq_len, batch, input_size)\n",
    "        # s, b, h = x.shape  # x is output, size (seq_len, batch, hidden_size)\n",
    "        # x = x.view(s*b, h)\n",
    "        # x = self.forwardCalculation(x)\n",
    "        # x = x.view(s, b, -1)\n",
    "        # x = self.sigmoid(x)\n",
    "        # return x\n",
    "        \n",
    "        # 3'\n",
    "        out, (h_n, c_n) = self.lstm(x)\n",
    "        # At this time, the final output state h can be obtained from out\n",
    "        # x = out[:, -1, :]\n",
    "        x = h_n[-1, :, :]\n",
    "        x = self.mid(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.activa(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "        x = self.final(x)\n",
    "        return x        \n",
    "\n",
    "class MV_LSTM(nn.Module):\n",
    "    '''\n",
    "    model for timeseries classification\n",
    "    '''\n",
    "    def __init__(self, num_layers, input_size, hidden_size, num_classes):\n",
    "        super(MV_LSTM, self).__init__()\n",
    "        self.lstm= nn.LSTM(input_size,hidden_size,num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.linear.weight.data.uniform_(-0.1,0.1)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self,batch_input):\n",
    "        out,_ = self.lstm(batch_input)\n",
    "        out = self.linear(out[:,-1, :])  #Extract outout of lasttime step\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "class logger(object):\n",
    "    def __init__(self):\n",
    "        self.log= []\n",
    "        self.itr= []\n",
    "\n",
    "    def reset(self):\n",
    "        self.log= []\n",
    "        self.itr= []\n",
    "\n",
    "    def update(self,logval, itrval):\n",
    "        self.log+=[logval]\n",
    "        self.itr+=[itrval]\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) + x\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            # nn.GELU(),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim ** -0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "# b, 65, 1024, heads = 8\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "\n",
    "# self.to_qkv(x): b, 65, 64*8*3\n",
    "# qkv: b, 65, 64*8\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "\n",
    "# b, 65, 64, 8\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
    "\n",
    "# dots:b, 65, 64, 64\n",
    "        dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\n",
    "        mask_value = -torch.finfo(dots.dtype).max\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = F.pad(mask.flatten(1), (1, 0), value = True)\n",
    "            assert mask.shape[-1] == dots.shape[-1], 'mask has incorrect dimensions'\n",
    "            mask = mask[:, None, :] * mask[:, :, None]\n",
    "            dots.masked_fill_(~mask, mask_value)\n",
    "            del mask\n",
    "\n",
    "# attn:b, 65, 64, 64\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "# 使用einsum表示矩阵乘法：\n",
    "# out:b, 65, 64, 8\n",
    "        out = torch.einsum('bhij,bhjd->bhid', attn, v)\n",
    "\n",
    "# out:b, 64, 65*8\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "\n",
    "# out:b, 64, 1024\n",
    "        out =  self.to_out(out)\n",
    "        return out\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Residual(PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout))),\n",
    "                Residual(PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout)))\n",
    "            ]))\n",
    "    def forward(self, x, mask = None):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x, mask = mask)\n",
    "            x = ff(x)\n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    # def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
    "    def __init__(self, *, num_patches, patch_dim, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
    "        '''\n",
    "        ??\n",
    "        dim\n",
    "        depth\n",
    "        heads\n",
    "        mlp_dim\n",
    "        '''\n",
    "        super().__init__()\n",
    "        # assert image_size % patch_size == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "        # num_patches = (image_size // patch_size) ** 2\n",
    "        # patch_dim = channels * patch_size ** 2\n",
    "        # num_patches = 8 # N是sequence的长度\n",
    "        # patch_dim = 30 # D是sequence的每个向量的维度\n",
    "        \n",
    "        # assert num_patches > MIN_NUM_PATCHES, f'your number of patches ({num_patches}) is way too small for attention to be effective (at least 16). Try decreasing your patch size'\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        # self.patch_size = patch_size\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.patch_to_embedding = nn.Linear(patch_dim, dim)\n",
    "        # print(f'dim:{dim}')\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        # print(f'self.cls_token.shape:{self.cls_token.shape}')\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    # def forward(self, img, mask = None):\n",
    "    def forward(self, seq, mask = None):\n",
    "#         p = self.patch_size\n",
    "\n",
    "# # 图片分块\n",
    "#         x = rearrange(img, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = p, p2 = p)\n",
    "\n",
    "# # 降维(b,N,d)\n",
    "#         x = self.patch_to_embedding(x)\n",
    "        x = seq\n",
    "        b, n, _ = x.shape\n",
    "        # print(f'x:{x}')\n",
    "        # print(f'x.shape:{x.shape}')\n",
    "        # print(f'b:{b}')\n",
    "        # print(f'n:{n}')\n",
    "        # print(f'_:{_}')\n",
    "\n",
    "# 多一个可学习的x_class，与输入concat在一起，一起输入Transformer的Encoder。(b,1,d)\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "        # print(f'cls_tokens:{cls_tokens}')\n",
    "        # print(f'cls_tokens.shape:{cls_tokens.shape}')\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        # print(f'x+cls_tokens.shape:{x.shape}')\n",
    "\n",
    "# Positional Encoding：(b,N+1,d)\n",
    "        embedding_x = self.pos_embedding[:, :(n + 1)]\n",
    "        # print(f'embedding_x.shape:{embedding_x.shape}')\n",
    "        x += embedding_x\n",
    "        x = self.dropout(x)\n",
    "\n",
    "# Transformer的输入维度x的shape是：(b,N+1,d)\n",
    "        x = self.transformer(x, mask)\n",
    "        # print(f\"The output of tranformer encoder's shape:{x.shape}\")\n",
    "\n",
    "# (b,1,d)\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        # print(f\"The input of mlp_head's shape:{x.shape}\")\n",
    "        return self.mlp_head(x)\n",
    "# (b,1,num_class)\n",
    "\n",
    "# class attention_mlp():     #todo\n",
    "\n",
    "class DirectTraffic:\n",
    "    def __init__(self, app_series_name, rg_id, pid, sub_app_list):\n",
    "        self._app_series_name = app_series_name\n",
    "        self._rg_id = rg_id\n",
    "        self._pid = pid\n",
    "        self._sub_app_list = sub_app_list\n",
    "    \n",
    "    def get_name(self):\n",
    "        return self._app_series_name\n",
    "    \n",
    "    def get_cp_id(self):\n",
    "        return self._rg_id\n",
    "\n",
    "class Offer:\n",
    "    def __init__(self, offer_name, offer_id, price, traffic, comm, app_series_list, traffic_cost, comm_cost, traffic_upthreshold, mode=\"normal\"):\n",
    "        self._offer_name = offer_name\n",
    "        self._offer_id = offer_id\n",
    "        self._price = price\n",
    "        self._traffic = traffic\n",
    "        self._comm = comm\n",
    "        self._app_series_list = app_series_list\n",
    "        self._traffic_cost = traffic_cost\n",
    "        self._comm_cost = comm_cost\n",
    "        \n",
    "    def output_list(self):\n",
    "        global dt_series_name_list\n",
    "        \n",
    "        li = [self._offer_name, self._offer_id, self._price, self._traffic, self._comm, self._traffic_cost, self._comm_cost]\n",
    "        app_li = list(np.zeros(len(dt_series_name_list)))\n",
    "        for app_series in self._app_series_list:\n",
    "            app_name = app_series.get_name()\n",
    "            if app_name in dt_series_name_list:\n",
    "                ind = dt_series_name_list.index(app_name)\n",
    "                app_li[ind] = 1\n",
    "        li.extend(app_li)\n",
    "        return li\n",
    "        \n",
    "# --Endings--\n",
    "class MulIntMetrics:\n",
    "    def __init__(self, model_name):   \n",
    "        self.model_name = model_name\n",
    "        \n",
    "        self.result_dict = {\n",
    "            \"regret\" : [],\n",
    "            \"reward\" : [],\n",
    "            \"precision\" : [],\n",
    "            \"recall\" : [],\n",
    "            \"f1_score\" : [],\n",
    "            \"total_revenue\" : [],\n",
    "            \"len_of_regret\" : [],\n",
    "            \"rewards\" : []\n",
    "            \n",
    "        }\n",
    "#         self.regret_list = []\n",
    "#         self.reward_list = []\n",
    "#         self.precision_list = []\n",
    "#         self.recall_list = []\n",
    "#         self.total_revenue_list = []\n",
    "#         self.len_of_regret_list = []\n",
    "        \n",
    "#         self.regret = regret\n",
    "#         self.reward = reward\n",
    "#         self.precision = precision\n",
    "#         self.recall = recall\n",
    "#         self.total_revenue = total_revenue\n",
    "#         self.len_of_regret = len_of_regret\n",
    "    \n",
    "    def append_single_experiment(self, metrics):\n",
    "        regret, reward, precision, recall, f1_score, total_revenue, len_of_regret, rewards = metrics\n",
    "        \n",
    "#         self.regret_list.append(regret)\n",
    "#         self.reward_list.append(reward)\n",
    "#         self.precision_list.append(precision)\n",
    "#         self.recall_list.append(recall)\n",
    "#         self.total_revenue_list.append(total_revenue)\n",
    "#         self.len_of_regret_list.append(len_of_regret)\n",
    "\n",
    "        self.result_dict[\"regret\"].append(regret)\n",
    "        self.result_dict[\"reward\"].append(reward)\n",
    "        self.result_dict[\"precision\"].append(precision)\n",
    "        self.result_dict[\"recall\"].append(recall)\n",
    "        self.result_dict[\"f1_score\"].append(f1_score)\n",
    "        self.result_dict[\"total_revenue\"].append(total_revenue)\n",
    "        self.result_dict[\"len_of_regret\"].append(len_of_regret)\n",
    "        self.result_dict[\"rewards\"].append(rewards)\n",
    "    \n",
    "    def get_reward(self):\n",
    "        return (np.min(self.reward_list), np.average(self.reward_list), np.max(self.reward_list))\n",
    "    \n",
    "    def get_rewards(self):\n",
    "        return self.result_dict[\"rewards\"]\n",
    "#     def get_statistics(self, mode):\n",
    "#         if mode == 'all':\n",
    "#             pass\n",
    "#         elif mode == 'reward':\n",
    "            \n",
    "    def get_statistics(self, mode):\n",
    "#         statistics = None\n",
    "#         statistics = eval(f\"(np.min(self.{mode}_list), np.average(self.{mode}_list), np.max(self.{mode}_list))\")\n",
    "#         print(f\"self.model_name:{self.model_name}\")\n",
    "#         print(f\"self.regret_list:{self.regret_list}\")\n",
    "#         statistics = (np.min(self.regret_list), np.average(self.regret_list), np.max(self.regret_list))\n",
    "#         exec(f\"statistics = (np.min(self.{mode}_list), np.average(self.{mode}_list), np.max(self.{mode}_list))\")\n",
    "#         exec(\"statistics = (np.min(self.\"+mode+\"_list), np.average(self.\"+mode+\"_list), np.max(self.\"+mode+\"_list))\")\n",
    "\n",
    "        obj_list = self.result_dict[mode]\n",
    "#         statistics = (np.min(obj_list), np.average(obj_list), np.max(obj_list))\n",
    "        statistics = (np.average(obj_list) - np.min(obj_list), np.average(obj_list), np.max(obj_list) - np.average(obj_list))\n",
    "        return statistics\n",
    "    \n",
    "#     def print_statistics(self, mode):\n",
    "#         eval(f\"print (np.min(self.{mode}_list), np.average(self.{mode}_list), np.max(self.{mode}_list)\")\n",
    "#         return \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-sympathy",
   "metadata": {},
   "source": [
    "## Function of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "placed-company",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model train and test\n",
    "def test(model, criterion, test_data):\n",
    "    global logs\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    acc = 0\n",
    "    testloss = 0\n",
    "    num_batches = len(test_data)\n",
    "    y_hat = torch.tensor([])\n",
    "    print(f'num_batchs:{num_batches}')\n",
    "    for index, (features, targets) in enumerate(test_data):\n",
    "        inputs= Variable(features)\n",
    "        targets= Variable(targets)\n",
    "        outputs = model(inputs)\n",
    "        y_hat = torch.cat((y_hat,outputs),0)\n",
    "        loss = criterion(outputs, targets.float())\n",
    "        testloss+=loss.item()\n",
    "        _,predicted=torch.max(outputs.data,1)\n",
    "    #   print(f'type(predicted):{type(predicted)}')\n",
    "    #   print(f'predicted:{predicted}')\n",
    "    #   correct += predicted.eq(targets.data).cpu().sum()\n",
    "        total+=targets.size(0)\n",
    "    #   print(f'total:{total}')\n",
    "        del features, inputs, outputs, targets\n",
    "        gc.collect()\n",
    "    \n",
    "    acc = 100.* correct/total\n",
    "    avgtestloss = testloss/num_batches\n",
    "    print(\"Validation Accuracy: {} Validation Loss: {}\".format(acc, avgtestloss))\n",
    "    return (y_hat, acc, avgtestloss)\n",
    "    \n",
    "# @profile\n",
    "def train_and_test_mvlstm(train_data,valid_data):\n",
    "    global epochs\n",
    "    global logs\n",
    "    model = MV_LSTM(2, 4, 32, 1)\n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.02)\n",
    "    num_batches =  len(train_data)\n",
    "    print('total batches : {}'.format(num_batches))\n",
    "    y_hat = None\n",
    "    model.train()\n",
    "    for epoch in range(0, epochs):\n",
    "        for index, (features,targets) in enumerate(train_data):\n",
    "            inputs = Variable(features)\n",
    "            targets = Variable(targets)\n",
    "            outputs= model(inputs)\n",
    "            loss = criterion(outputs, targets.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            itrepoch = epoch + 1.* ((index + 1)/num_batches)\n",
    "            logs['train_loss'].update(loss.item(),itrepoch)\n",
    "            if((index+1)% 100 == 0):\n",
    "                print('Epoch = {}, Loss = {}'.format(epoch, loss.item()))\n",
    "            del inputs,outputs, targets,features,loss\n",
    "            gc.collect()\n",
    "\n",
    "        \n",
    "        print('----------Starting Test ----------------')\n",
    "        model.eval()\n",
    "        y_hat, testacc, testloss = test(model, criterion, valid_data)\n",
    "        # model.train()\n",
    "        logs['test_loss'].update(testloss, epoch)\n",
    "        logs['test_acc'].update(testacc, epoch)\n",
    "\n",
    "        if((epoch + 1)% 2 ==0):\n",
    "            torch.save(model.state_dict(), './checkpoint_epoch_{}.pth'.format(epoch))\n",
    "        with open(logfile, 'wb') as mylogger:\n",
    "            dill.dump(logs, mylogger)\n",
    "    # y_hat = model(valid_data)\n",
    "    return y_hat\n",
    "\n",
    "def train_and_test_lstmrnn(dataset):\n",
    "    X_train,y_train,X_valid,y_valid = dataset\n",
    "    \n",
    "    lstm_model = LSTMRNN(num_layers=1,input_dim=4,hidden_dim=16,output_dim=1) # 16 hidden units\n",
    "    print('LSTM model:', lstm_model)\n",
    "    print('model.parameters:', lstm_model.parameters)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    loss_dict = {'train_loss':[],'val_loss':[]}\n",
    "    optimizer = torch.optim.Adam(lstm_model.parameters(), lr=1e-2)\n",
    "    \n",
    "    # training_dataset = Data.TensorDataset(X_train,y_train)\n",
    "    # data_loader = Data.DataLoader(\n",
    "    #     dataset = training_dataset,\n",
    "    #     batch_size = batch_size,\n",
    "    #     shuffle = True)\n",
    "    epochs = 1\n",
    "    for epoch in range(epochs):\n",
    "        outputs = lstm_model(X_train)\n",
    "        loss = criterion(outputs,y_train)\n",
    "        # loss = criterion(outputs,y_train.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # ave_train_loss = 0\n",
    "        \n",
    "     \n",
    "        # if loss.item() < 1e-4:\n",
    "        #     print('Epoch [{}/{}], Loss: {:.5f}'.format(epoch+1, max_epochs, loss.item()))\n",
    "        #     print(\"The loss value is reached\")\n",
    "        #     break\n",
    "        # elif (epoch+1) % 100 == 0:\n",
    "        #     print('Epoch: [{}/{}], Loss:{:.5f}'.format(epoch+1, max_epochs, loss.item()))\n",
    "        # loss_dict['train_loss'].append(ave_train_loss)\n",
    "        \n",
    "        # # print('************valid start********************')\n",
    "        # lstm_model.eval()\n",
    "        # y_hat = lstm_model(X_valid)\n",
    "        # loss = criterion(y_hat.squeeze(),y_valid)\n",
    "        # val_loss = loss.item()\n",
    "        # loss_dict['val_loss'].append(val_loss)\n",
    "    \n",
    "    # Prediction\n",
    "    lstm_model = lstm_model.eval()\n",
    "    # model.load_state_dict(torch.load('checkpoint.pt')) # load the best performance in early stopping\n",
    "    # training_time = time_count('Model Training',data_time)\n",
    "    y_hat = lstm_model(X_valid)\n",
    "    \n",
    "    return y_hat\n",
    "\n",
    "def train_test_lstm_keras(dataset):\n",
    "    X_train,y_train,X_valid,y_valid = dataset\n",
    "    # print(type(X_train))\n",
    "    # check_format(X_train,'simple')\n",
    "    # check_format(X_train,'simple')\n",
    "    length = X_train.shape[1]\n",
    "    n_features = X_train.shape[2]\n",
    "    print(f'length:{length}')\n",
    "    print(f'n_features:{n_features}')\n",
    "    \n",
    "    # failed\n",
    "    # model = tf.keras.models.Sequential([\n",
    "    #     LSTM(32, input_shape=(length, n_features),return_sequences=True),\n",
    "    #     tf.keras.layers.LayerNormalization(axis=1),\n",
    "    #     LSTM(32, input_shape=(length, n_features),return_sequences=True),\n",
    "    #     tf.keras.layers.LayerNormalization(axis=1),\n",
    "    #     LSTM(32, input_shape=(length, n_features)),\n",
    "    #     # model.add(Dense(n_features, activation='relu'))\n",
    "    #     Dense(1, activation='sigmoid')\n",
    "    # ])\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(32, input_shape=(length, n_features),return_sequences=True))\n",
    "    # ln1 = tf.keras.layers.LayerNormalization(axis=1)\n",
    "    # model.add(ln1)\n",
    "    model.add(LSTM(32, input_shape=(length, n_features),return_sequences=True))\n",
    "    # ln2 = tf.keras.layers.LayerNormalization(axis=1)\n",
    "    # model.add(ln2)\n",
    "    model.add(LSTM(32, input_shape=(length, n_features)))\n",
    "    # model.add(Dense(n_features, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "    print(model.summary())\n",
    "    \n",
    "    # Fit model\n",
    "    # model.fit(X_train, y_train, epochs=5, verbose=2,batch_size=512,validation_split=0.25)\n",
    "    model.fit(X_train, y_train, epochs=5, verbose=2, batch_size=512)\n",
    "    \n",
    "    # Evaluate model\n",
    "    # valid version\n",
    "    y_hat = model.predict(X_valid)\n",
    "    \n",
    "    # y_valid_df = pd.DataFrame(y_valid)\n",
    "    # y_hat_df = pd.DataFrame(y_hat)\n",
    "    # y_result_df = pd.concat([y_valid_df,y_hat_df],axis='columns')\n",
    "    \n",
    "    # return (y_hat_df,y_valid_df,y_result_df),model\n",
    "    \n",
    "    # feature extraction version\n",
    "    X_all = np.r_[X_train,X_valid]\n",
    "    y_hat_all_np = model.predict(X_all)\n",
    "    y_hat_all_df = pd.DataFrame(y_hat_all_np)\n",
    "    \n",
    "    y_truth_all_np = np.r_[y_train,y_valid]\n",
    "    y_truth_all_df = pd.DataFrame(y_truth_all_np)\n",
    "    \n",
    "    y_result_all_df = pd.concat([y_truth_all_df,y_hat_all_df],axis='columns')\n",
    "    y_result_all_df.rename(columns={0:'deleted',1:'hat'},inplace=True)\n",
    "    \n",
    "    \n",
    "    # return y_hat\n",
    "    return (y_hat_all_df,y_truth_all_df,y_result_all_df),model\n",
    "    \n",
    "\n",
    "def train_test_cnnlstm_keras(dataset):\n",
    "    X_train,y_train,X_valid,y_valid = dataset\n",
    "    print(f\"X_train.shape:{X_train.shape}\")\n",
    "    length = X_train.shape[1]\n",
    "    n_features = X_train.shape[2]\n",
    "    \n",
    "    # 1'\n",
    "    # cnn = Sequential()\n",
    "    # cnn.add(Conv2D(1, (2,2), activation='relu', padding='same', input_shape=(10,10,1)))\n",
    "    # cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    # cnn.add(Flatten())\n",
    "    \n",
    "    # model = Sequential()\n",
    "    # model.add(TimeDistributed(cnn,)) #todo\n",
    "    # model.add(LSTM(32, input_shape=(length, n_features),return_sequences=True))\n",
    "    # model.add(LSTM(32, input_shape=(length, n_features),return_sequences=True))\n",
    "    \n",
    "    # model.add(LSTM(32, input_shape=(length, n_features)))\n",
    "    # model.add(Dense(n_features, activation='relu'))\n",
    "    # model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    # # model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "    # # print(model.summary())\n",
    "    \n",
    "    # # Fit model\n",
    "    # model.fit(X_train, y_train, epochs=15, verbose=2,batch_size=512,validation_split=0.25)\n",
    "    # # model.fit(X_train, y_train, epochs=1, verbose=2,batch_size=512)    \n",
    "    \n",
    "    # 2' #todo\n",
    "    # model = Sequential()\n",
    "    # model.add(TimeDistributed(Conv2D(2, (2,2), activation='relu'), input_shape=(128,length,n_features)))\n",
    "    # model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "    # model.add(TimeDistributed(Flatten()))\n",
    "    # model.add(LSTM(50))\n",
    "    # model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    # # model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "    # # print(model.summary())\n",
    "    \n",
    "    # # Fit model\n",
    "    # model.fit(X_train, y_train, epochs=15, verbose=2,batch_size=512,validation_split=0.25)\n",
    "    # # model.fit(X_train, y_train, epochs=1, verbose=2,batch_size=512)\n",
    "    \n",
    "    # 3'\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=496, kernel_size=2, input_shape=(length,n_features), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=7))\n",
    "    # model.add(Dropout(0.5))\n",
    "    model.add((LSTM(units=100,dropout=0.3, recurrent_dropout=0.3)))\n",
    "    # model.add(Dropout(0.5))\n",
    "    # model.add(Dense(units=2, activation='softmax'))\n",
    "    # model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    \n",
    "    model.fit(X_train,y_train, validation_data=(X_valid,y_valid), epochs=4, batch_size=64, verbose=2)\n",
    "    \n",
    "    # Evaluate model\n",
    "\n",
    "    # # valid version\n",
    "    # y_hat = model.predict(X_valid)\n",
    "    \n",
    "    # y_valid_df = pd.DataFrame(y_valid)\n",
    "    # y_hat_df = pd.DataFrame(y_hat)\n",
    "    # y_result_df = pd.concat([y_valid_df,y_hat_df],axis='columns')\n",
    "    \n",
    "    # return y_hat_df,y_valid_df,y_result_df\n",
    "    \n",
    "    # feature extraction version\n",
    "    X_all = np.r_[X_train,X_valid]\n",
    "    y_hat_all_np = model.predict(X_all)\n",
    "    y_hat_all_df = pd.DataFrame(y_hat_all_np)\n",
    "    \n",
    "    y_truth_all_np = np.r_[y_train,y_valid]\n",
    "    y_truth_all_df = pd.DataFrame(y_truth_all_np)\n",
    "    \n",
    "    y_result_all_df = pd.concat([y_truth_all_df,y_hat_all_df],axis='columns')\n",
    "    y_result_all_df.rename(columns={0:'deleted',1:'hat'},inplace=True)\n",
    "    \n",
    "    \n",
    "    # return y_hat\n",
    "    return (y_hat_all_df,y_truth_all_df,y_result_all_df),model\n",
    "\n",
    "def train_test_mlp(dataset):\n",
    "    X_train,y_train,X_valid,y_valid = dataset\n",
    "    global lr\n",
    "    global epochs\n",
    "    global drop_out\n",
    "    global patience\n",
    "    \n",
    "    model = MLP(6,X_train.shape[1],128,1,drop_out)\n",
    "    # output_str(model_content,output_mode,output_filename,'a')\n",
    "    \n",
    "    #  Criterion\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    #  Optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr = lr)\n",
    "    \n",
    "    # Train the model\n",
    "    loss_dict = {'train_loss':[],'val_loss':[]}\n",
    "    \n",
    "    patience = 7\n",
    "    early_stopping = EarlyStopping(patience,verbose=True)\n",
    "    \n",
    "    training_dataset = Data.TensorDataset(X_train,y_train)\n",
    "    data_loader = Data.DataLoader(\n",
    "        dataset = training_dataset,\n",
    "        batch_size = batch_size,\n",
    "        shuffle = True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # print('Epoch {} start'.format(str(epoch).ljust(3)))\n",
    "        model.train()\n",
    "        ave_train_loss = 0\n",
    "\n",
    "        for batch,(data,target) in enumerate(data_loader):\n",
    "            # print('************batch train start********************')\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            prediction = model(data)\n",
    "            \n",
    "            loss = criterion(prediction.squeeze(),target)\n",
    "            train_loss = loss.item() #float\n",
    "            \n",
    "            ave_train_loss = (ave_train_loss*batch + train_loss) / (batch+1)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # print('************valid start********************')\n",
    "        loss_dict['train_loss'].append(ave_train_loss)\n",
    "        model.eval()\n",
    "        \n",
    "        y_pred = model(X_valid)\n",
    "        loss = criterion(y_pred.squeeze(),y_valid)\n",
    "        val_loss = loss.item()\n",
    "        loss_dict['val_loss'].append(val_loss)\n",
    "        # print('************early stopping start********************')\n",
    "        early_stopping(val_loss,model)\n",
    "    \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "    \n",
    "    torch.save(model.state_dict(), 'mlp.pt')\n",
    "    model.load_state_dict(torch.load('mlp.pt')) # load the best performance in early stopping\n",
    "    \n",
    "    # training_time = time_count('Model Training',data_time)\n",
    "    \n",
    "    y_hat = model(X_valid)\n",
    "    \n",
    "    y_valid_df = pd.DataFrame(y_valid.detach().numpy())\n",
    "    # y_valid_df = y_valid_hat_df.astype('int')\n",
    "    # print('before concat y_valid_df.shape:{}'.format(y_valid_df.shape))\n",
    "    y_hat_df = pd.DataFrame(y_hat.detach().numpy())\n",
    "    # print('before concat y_pred_df.shape:{}'.format(y_pred_df.shape))\n",
    "    \n",
    "    # print('y_valid:')\n",
    "    y_valid_df.reset_index(drop=True,inplace=True)\n",
    "    # check_format(y_valid_df)\n",
    "    # check_detail(y_valid_df,10)\n",
    "    # print('y_hat:')\n",
    "    \n",
    "    # check_format(y_hat_df)\n",
    "    # check_detail(y_hat_df,10)\n",
    "    \n",
    "    y_result_df = y_valid_df.merge(right=y_hat_df,how='inner',left_index=True,right_index=True)\n",
    "    return (y_hat_df,y_valid_df,y_result_df),model\n",
    "    \n",
    "    # # feature extraction version\n",
    "    # X_all = np.r_[X_train,X_valid]\n",
    "    # # print(f\"type(X_all):{type(X_all)}\")\n",
    "    # X_all_tensor = torch.FloatTensor(X_all)\n",
    "    # # print(f\"type(X_all_tensor):{type(X_all_tensor)}\")\n",
    "    # y_hat_all_np = model(X_all_tensor)\n",
    "    # y_hat_all_df = pd.DataFrame(y_hat_all_np.detach().numpy())\n",
    "    \n",
    "    # y_truth_all_np = np.r_[y_train,y_valid]\n",
    "    # y_truth_all_tensor = torch.FloatTensor(y_truth_all_np)\n",
    "    # y_truth_all_df = pd.DataFrame(y_truth_all_tensor.detach().numpy())\n",
    "    \n",
    "    # y_result_all_df = pd.concat([y_truth_all_df,y_hat_all_df],axis='columns')\n",
    "    # y_result_all_df.rename(columns={0:'deleted',1:'halt'},inplace=True)\n",
    "    \n",
    "    \n",
    "    # # return y_hat\n",
    "    # return (y_hat_all_df,y_truth_all_df,y_result_all_df),model\n",
    "    \n",
    "\n",
    "def load_test_mlp(dataset):\n",
    "    X_train,y_train,X_valid,y_valid = dataset\n",
    "    global lr\n",
    "    global epochs\n",
    "    global drop_out\n",
    "    global patience\n",
    "    \n",
    "    model = MLP(6,X_train.shape[1],128,1,drop_out)\n",
    "    # output_str(model_content,output_mode,output_filename,'a')\n",
    "    \n",
    "    #  Criterion\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    #  Optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr = lr)\n",
    "    \n",
    "    # Train the model\n",
    "    loss_dict = {'train_loss':[],'val_loss':[]}\n",
    "    \n",
    "    patience = 7\n",
    "    early_stopping = EarlyStopping(patience,verbose=True)\n",
    "    \n",
    "    training_dataset = Data.TensorDataset(X_train,y_train)\n",
    "    data_loader = Data.DataLoader(\n",
    "        dataset = training_dataset,\n",
    "        batch_size = batch_size,\n",
    "        shuffle = True)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    \n",
    "    model.load_state_dict(torch.load('mlp.pt')) # load the best performance in early stopping\n",
    "    \n",
    "    # training_time = time_count('Model Training',data_time)\n",
    "    \n",
    "    y_hat = model(X_valid)\n",
    "    \n",
    "    y_valid_df = pd.DataFrame(y_valid.detach().numpy())\n",
    "    # print('before concat y_valid_df.shape:{}'.format(y_valid_df.shape))\n",
    "    y_hat_df = pd.DataFrame(y_hat.detach().numpy())\n",
    "    # print('before concat y_pred_df.shape:{}'.format(y_pred_df.shape))\n",
    "    \n",
    "    # print('y_valid:')\n",
    "    y_valid_df.reset_index(drop=True,inplace=True)\n",
    "    # check_format(y_valid_df)\n",
    "    # check_detail(y_valid_df,10)\n",
    "    # print('y_hat:')\n",
    "    \n",
    "    # check_format(y_hat_df)\n",
    "    # check_detail(y_hat_df,10)\n",
    "    \n",
    "    y_result_df = y_valid_df.merge(right=y_hat_df,how='inner',left_index=True,right_index=True)\n",
    "    return (y_hat_df,y_valid_df,y_result_df),model\n",
    "\n",
    "def train_test_resmlp(dataset):\n",
    "    X_train,y_train,X_valid,y_valid = dataset\n",
    "    global lr\n",
    "    global epochs\n",
    "    global drop_out\n",
    "    global patience\n",
    "    \n",
    "    model = ResMLP(6,X_train.shape[1],128,1,drop_out)\n",
    "    # output_str(model_content,output_mode,output_filename,'a')\n",
    "    \n",
    "    #  Criterion\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    #  Optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr = lr)\n",
    "    \n",
    "    # Train the model\n",
    "    loss_dict = {'train_loss':[],'val_loss':[]}\n",
    "    \n",
    "    patience = 7\n",
    "    early_stopping = EarlyStopping(patience,verbose=True)\n",
    "    \n",
    "    training_dataset = Data.TensorDataset(X_train,y_train)\n",
    "    data_loader = Data.DataLoader(\n",
    "        dataset = training_dataset,\n",
    "        batch_size = batch_size,\n",
    "        shuffle = True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # print('Epoch {} start'.format(str(epoch).ljust(3)))\n",
    "        model.train()\n",
    "        ave_train_loss = 0\n",
    "\n",
    "        for batch,(data,target) in enumerate(data_loader):\n",
    "            # print('************batch train start********************')\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            prediction = model(data)\n",
    "            \n",
    "            loss = criterion(prediction.squeeze(),target)\n",
    "            train_loss = loss.item() #float\n",
    "            \n",
    "            ave_train_loss = (ave_train_loss*batch + train_loss) / (batch+1)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # print('************valid start********************')\n",
    "        loss_dict['train_loss'].append(ave_train_loss)\n",
    "        model.eval()\n",
    "        \n",
    "        y_pred = model(X_valid)\n",
    "        loss = criterion(y_pred.squeeze(),y_valid)\n",
    "        val_loss = loss.item()\n",
    "        loss_dict['val_loss'].append(val_loss)\n",
    "        # print('************early stopping start********************')\n",
    "        early_stopping(val_loss,model)\n",
    "    \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "    \n",
    "    # model.load_state_dict(torch.load('checkpoint_resmlp.pt')) # load the best performance in early stopping\n",
    "    torch.save(model.state_dict(), 'resmlp.pt')\n",
    "    model.load_state_dict(torch.load('resmlp.pt')) # load the best performance in early stopping\n",
    "    \n",
    "    # training_time = time_count('Model Training',data_time)\n",
    "    \n",
    "    y_hat = model(X_valid)\n",
    "    \n",
    "    y_valid_df = pd.DataFrame(y_valid.detach().numpy())\n",
    "    # y_valid_df = y_valid_hat_df.astype('int')\n",
    "    # print('before concat y_valid_df.shape:{}'.format(y_valid_df.shape))\n",
    "    y_hat_df = pd.DataFrame(y_hat.detach().numpy())\n",
    "    # print('before concat y_pred_df.shape:{}'.format(y_pred_df.shape))\n",
    "    \n",
    "    # print('y_valid:')\n",
    "    y_valid_df.reset_index(drop=True,inplace=True)\n",
    "    # check_format(y_valid_df)\n",
    "    # check_detail(y_valid_df,10)\n",
    "    # print('y_hat:')\n",
    "    \n",
    "    # check_format(y_hat_df)\n",
    "    # check_detail(y_hat_df,10)\n",
    "    \n",
    "    y_result_df = y_valid_df.merge(right=y_hat_df,how='inner',left_index=True,right_index=True)\n",
    "    return (y_hat_df,y_valid_df,y_result_df),model\n",
    "    \n",
    "    # # feature extraction version\n",
    "    # X_all = np.r_[X_train,X_valid]\n",
    "    # # print(f\"type(X_all):{type(X_all)}\")\n",
    "    # X_all_tensor = torch.FloatTensor(X_all)\n",
    "    # # print(f\"type(X_all_tensor):{type(X_all_tensor)}\")\n",
    "    # y_hat_all_np = model(X_all_tensor)\n",
    "    # y_hat_all_df = pd.DataFrame(y_hat_all_np.detach().numpy())\n",
    "    \n",
    "    # y_truth_all_np = np.r_[y_train,y_valid]\n",
    "    # y_truth_all_tensor = torch.FloatTensor(y_truth_all_np)\n",
    "    # y_truth_all_df = pd.DataFrame(y_truth_all_tensor.detach().numpy())\n",
    "    \n",
    "    # y_result_all_df = pd.concat([y_truth_all_df,y_hat_all_df],axis='columns')\n",
    "    # y_result_all_df.rename(columns={0:'deleted',1:'halt'},inplace=True)\n",
    "    \n",
    "    \n",
    "    # # return y_hat\n",
    "    # return (y_hat_all_df,y_truth_all_df,y_result_all_df),model\n",
    "    \n",
    "\n",
    "def load_test_resmlp(dataset):\n",
    "    X_train,y_train,X_valid,y_valid = dataset\n",
    "    global lr\n",
    "    global epochs\n",
    "    global drop_out\n",
    "    global patience\n",
    "    \n",
    "    model = ResMLP(6,X_train.shape[1],128,1,drop_out)\n",
    "    # output_str(model_content,output_mode,output_filename,'a')\n",
    "    \n",
    "    #  Criterion\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    #  Optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr = lr)\n",
    "    \n",
    "    # Train the model\n",
    "    loss_dict = {'train_loss':[],'val_loss':[]}\n",
    "    \n",
    "    patience = 7\n",
    "    early_stopping = EarlyStopping(patience,verbose=True)\n",
    "    \n",
    "    training_dataset = Data.TensorDataset(X_train,y_train)\n",
    "    data_loader = Data.DataLoader(\n",
    "        dataset = training_dataset,\n",
    "        batch_size = batch_size,\n",
    "        shuffle = True)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    model.load_state_dict(torch.load('resmlp.pt')) # load the best performance in early stopping\n",
    "    \n",
    "    # training_time = time_count('Model Training',data_time)\n",
    "    \n",
    "    y_hat = model(X_valid)\n",
    "    \n",
    "    y_valid_df = pd.DataFrame(y_valid.detach().numpy())\n",
    "    # print('before concat y_valid_df.shape:{}'.format(y_valid_df.shape))\n",
    "    y_hat_df = pd.DataFrame(y_hat.detach().numpy())\n",
    "    # print('before concat y_pred_df.shape:{}'.format(y_pred_df.shape))\n",
    "    \n",
    "    # print('y_valid:')\n",
    "    y_valid_df.reset_index(drop=True,inplace=True)\n",
    "    # check_format(y_valid_df)\n",
    "    # check_detail(y_valid_df,10)\n",
    "    # print('y_hat:')\n",
    "    \n",
    "    # check_format(y_hat_df)\n",
    "    # check_detail(y_hat_df,10)\n",
    "    \n",
    "    y_result_df = y_valid_df.merge(right=y_hat_df,how='inner',left_index=True,right_index=True)\n",
    "    return (y_hat_df,y_valid_df,y_result_df),model\n",
    "\n",
    "    \n",
    "def train_test_vit(dataset):\n",
    "    X_train, y_train, X_valid, y_valid = dataset\n",
    "    global lr\n",
    "    global epochs\n",
    "    global drop_out\n",
    "    global patience\n",
    "    # ViT\n",
    "    # model = ViT(num_patches=X_train.shape[1], patch_dim=X_train.shape[2], num_classes=1, dim=X_train.shape[2], depth=3, heads=3, mlp_dim=128, dropout=0., emb_dropout=0.)# num_patches = 8, patch_dim = dim = 31\n",
    "    # ViT_T\n",
    "    print(f\"X_train.shape:{X_train.shape}\")\n",
    "    \n",
    "    #def __init__(self, *, num_patches, patch_dim, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
    "    model = ViT(num_patches=X_train.shape[1], patch_dim=X_train.shape[1], num_classes=1, dim=X_train.shape[2], depth=3, heads=3, mlp_dim=256, dropout=0., emb_dropout=0.1)# num_patches = , patch_dim = dim =     \n",
    "    \n",
    "    \n",
    "    # output_str(model_content,output_mode,output_filename,'a')\n",
    "    \n",
    "    #  Criterion\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    #  Optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr = lr)\n",
    "    \n",
    "    # Train the model\n",
    "    loss_dict = {'train_loss':[],'val_loss':[]}\n",
    "    \n",
    "    patience = 7\n",
    "    early_stopping = EarlyStopping(patience,verbose=True)\n",
    "    \n",
    "    training_dataset = Data.TensorDataset(X_train,y_train)\n",
    "    data_loader = Data.DataLoader(\n",
    "        dataset = training_dataset,\n",
    "        batch_size = batch_size,\n",
    "        shuffle = True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # print('Epoch {} start'.format(str(epoch).ljust(3)))\n",
    "        model.train()\n",
    "        ave_train_loss = 0\n",
    "\n",
    "        for batch,(data,target) in enumerate(data_loader):\n",
    "            # print('************batch train start********************')\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            prediction = model(data)\n",
    "            \n",
    "            loss = criterion(prediction.squeeze(),target)\n",
    "            train_loss = loss.item() #float\n",
    "            \n",
    "            ave_train_loss = (ave_train_loss*batch + train_loss) / (batch+1)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # print('************valid start********************')\n",
    "        loss_dict['train_loss'].append(ave_train_loss)\n",
    "        model.eval()\n",
    "        \n",
    "        y_pred = model(X_valid)\n",
    "        loss = criterion(y_pred.squeeze(),y_valid)\n",
    "        val_loss = loss.item()\n",
    "        loss_dict['val_loss'].append(val_loss)\n",
    "        # print('************early stopping start********************')\n",
    "        early_stopping(val_loss,model)\n",
    "    \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "    \n",
    "    model.eval()\n",
    "    torch.save(model.state_dict(), 'vit.pt')\n",
    "    model.load_state_dict(torch.load('vit.pt')) # load the best performance in early stopping\n",
    "    # training_time = time_count('Model Training',data_time)\n",
    "    \n",
    "    # # validation version\n",
    "    # y_hat = model(X_valid)\n",
    "    # print(f'type of y_hat:{type(y_hat)}')\n",
    "    \n",
    "    # y_valid_df = pd.DataFrame(y_valid.detach().numpy())\n",
    "    # # print('before concat y_valid_df.shape:{}'.format(y_valid_df.shape))\n",
    "    # y_hat_df = pd.DataFrame(y_hat.detach().numpy())\n",
    "    # # print('before concat y_pred_df.shape:{}'.format(y_pred_df.shape))\n",
    "    \n",
    "    # # print('y_valid:')\n",
    "    # y_valid_df.reset_index(drop=True,inplace=True)\n",
    "    # # check_format(y_valid_df)\n",
    "    # # check_detail(y_valid_df,10)\n",
    "    # # print('y_hat:')\n",
    "    \n",
    "    # # check_format(y_hat_df)\n",
    "    # # check_detail(y_hat_df,10)\n",
    "    # y_result_df = y_valid_df.merge(right=y_hat_df,how='inner',left_index=True,right_index=True)\n",
    "    \n",
    "    # return (y_hat_df,y_valid_df,y_result_df)\n",
    "\n",
    "    # feature extraction version\n",
    "    X_all_np = np.r_[X_train,X_valid]\n",
    "    X_all_tensor = torch.FloatTensor(X_all_np)\n",
    "    y_hat_all_tensor = model(X_all_tensor)\n",
    "    # print(f'type of y_hat_all_tensor:{type(y_hat_all_tensor)}')\n",
    "    y_hat_all_np = y_hat_all_tensor.detach().numpy()\n",
    "    y_hat_all_df = pd.DataFrame(y_hat_all_np)\n",
    "    \n",
    "    y_truth_all_np = np.r_[y_train,y_valid]\n",
    "    y_truth_all_df = pd.DataFrame(y_truth_all_np)\n",
    "    \n",
    "    y_result_all_df = pd.concat([y_truth_all_df,y_hat_all_df],axis='columns')\n",
    "    y_result_all_df.rename(columns={0:'deleted',1:'hat'},inplace=True)\n",
    "    \n",
    "    return (y_hat_all_df,y_truth_all_df,y_result_all_df),model\n",
    "\n",
    "def load_test_vit(dataset):\n",
    "    X_train,y_train,X_valid,y_valid = dataset\n",
    "    global lr\n",
    "    global epochs\n",
    "    global drop_out\n",
    "    global patience\n",
    "    # ViT\n",
    "    # model = ViT(num_patches=X_train.shape[1], patch_dim=X_train.shape[2], num_classes=1, dim=X_train.shape[2], depth=3, heads=3, mlp_dim=128, dropout=0., emb_dropout=0.)# num_patches = 8, patch_dim = dim = 31\n",
    "    # ViT_T\n",
    "    print(f\"X_train.shape:{X_train.shape}\")\n",
    "    \n",
    "    #def __init__(self, *, num_patches, patch_dim, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
    "    model = ViT(num_patches=X_train.shape[1], patch_dim=X_train.shape[1], num_classes=1, dim=X_train.shape[2], depth=3, heads=3, mlp_dim=256, dropout=0., emb_dropout=0.1)# num_patches = , patch_dim = dim =     \n",
    "    \n",
    "    \n",
    "    # output_str(model_content,output_mode,output_filename,'a')\n",
    "    \n",
    "    #  Criterion\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    #  Optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr = lr)\n",
    "    \n",
    "    # Train the model\n",
    "    loss_dict = {'train_loss':[],'val_loss':[]}\n",
    "    \n",
    "    patience = 7\n",
    "    early_stopping = EarlyStopping(patience,verbose=True)\n",
    "    \n",
    "    training_dataset = Data.TensorDataset(X_train,y_train)\n",
    "    data_loader = Data.DataLoader(\n",
    "        dataset = training_dataset,\n",
    "        batch_size = batch_size,\n",
    "        shuffle = True)\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    model.load_state_dict(torch.load('vit.pt')) # load the best performance in early stopping\n",
    "    # training_time = time_count('Model Training',data_time)\n",
    "    \n",
    "    y_hat = model(X_valid)\n",
    "    \n",
    "    y_valid_df = pd.DataFrame(y_valid.detach().numpy())\n",
    "    # print('before concat y_valid_df.shape:{}'.format(y_valid_df.shape))\n",
    "    y_hat_df = pd.DataFrame(y_hat.detach().numpy())\n",
    "    # print('before concat y_pred_df.shape:{}'.format(y_pred_df.shape))\n",
    "    \n",
    "    # print('y_valid:')\n",
    "    y_valid_df.reset_index(drop=True,inplace=True)\n",
    "    # check_format(y_valid_df)\n",
    "    # check_detail(y_valid_df,10)\n",
    "    # print('y_hat:')\n",
    "    \n",
    "    # check_format(y_hat_df)\n",
    "    # check_detail(y_hat_df,10)\n",
    "    \n",
    "    y_result_df = y_valid_df.merge(right=y_hat_df,how='inner',left_index=True,right_index=True)\n",
    "    return (y_hat_df,y_valid_df,y_result_df), model \n",
    "\n",
    "\n",
    "def train_valid_pca(X_train,X_valid):\n",
    "    pca = PCA(n_components = pca_ratio,svd_solver='auto')\n",
    "    pca.fit(X_train)\n",
    "    X_train = pd.DataFrame(pca.transform(X_train))\n",
    "    \n",
    "    # PCA-X_test\n",
    "    X_valid = pd.DataFrame(pca.transform(X_valid))\n",
    "    return X_train, X_valid, pca\n",
    "\n",
    "def load_test_pca(pca, X_test):\n",
    "    X_test = pd.DataFrame(pca.transform(X_test))\n",
    "    \n",
    "    return X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-dodge",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "executive-hours",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month:202011, month_days:30\n",
      "table_dict:{'ic_info_202004': 'tmp_app_exp_hlwk_label_info_202004', 'ic_info_202011': 'tmp_app_exp_hlwk_label_info_202011', 'ic_info_202012': 'tmp_app_exp_hlwk_label_info_202012', 'ic_info_202101': 'tmp_app_exp_hlwk_label_info_202101', 'ic_info_202102': 'tmp_app_exp_hlwk_label_info_202102', 'ic_info_202106': 'tmp_app_exp_hlwk_label_info_202106', 'ic_info_202107': 'tmp_app_exp_hlwk_label_info_202107', 'ic_info_202108': 'tmp_app_exp_hlwk_label_info_202108', 'ic_halt_202011': 'tmp_app_exp_stop_user_202011_to_202102', 'ic_halt_202012': 'tmp_app_exp_stop_user_202011_to_202102', 'ic_halt_202101': 'tmp_app_exp_stop_user_202011_to_202102', 'ic_halt_202102': 'tmp_app_exp_stop_user_202011_to_202102', 'ic_halt_202108': 'tmp_app_exp_stop_user_202011_to_202102', 'ic_halt_202109': 'tmp_app_exp_stop_user_202011_to_202102', 'ic_cdr_202011': 'tmp_app_exp_event_tgcdr_202011_new', 'ic_cdr_202012': 'tmp_app_exp_event_tgcdr_202012_new', 'ic_traffic_202011': 'tmp_app_exp_event_tgps_202011_2', 'ic_traffic_202012': 'tmp_app_exp_event_tgps_202012_2'}\n",
      "database_dict:{'ic_info_202004': 'hunan843.', 'ic_info_202011': 'hunan843.', 'ic_info_202012': 'hunan843.', 'ic_halt_202011': 'hunan843.', 'ic_halt_202012': 'hunan843.', 'ic_halt_202101': 'hunan843.', 'ic_halt_202102': 'hunan843.', 'ic_halt_202106': 'hunan843.', 'ic_halt_202107': 'hunan843.', 'ic_halt_202108': 'hunan843.', 'ic_halt_202109': 'hunan843.', 'ic_cdr_202011': 'hunan843.', 'ic_cdr_202012': 'hunan843.', 'ic_traffic_202011': 'hunan843.', 'ic_traffic_202012': 'hunan843.'}\n"
     ]
    }
   ],
   "source": [
    "# --Constant--\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "appName = \"Churner Prediction\"\n",
    "master = \"local\"\n",
    "# Create Spark session with Hive supported.\n",
    "# spark_qk = SparkSession.builder.appName(appName).master(master).enableHiveSupport().getOrCreate()\n",
    "\n",
    "# Common\n",
    "# counter\n",
    "counter = 0\n",
    "\n",
    "# file\n",
    "save_path = './'\n",
    "user_set = None\n",
    "\n",
    "# hardware\n",
    "# cpu_worker_num = 40\n",
    "cpu_worker_num = 200\n",
    "# cpu_worker_num = 1024\n",
    "\n",
    "# month\n",
    "# month = '202110'\n",
    "month = '202011'\n",
    "# month = '202012'\n",
    "# month = '202101'\n",
    "# month = '202108'\n",
    "# month = '202102'\n",
    "month_abbr = month[-4:]\n",
    "month_period_dict = {1:31,2:28,3:31,4:30,5:31,6:30,7:31,8:31,9:30,10:31,11:30,12:31}\n",
    "month_days = month_period_dict[int(month[-2:])]\n",
    "print(f\"month:{month}, month_days:{month_days}\")\n",
    "\n",
    "# week\n",
    "week_22_1_3 = generate_date(2022, 1, 3, 'Sunday')\n",
    "week_22_1_4 = generate_date(2022, 1, 4, 'Sunday')\n",
    "week_22_1_5 = generate_date(2022, 1, 5, 'Sunday')\n",
    "week_22_2_1 = generate_date(2022, 2, 1, 'Sunday')\n",
    "\n",
    "week_list = []\n",
    "week_list.append(week_22_1_3)\n",
    "week_list.append(week_22_1_4)\n",
    "week_list.append(week_22_1_5)\n",
    "week_list.append(week_22_2_1)\n",
    "\n",
    "week_ith = 0\n",
    "week_cur = week_list[week_ith]\n",
    "week_next = week_list[week_ith + 1]\n",
    "week_next_2 = week_list[week_ith + 2]\n",
    "\n",
    "# random number seed\n",
    "random_state=42\n",
    "random_state=2021\n",
    "torch.manual_seed(random_state)\n",
    "\n",
    "# file\n",
    "ic_wide_table_2011 = 'ic_2011'\n",
    "ic_wide_table_2012 = 'ic_2012'\n",
    "ic_wide_table_2012_quick = 'ic_2012_quick'\n",
    "ic_wide_table_2012_quick_01 = 'ic_2012_quick_01'\n",
    "ic_wide_table_2012_quick_02 = 'ic_2012_quick_02'\n",
    "\n",
    "ic_wide_table_2101_quick = 'ic_2101_quick'\n",
    "ic_wide_table_2101_quick_back = 'ic_2101_quick_back'\n",
    "\n",
    "ic_wide_table_zjj_2011 = 'ic_21_2011'\n",
    "ic_simple_cdr_statistics_zjj_2011 = 'ic_simple_cdr_statistics_21_2011'\n",
    "ic_traffic_statistics_zjj_2011 ='ic_traffic_statistics_21_2011_v2'\n",
    "\n",
    "ic_wide_table_test = 'wideTestTable'\n",
    "\n",
    "file_suffix = '.parquet'\n",
    "file_selection =  ic_wide_table_test\n",
    "# file_selection =  ic_wide_table_2012\n",
    "# file_selection =  ic_wide_table_2101_quick #*\n",
    "# file_selection =  ic_wide_table_2012_quick_01 #*\n",
    "deal_filename = file_selection + file_suffix\n",
    "deal_filename_backup = deal_filename[:-len(file_suffix)] + '_backup' + file_suffix\n",
    "\n",
    "# columns\n",
    "id_col_list = ['prod_inst_id','acc_nbr']\n",
    "\n",
    "# categorical_col_list = ['own_gender_cd','work','src_offer_id','game_app_top1_name','home','mkt_res_inst_nbr','reg_term_model','second_card_slot_l1m']\n",
    "categorical_col_list = ['own_gender_cd','src_offer_id','work','home','mkt_res_inst_nbr','reg_term_model','second_card_slot_l1m']\n",
    "simple_categorical_col_list = ['own_gender_cd','src_offer_id','second_card_slot_l1m']\n",
    "sparse_categorical_col_list = ['work','home','mkt_res_inst_nbr','reg_term_model']\n",
    "new_categorical_col_list = ['game_app_top1_name']\n",
    "\n",
    "# daily_traffic_col_list = ['prod_inst_id','byte_in','byte_out','p_day_id','duration']\n",
    "daily_traffic_col_list = ['prod_inst_id','byte_in','byte_out','day_id','duration']\n",
    "# daily_cdr_col_list =['prod_inst_id','acc_nbr','calling_nbr','called_nbr','duration']\n",
    "daily_cdr_col_list =['prod_inst_id','acc_nbr','calling_nbr_new','called_nbr_new','duration']\n",
    "sepe_pure_traffic_col_list =['prod_inst_id','byte_out','byte_in','duration','start_year','start_month','start_day','start_hour',\n",
    "'start_min','start_sec','end_year','end_month','end_day','end_hour','end_min','end_sec']\n",
    "\n",
    "user_profile_col_list = ['prd_inst_id','game_app_top1_name','games_app_days_l1m','music_app_days_l1m','video_app_days_l1m','shopping_app_days_l1m','use_dura_l1m','use_dura_l2m','use_dura_avg_3m','use_dura_cm','own_gender_cd','own_age','work','lan_id','home','night_latitude','night_longitude','day_latitude','day_longitude','exceed_flow_l1m','surplus_flow_l1m','total_offer_flow_l1m','use_offer_percent','use_flow_l1m','exceed_flow_l2m','surplus_flow_l2m','use_flow_l2m','l1_use_offer_percent','use_flow_avg_3m','surplus_flow_cm','use_flow_cm','avg_over_flow','hlwk_offer_aim_flow_cm','l2_use_offer_percent','reg_term_model','mkt_res_inst_nbr','reg_term_price','user_terminal_dur','top1_start_hour','top1_end_hour','top2_start_hour','top2_end_hour','top3_start_hour','top3_end_hour','balance','partition_nf_amount_1m','rel_amount','src_offer_id','offer_grade','second_card_slot_l1m','innet_dur','s_stop_cnt_sum_3m','d_stop_cnt_sum_3m','eff_date']\n",
    "\n",
    "\n",
    "\n",
    "# DataProcessing\n",
    "# sql\n",
    "simple_sql = \"SELECT {} FROM {} {}\"\n",
    "condition_sql = \"SELECT {} FROM {} WHERE {} = {} {}\"\n",
    "\n",
    "record_num = 100\n",
    "limit = set_limit(record_num)\n",
    "no_limit = ''\n",
    "limit_mode = no_limit\n",
    "\n",
    "\n",
    "\n",
    "# database\n",
    "# table_database_dict = {}\n",
    "database_dict = {}\n",
    "table_dict = {}\n",
    "database = ''\n",
    "group_project_database = 'hunan843.'\n",
    "old_project_database_bas = 'int3bas.'\n",
    "old_project_database_ass = 'int3ass.'\n",
    "old_project_database_bsn = 'inf3bsn.'\n",
    "old_project_database_sjwj = 'user_sjwj.'\n",
    "personal_database = 'lr_pro2019_role2_45.'\n",
    "\n",
    "# if platform == 'group':\n",
    "#     database = group_project_database\n",
    "# else:\n",
    "#     database = ''\n",
    "\n",
    "# table\n",
    "# all\n",
    "table_ic_wide_table_2011 = 'hlwk_2011_v1'\n",
    "table_ic_wide_table_2012 = 'hlwk_2012'\n",
    "table_ic_traffic_2012 = 'hlwk_simple_traffic_2012'\n",
    "table_ic_cdr_2012 = 'hlwk_simple_cdr_2012'\n",
    "\n",
    "# zjj\n",
    "table_ic_cdr_2011 = 'hlwk_simple_cdr_21_202011'\n",
    "table_ic_inst_nbr_2011 = 'hlwk_inst_nbr_2011'\n",
    "# quick\n",
    "table_ic_wide_table_2012_quick = 'hlwk_2012_quick'\n",
    "table_ic_traffic_2012_quick = 'hlwk_simple_traffic_2012_quick'\n",
    "table_ic_cdr_2012_quick = 'hlwk_simple_cdr_2012_quick'\n",
    "\n",
    "table_ic_wide_table_2101_quick = 'hlwk_2101_quick'\n",
    "table_ic_traffic_2101_quick = 'hlwk_simple_traffic_2101_quick'\n",
    "table_ic_cdr_2101_quick = 'hlwk_simple_cdr_2101_quick'\n",
    "\n",
    "\n",
    "# int3bas\n",
    "if platform == 'group':\n",
    "    table_dict['ic_info_202004'] = 'tmp_app_exp_hlwk_label_info_202004'\n",
    "    table_dict['ic_info_202011'] = 'tmp_app_exp_hlwk_label_info_202011'\n",
    "    table_dict['ic_info_202012'] = 'tmp_app_exp_hlwk_label_info_202012'\n",
    "    table_dict['ic_info_202101'] = 'tmp_app_exp_hlwk_label_info_202101'\n",
    "    table_dict['ic_info_202102'] = 'tmp_app_exp_hlwk_label_info_202102'\n",
    "    table_dict['ic_info_202106'] = 'tmp_app_exp_hlwk_label_info_202106'\n",
    "    table_dict['ic_info_202107'] = 'tmp_app_exp_hlwk_label_info_202107'\n",
    "    table_dict['ic_info_202108'] = 'tmp_app_exp_hlwk_label_info_202108'\n",
    "elif platform == 'old':\n",
    "    table_dict['ic_info_202109'] = 'tmp_app_exp_hlwk_label_info_202109'\n",
    "    table_dict['ic_info_202110'] = 'view_tmp_app_exp_hlwk_label_info_202110_apn_15303345281_20201124_001'\n",
    "    table_dict['ic_info_202111'] = 'view_tmp_app_exp_hlwk_label_info_202111_apn_15303345281_20201124_001'\n",
    "    table_dict['ic_info_202112'] = 'view_tmp_app_exp_hlwk_label_info_202112_apn_15303345281_20201124_001'\n",
    "\n",
    "\n",
    "\n",
    "if platform == 'group':\n",
    "    database_dict['ic_info_202004'] = group_project_database\n",
    "    database_dict['ic_info_202011'] = group_project_database\n",
    "    database_dict['ic_info_202012'] = group_project_database\n",
    "\n",
    "else:\n",
    "    database_dict['ic_info_202109'] = old_project_database_bas\n",
    "    database_dict['ic_info_202111'] = old_project_database_sjwj\n",
    "    database_dict['ic_info_202112'] = old_project_database_sjwj\n",
    "    \n",
    "# int3ass\n",
    "\n",
    "if platform == 'group':\n",
    "    table_dict['ic_halt_202011'] = 'tmp_app_exp_stop_user_202011_to_202102'\n",
    "    table_dict['ic_halt_202012'] = 'tmp_app_exp_stop_user_202011_to_202102'\n",
    "    table_dict['ic_halt_202101'] = 'tmp_app_exp_stop_user_202011_to_202102'\n",
    "    table_dict['ic_halt_202102'] = 'tmp_app_exp_stop_user_202011_to_202102'    \n",
    "    table_dict['ic_halt_202108'] = 'tmp_app_exp_stop_user_202011_to_202102'\n",
    "    table_dict['ic_halt_202109'] = 'tmp_app_exp_stop_user_202011_to_202102'\n",
    "else:\n",
    "    table_dict['ic_halt_202011'] = 'tmp_app_exp_stop_user_202011'\n",
    "    table_dict['ic_halt_202012'] = 'tmp_app_exp_stop_user_202012'\n",
    "    table_dict['ic_halt_202101'] = 'tmp_app_exp_stop_user_202101'\n",
    "    table_dict['ic_halt_202102'] = 'tmp_app_exp_stop_user_202102'    \n",
    "    table_dict['ic_halt_202108'] = 'tmp_app_exp_stop_user_202108'\n",
    "    table_dict['ic_halt_202109'] = 'tmp_app_exp_stop_user_202109'\n",
    "    table_dict['ic_halt_202110'] = 'view_tmp_app_exp_stop_user_202110_apn_15303345281_20201124_001'\n",
    "    table_dict['ic_halt_202111'] = 'view_tmp_app_exp_stop_user_202111_apn_15303345281_20201124_001'\n",
    "    \n",
    "if platform == 'group':\n",
    "    database_dict['ic_halt_202011'] = group_project_database\n",
    "    database_dict['ic_halt_202012'] = group_project_database\n",
    "    database_dict['ic_halt_202101'] = group_project_database\n",
    "    database_dict['ic_halt_202102'] = group_project_database\n",
    "    database_dict['ic_halt_202106'] = group_project_database\n",
    "    database_dict['ic_halt_202107'] = group_project_database    \n",
    "    database_dict['ic_halt_202108'] = group_project_database\n",
    "    database_dict['ic_halt_202109'] = group_project_database\n",
    "else:\n",
    "    # database_dict[table_ic_halted_2011] = old_project_database_2\n",
    "    # database_dict[table_ic_halted_2012] = old_project_database_2\n",
    "    # database_dict[table_ic_halted_2101] = old_project_database_2\n",
    "    # table_database_dict[table_ic_halted_2102] = old_project_database_2   \n",
    "    # table_database_dict[table_ic_halted_2108] = old_project_database_2\n",
    "    # table_database_dict[table_ic_halted_2109] = old_project_database_2\n",
    "    database_dict['ic_halt_202110'] = old_project_database_sjwj\n",
    "    database_dict['ic_halt_202111'] = old_project_database_sjwj\n",
    "    pass\n",
    "    \n",
    "# inf3bsn\n",
    "table_ic_cdr_2011 = 'tmp_app_exp_event_tgcdr_202011_new'\n",
    "table_ic_cdr_2012 = 'tmp_app_exp_event_tgcdr_202012_new'\n",
    "# table_ic_cdr_2011 = 'tmp_app_exp_event_tgcdr_202011'\n",
    "# table_ic_cdr_2012 = 'tmp_app_exp_event_tgcdr_202012'\n",
    "table_ic_cdr_2101 = 'tmp_app_exp_event_tgcdr_202101'\n",
    "table_ic_cdr_2102 = 'tmp_app_exp_event_tgcdr_202102'\n",
    "\n",
    "table_ic_cdr_2107 = 'tmp_app_exp_event_tgcdr_202107'\n",
    "table_ic_cdr_2109 = 'tmp_app_exp_event_tgcdr_202109'\n",
    "\n",
    "if platform == 'group':\n",
    "    table_dict['ic_cdr_202011'] = table_ic_cdr_2011\n",
    "    table_dict['ic_cdr_202012'] = table_ic_cdr_2012\n",
    "\n",
    "if platform == 'group':\n",
    "    database_dict['ic_cdr_202011'] = group_project_database\n",
    "    database_dict['ic_cdr_202012'] = group_project_database\n",
    "\n",
    "table_ic_traffic_2011 = 'tmp_app_exp_event_tgps_202011_2'\n",
    "table_ic_traffic_2012 = 'tmp_app_exp_event_tgps_202012_2'\n",
    "table_ic_traffic_2101 = 'tmp_app_exp_event_tgps_202101'\n",
    "table_ic_traffic_2102 = 'tmp_app_exp_event_tgps_202102'\n",
    "\n",
    "table_ic_traffic_2107 = 'tmp_app_exp_event_tgps_202107'\n",
    "table_ic_traffic_2109 = 'tmp_app_exp_event_tgps_202109'\n",
    "\n",
    "if platform == 'group':\n",
    "    table_dict['ic_traffic_202011'] = table_ic_traffic_2011\n",
    "    table_dict['ic_traffic_202012'] = table_ic_traffic_2012\n",
    "\n",
    "if platform == 'group':\n",
    "    database_dict['ic_traffic_202011'] = group_project_database\n",
    "    database_dict['ic_traffic_202012'] = group_project_database\n",
    "\n",
    "print(f'table_dict:{table_dict}')\n",
    "print(f'database_dict:{database_dict}')\n",
    "# other\n",
    "\n",
    "# Model\n",
    "# dataset\n",
    "train_dataset = None\n",
    "valid_dataset = None\n",
    "sequence_length = 30\n",
    "\n",
    "# function parameter\n",
    "pca_feature_num = 53\n",
    "pca_ratio = 0.99\n",
    "tp=0\n",
    "tn=0\n",
    "fp=0\n",
    "fn=0\n",
    "\n",
    "# hyperparameter\n",
    "# todo what does these para do\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "BATCHSIZE = 128\n",
    "CLASSES = 2\n",
    "DIR = os.getcwd()\n",
    "EPOCHS = 20\n",
    "LOG_INTERVAL = 20\n",
    "\n",
    "layer_num_list = range(5,11,1)\n",
    "hidden_units_num_list = [8,16,32,64,128,256]\n",
    "lr_list = [0.004,0.006,0.008,0.010]\n",
    "lr = 4.5*1e-3\n",
    "epoch_list = range(50,300,50)\n",
    "epochs = 100\n",
    "drop_out = 0.05\n",
    "batch_size = 256 # best\n",
    "# batch_size = 2048 # fastest\n",
    "valid_ratio=0.1\n",
    "train_ratio=0.8\n",
    "test_ratio=1-valid_ratio-train_ratio\n",
    "\n",
    "# profile\n",
    "time_count_dict = {}\n",
    "\n",
    "# sampling\n",
    "base_user_num = 100096\n",
    "user_num_ratio = 0.1\n",
    "# user_num_ratio = int(10*3.9)\n",
    "\n",
    "# 1. DirectTraffic Initialization\n",
    "baidu_series = DirectTraffic(\"baidu\", \"3532707000\", \"35327070000000000000\", [\"手机百度\",\"百度贴吧\", \"百度地图\",\"爱奇艺视频\",\"百度云盘\",\"百度手机助手\",\"好看视频\",\"百度百科\",\"秒懂百科\",\"百度知道\",\"百度文库\",\"百度输入法\",\"地图淘金\",\"百度糯米\",\"千千音乐\",\"百度翻译\",\"百度新闻\",\"百度浏览器\"])\n",
    "wangyi_series = DirectTraffic(\"wangyi\", \"3532707100\", \"35327071000000000000\", [\"网易新闻\",\"网易云音乐\",\"网易公开课\",\"邮箱大师\",\"易信\",\"有道云笔记\",\"网易云阅读\",\"网易云课堂\",\"网易蜗牛阅读\",\"网易100分\",\"荒野行动\",\"终结者2\",\"迷雾世界\",\"元气战姬学院\",\"坦克连\",\"梦幻西游\",\"大话西游\",\"阴阳师\",\"倩女幽魂\",\"率土之滨\",\"镇魔曲\",\"三国如龙传\",\"大航海之路\",\"汉王纷争\",\"决战平安京\",\"我的世界\",\"天下\",\"光明大陆\",\"玩具大乱斗\",\"格罗娅传奇\"])\n",
    "aiqiyi_series = DirectTraffic(\"aiqiyi\", \"3532704600\", \"35327046000000000000\", [\"爱奇艺\"])\n",
    "yixin_series = DirectTraffic(\"yixin\", \"3532702900\", \"35327029000000000000\", [\"易信\"])\n",
    "kuaishou_series = DirectTraffic(\"kuaishou\", \"1600000789\", \"16000007890000000000\", [\"快手\"])\n",
    "jinritoutiao_series = DirectTraffic(\"jinritoutiao\", \"3532706700\", \"35327067000000000000\", [\"今日头条\",\"今日头条lite版\",\"西瓜视频\",\"抖音火山版\",\"抖音短视频\",\"皮皮虾\",\"懂车帝\",\"半次元\"])\n",
    "\n",
    "# pptv_series = DirectTraffic(\"pptv\", \"3532706300\", \"35327063000000000000\", [\"PPTV\"])\n",
    "julishipin_series = DirectTraffic(\"julishipin\", \"3532706300\", \"35327063000000000000\", [\"聚力视频\"])\n",
    "\n",
    "tianyishixun_series = DirectTraffic(\"tianyishixun\", \"3532400103\", \"35324001000000000003\", [\"天翼视讯\"])\n",
    "\n",
    "youkutudou_series = DirectTraffic(\"youkutudou\", \"3532704700\", \"35327047000000000000\", [\"优酷土豆\"])\n",
    "aliyun_series = DirectTraffic(\"aliyun\", \"3532707700\", \"35327077000000000000\", [\"阿里云\"])\n",
    "alibaba_series = DirectTraffic(\"alibaba\", \"3532703600\", \"35327036000000000000\", [\"阿里巴巴\"])\n",
    "xiamiyinyue_series = DirectTraffic(\"xiamiyinyue\", \"3532706900\", \"35327069000000000000\", [\"虾米音乐\"])\n",
    "shuqixiaoshuo_series = DirectTraffic(\"shuqixiaoshuo\", \"3532706800\", \"35327068000000000000\", [\"书旗小说\"])\n",
    "\n",
    "gaodeditu_series = DirectTraffic(\"gaodeditu\", \"3532707200\", \"35327072000000000000\", [\"高德地图\"])\n",
    "\n",
    "shoutaotianmao_series = DirectTraffic(\"shoutaotianmao\", \"3532707600\", \"35327076000000000000\", [\"手淘天猫\"])\n",
    "\n",
    "xinlangweibo_series = DirectTraffic(\"xinlangweibo\", \"3532707400\", \"35327074000000000000\", [\"新浪微博\"])\n",
    "daxiaoyu_series = DirectTraffic(\"daxiaoyu\", \"0\", \"0\", [\"九游\",\"优酷视频\",\"UC浏览器\",\"虾米音乐\",\"高德地图\",\"书旗小说\",\"天翼视讯\",\"PP视频\"]) # 其中阿里云定向流量向支付宝、九游提供免流支持，阿里巴巴定向流量向阿里系应用提供图片、数据统计等资源支持。\n",
    "bilibili_series = DirectTraffic(\"bilibili\", \"1600000002\", \"16000000020000000000\", [\"哔哩哔哩\"])\n",
    "tengxunshipin_series = DirectTraffic(\"tengxunshipin\", \"3532704500\", \"35327045000000000000\", [\"腾讯视频\"])\n",
    "\n",
    "# delicated series\n",
    "ppzhushou_series = DirectTraffic(\"ppzhushou\", \"3532706500\", \"35327065000000000000\", [\"PP助手\"])\n",
    "ucliulanqi_series = DirectTraffic(\"ucliulanqi\", \"3532706500\", \"35327065000000000000\", [\"UC浏览器\"])\n",
    "mangguotv_series = DirectTraffic(\"mangguotv\", \"3532706500\", \"35327065000000000000\", [\"芒果TV\"])\n",
    "douyu_series = DirectTraffic(\"douyu\", \"3532706500\", \"35327065000000000000\", [\"斗鱼\"])\n",
    "liuliangkong_series = DirectTraffic(\"liuliangkong\", \"3532706500\", \"35327065000000000000\", [\"流量控\"])\n",
    "\n",
    "\n",
    "                \n",
    "# 2. Offer Initialization\n",
    "baidu_dashen = Offer(\"baidu_dashen\",\"1185443\", 19, 1, 100, [baidu_series, wangyi_series, aiqiyi_series, yixin_series, kuaishou_series], 800, 0.1, 40)\n",
    "\n",
    "# baijin_lianmeng = Offer(\"baijin_lianmeng\",\"1185824\", 19, 1, 100, [jinritoutiao_series, mangguotv_series, pptv_series, tianyishixun_series, douyu_series, youkutudou_series], 800, 0.1, 40)\n",
    "baijin_lianmeng = Offer(\"baijin_lianmeng\",\"1185824\", 19, 1, 100, [jinritoutiao_series, liuliangkong_series, julishipin_series, tianyishixun_series, youkutudou_series], 800, 0.1, 40)\n",
    "\n",
    "# dayu_taobao = Offer(\"dayu_taobao\",\"9012363\", 19, 1, 100, [aliyun_series, alibaba_series, youkutudou_series, xiamiyinyue_series, shuqixiaoshuo_series, ucliulanqi_series, gaodeditu_series, tianyishixun_series, ppzhushou_series, shoutaotianmao_series], 800, 0.1, 40)\n",
    "dayu_taobao = Offer(\"dayu_taobao\",\"9012363\", 19, 1, 100, [aliyun_series, alibaba_series, youkutudou_series, xiamiyinyue_series, shuqixiaoshuo_series, gaodeditu_series, tianyishixun_series, liuliangkong_series, shoutaotianmao_series], 800, 0.1, 40)\n",
    "\n",
    "# dabaijin = Offer(\"dabaijin\",'9001014', 19, 1, 100, [baidu_series, aiqiyi_series, wangyi_series, yixin_series, julishipin_series, tianyishixun_series, douyu_series, kuaishou_series], 800, 0.1, 40)\n",
    "dabaijin = Offer(\"dabaijin\",'9001014', 19, 1, 100, [baidu_series, aiqiyi_series, wangyi_series, yixin_series, julishipin_series, tianyishixun_series, liuliangkong_series, kuaishou_series], 800, 0.1, 40)\n",
    "\n",
    "# dayu = Offer(\"dayu\",'1184223', 19, 1, 100, [xinlangweibo_series, aliyun_series, alibaba_series, youkutudou_series, xiamiyinyue_series, shuqixiaoshuo_series, ucliulanqi_series, gaodeditu_series, tianyishixun_series, julishipin_series], 800, 0.1, 40)\n",
    "dayu = Offer(\"dayu\",'1184223', 19, 1, 100, [xinlangweibo_series, aliyun_series, alibaba_series, youkutudou_series, xiamiyinyue_series, shuqixiaoshuo_series, liuliangkong_series, gaodeditu_series, tianyishixun_series, julishipin_series], 800, 0.1, 40)\n",
    "\n",
    "# dawang = Offer(\"dawang\",'1185845', 19, 0, 100, [youkutudou_series, xiamiyinyue_series, shuqixiaoshuo_series, ucliulanqi_series, gaodeditu_series, baidu_series, aiqiyi_series, wangyi_series, tianyishixun_series, julishipin_series, alibaba_series, kuaishou_series], 800, 0.1, 40)\n",
    "dawang = Offer(\"dawang\",'1185845', 19, 0, 100, [youkutudou_series, xiamiyinyue_series, shuqixiaoshuo_series, liuliangkong_series, gaodeditu_series, baidu_series, aiqiyi_series, wangyi_series, tianyishixun_series, julishipin_series, alibaba_series, kuaishou_series], 800, 0.1, 40)\n",
    "\n",
    "zhenshi = Offer(\"zhenshi\",'9013357', 19, 2, 0, [bilibili_series], 800, 0.1, 40)\n",
    "xing_19 = Offer(\"xing_19\",'9015288', 19, 3, 0, [], 1000, 0.1, 40) # todo\n",
    "xing_19_rizu = Offer(\"xing_19_rizu\",'9015783', 19, 1, 100, [aiqiyi_series, youkutudou_series, jinritoutiao_series, kuaishou_series, baidu_series,wangyi_series, yixin_series], 800, 0.1, 40, mode=\"rizu\")\n",
    "\n",
    "\n",
    "# 3.Combine offer and direct traffic package\n",
    "src_offer_id_li = ['9011706', '1185443', '9011460', '9011707', '9014245', '1185824', '9012363', '9001014', '1184223', '1185845', '9014710', '9014715', '9013357', '9015727', '9015288', '9015730', '9015729', '9014714', '9015602', '9015783', '9016182', '9015712', '9016012', '9015717', '9015715', '9015718', '9014373', '9014375', '9014372', '9014376', '9014377', '9015487', '9015725', '9014374', '9015724', '9013355', '9015723']\n",
    "cur_src_offer_id_li = [\"1185443\",\"1185824\",\"9012363\",'9001014','1184223','1185845','9013357','9015288','9015783']\n",
    "\n",
    "offer_list = [baidu_dashen, baijin_lianmeng, dayu_taobao, dabaijin, dayu, dawang, zhenshi, xing_19, xing_19_rizu]\n",
    "# dt_series_list = [baidu_series,wangyi_series,aiqiyi_series,yixin_series, kuaishou_series,jinritoutiao_series, mangguotv_series, pptv_series,tianyishixun_series,douyu_series, youkutudou_series, aliyun_series,alibaba_series, xiamiyinyue_series, shuqixiaoshuo_series, ucliulanqi_series,gaodeditu_series,ppzhushou_series,shoutaotianmao_series,julishipin_series,xinlangweibo_series]\n",
    "dt_series_list = [baidu_series,wangyi_series,aiqiyi_series,yixin_series, kuaishou_series,jinritoutiao_series,tianyishixun_series, youkutudou_series, aliyun_series,alibaba_series, xiamiyinyue_series, shuqixiaoshuo_series,gaodeditu_series,shoutaotianmao_series,julishipin_series,xinlangweibo_series,liuliangkong_series]\n",
    "\n",
    "\n",
    "# --1Endings--"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-montreal",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "young-execution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --b\n",
    "\n",
    "'''\n",
    "Common Operator\n",
    "'''\n",
    "# 1. drop\n",
    "def spark_drop_column(spark_df, column):\n",
    "    spark_df = spark_df.drop(column)\n",
    "    print(f\"{column} has been deleted\")\n",
    "    return spark_df\n",
    "\n",
    "def spark_drop_columns(spark_df, columns):\n",
    "    spark_df = spark_df.drop(*columns)\n",
    "    print(f\"{columns} has been deleted\")\n",
    "    return spark_df    \n",
    "\n",
    "# 2. rename\n",
    "def spark_rename_column(spark_df, old_column_name, new_column_name):\n",
    "    spark_df = spark_df.withColumnRenamed(old_column_name, new_column_name)\n",
    "    print(f\"{old_column_name} has been replace by {new_column_name}\")\n",
    "    return spark_df\n",
    "\n",
    "def spark_rename_columns(spark_df, old_column_name_li, new_column_name_li):\n",
    "    for old_column_name, new_column_name in zip(old_column_name_li, new_column_name_li):\n",
    "        spark_df = spark_df.withColumnRenamed(old_column_name, new_column_name)\n",
    "        print(f\"{old_column_name} has been replace by {new_column_name}\")\n",
    "    return spark_df\n",
    "\n",
    "# 3. join\n",
    "def spark_join(main_spark_df, aux_spark_df, join_type = \"left_outer\", id_feat = \"prod_inst_id\"):\n",
    "    # from pyspark.sql.functions import col\n",
    "    \n",
    "    # main_spark_df.join(aux_spark_df, main_spark_df.id_feat == aux_spark_df.id_feat, join_type)\n",
    "    aux_id_feat = \"aux_\" + id_feat\n",
    "    aux_spark_df = spark_rename_column(aux_spark_df, id_feat, aux_id_feat)\n",
    "    main_spark_df = main_spark_df.join(aux_spark_df, main_spark_df[id_feat] == aux_spark_df[aux_id_feat], join_type)\n",
    "    main_spark_df = spark_drop_column(main_spark_df, aux_id_feat)\n",
    "    \n",
    "    new_feat_list = aux_spark_df.columns\n",
    "    new_feat_list.remove(aux_id_feat)\n",
    "    print(f\"{new_feat_list} has been added by key {id_feat}\")\n",
    "    \n",
    "    return main_spark_df\n",
    "    \n",
    "# 4. sample\n",
    "def spark_sample(spark_df, fraction, seed, sample_num = 10000):\n",
    "    spark_df = spark_df.sample(fraction = fraction, seed = seed, withReplacement = False).limit(sample_num)\n",
    "    \n",
    "    return spark_df\n",
    "\n",
    "# 5. select\n",
    "def spark_select_columns(spark_df, selected_columns):\n",
    "    spark_df = spark_df.select(*selected_columns)\n",
    "    print(f\"{selected_columns} have been selected\")\n",
    "    return spark_df\n",
    "\n",
    "# 6. union\n",
    "def spark_unionAll(dfs, common_feat_li):\n",
    "    from functools import reduce\n",
    "    from pyspark.sql import DataFrame   \n",
    "    \n",
    "    spark_dfs = []\n",
    "    \n",
    "    for spark_df in dfs:\n",
    "        spark_df = spark_select_columns(spark_df, common_feat_li)\n",
    "        spark_dfs.append(spark_df)\n",
    "    \n",
    "    return reduce(DataFrame.unionAll, spark_dfs)\n",
    "\n",
    "# 7. check\n",
    "def check_all_spark(spark_df, sample_num = 10000):\n",
    "    spark_df = spark_sample(spark_df, 0.05, sample_num)\n",
    "    pd_df = spark_df.toPandas()\n",
    "    print(\"check_all_spark:\")\n",
    "    check_all(pd_df)\n",
    "    \n",
    "    return \n",
    "\n",
    "'''\n",
    "Feature Operator For churner\n",
    "'''\n",
    "# traffic\n",
    "\n",
    "# check_spark_df(tmp_set)\n",
    "\n",
    "'''\n",
    "package fit\n",
    "'''\n",
    "# 1. save ic id\n",
    "# ic_df_2011 = parquet2sparkdf(\"ic\",\"wide\",\"202011\",\"5\")\n",
    "# ic_id_df_2011 = ic_df_2011.select(col(\"prod_inst_id\"))\n",
    "# sparkdf2hive(ic_id_df_2011,\"lr_pro2019_role2_45.ic_id_2011\")\n",
    "\n",
    "# # 2. extract feature of daily directed traffic \n",
    "# # ic_df_2011 = parquet2sparkdf(\"ic\",\"wide\",\"202011\",\"5\")\n",
    "# dt_date_li = period_generator(\"202011\",15,1)\n",
    "# # dt_date_li = [\"20201101\"]\n",
    "\n",
    "# id_table = \"lr_pro2019_role2_45.ic_id_202011\"\n",
    "# ic_seq_df = None\n",
    "# for dt_date in dt_date_li:\n",
    "#     # dt_df = simple_hiveql2df(f\"SELECT t3.prod_inst_id, sum(t3.byte_in/1024/1024) AS dt_byte_in_sum_{dt_date}, sum(t3.byte_out/1024/1024) AS dt_byte_out_sum_{dt_date}, sum(t3.duration/60) AS dt_duration_sum_{dt_date}, count(t3.prod_inst_id) AS dt_record_num_{dt_date} FROM ( SELECT t1.prod_inst_id,t1.byte_in,t1.byte_out,t1.duration FROM {id_table} t0 INNER JOIN hunan843.tmp_app_exp_event_tgps_202011_2 t1 ON (t0.prod_inst_id = t1.prod_inst_id) INNER JOIN hunan843.tmp_sp_content_l t2 ON (t1.cpid = t2.cpid AND t1.is_hlwk_user = 1 and t1.day_id = {dt_date} ) ) t3 GROUP BY t3.prod_inst_id\")\n",
    "#     dt_df = simple_hiveql2df(f\"SELECT t3.prod_inst_id, sum(t3.byte_in/1024/1024) AS dt_byte_in_sum_{dt_date}, sum(t3.byte_out/1024/1024) AS dt_byte_out_sum_{dt_date}, sum(t3.duration/60) AS dt_duration_sum_{dt_date}, count(t3.prod_inst_id) AS dt_record_num_{dt_date} FROM ( SELECT t1.prod_inst_id,t1.byte_in,t1.byte_out,t1.duration FROM {id_table} t0 INNER JOIN hunan843.tmp_app_exp_event_tgps_202011_2 t1 ON (t0.prod_inst_id = t1.prod_inst_id) INNER JOIN hunan843.tmp_sp_content_l t2 ON (t1.cpid = t2.cpid AND t1.is_hlwk_user = 1 and t1.day_id = {dt_date} ) ) t3 GROUP BY t3.prod_inst_id\")    \n",
    "#     if ic_seq_df is None:\n",
    "#         ic_seq_df = dt_df\n",
    "#     else:    \n",
    "#         ic_seq_df= ic_seq_df.join(dt_df,'prod_inst_id',\"left_outer\")\n",
    "# check_spark_df(ic_seq_df)\n",
    "\n",
    "# dt_date_li = period_generator(\"202011\",15,16)\n",
    "# # dt_date_li = [\"20201101\"]\n",
    "# ic_seq_df2 = None\n",
    "# for dt_date in dt_date_li:\n",
    "#     dt_df = simple_hiveql2df(f\"SELECT t3.prod_inst_id, sum(t3.byte_in/1024/1024) AS dt_byte_in_sum_{dt_date}, sum(t3.byte_out/1024/1024) AS dt_byte_out_sum_{dt_date}, sum(t3.duration/60) AS dt_duration_sum_{dt_date}, count(t3.prod_inst_id) AS dt_record_num_{dt_date} FROM ( SELECT t1.prod_inst_id,t1.byte_in,t1.byte_out,t1.duration FROM lr_pro2019_role2_45.ic_id_2011 t0 INNER JOIN hunan843.tmp_app_exp_event_tgps_202011_2 t1 ON (t0.prod_inst_id = t1.prod_inst_id) INNER JOIN hunan843.tmp_sp_content_l t2 ON (t1.cpid = t2.cpid AND t1.is_hlwk_user = 1 and t1.day_id = {dt_date} ) ) t3 GROUP BY t3.prod_inst_id\")\n",
    "#     if ic_seq_df2 is None:\n",
    "#         ic_seq_df2 = dt_df\n",
    "#     else:    \n",
    "#         ic_seq_df2= ic_seq_df2.join(dt_df,'prod_inst_id',\"left_outer\")\n",
    "# check_spark_df(ic_seq_df2)\n",
    "# # check_spark_df(ic_df_2011)\n",
    "\n",
    "# 3. extract feature of monthly app series traffic and common traffic\n",
    "# tmp_df = simple_hiveql2df(f'SELECT * FROM hunan843.tmp_sp_content_l t1 WHERE sp_content_name like \"爱奇艺%\"' )\n",
    "# # tmp_df = simple_hiveql2df(f'SELECT * FROM hunan843.tmp_sp_content_l t1 limit 10' )\n",
    "# check_spark_df(tmp_df)\n",
    "\n",
    "# dt_cp_id_list = []\n",
    "# for dt in dt_series_list:\n",
    "#     dt_cp_id_list.append(dt.get_cp_id())\n",
    "# print(f\"all:len(dt_cp_id_list):{len(dt_cp_id_list)}\")    \n",
    "# print(f\"dt_cp_id_list:{dt_cp_id_list}\")\n",
    "# dt_cp_id_tuple = tuple(dt_cp_id_list)\n",
    "\n",
    "# cp_id_df = simple_hiveql2df(f'SELECT * FROM hunan843.tmp_sp_content_l t1 WHERE t1.cpid in {dt_cp_id_tuple}' )\n",
    "# cp_id_pd_df = cp_id_df.toPandas()\n",
    "\n",
    "# dt_cp_id_list = list(cp_id_pd_df[\"cpid\"].values)\n",
    "# dt_cp_id_tuple = tuple(dt_cp_id_list)\n",
    "# dt_cp_name_list = list(cp_id_pd_df[\"sp_content_name\"].values)\n",
    "# # dt_cp_name_list = ['快手', '天翼视讯流量', '易信', '阿里巴巴', '爱奇艺', '优酷土豆', '聚力视频', '流量控', '今日头条', '书旗小说', '虾米音乐', '百度系应用', '网易系', '高德地图', '新浪微博', '手淘天猫', '阿里云']\n",
    "# dt_cp_name_list = ['kuaishou', 'tianyishixun', 'yixin', 'alibaba', 'aiqiyi', 'youkutudou', 'julishipin', 'liuliangkong', 'jinritoutiao', 'shuqixiaoshuo', 'xiamiyinyue', 'baidu', 'wangyi', 'gaodeditu', 'xinlangweibo', 'shoutaotianmao', 'aliyun']\n",
    "# print(f\"after filtering:len(dt_cp_id_list):{len(dt_cp_id_list)}\")\n",
    "# print(f\"dt_cp_id_list:{dt_cp_id_list}\")\n",
    "# print(f\"dt_cp_name_list:{dt_cp_name_list}\")\n",
    "\n",
    "# monthly_trc_df = simple_hiveql2df(f'SELECT prod_inst_id, sum(byte_in/1024/1024)+sum(byte_out/1024/1024) AS cm_tfc_sum FROM hunan843.tmp_app_exp_event_tgps_202011_2 t1 WHERE t1.cpid = \"3000000000\" AND t1.is_hlwk_user = 1 AND t1.day_id BETWEEN \"20201101\" AND \"20201130\" group by prod_inst_id' ) # cm_trc_df\n",
    "# # check_spark_df(monthly_trc_df)\n",
    "\n",
    "# for i,(dt_cp_id, dt_cp_name) in enumerate(zip(dt_cp_id_tuple, dt_cp_name_list)):\n",
    "#     if i < len(dt_cp_id_tuple) / 2:\n",
    "#         dt_trc_df = simple_hiveql2df(f'SELECT prod_inst_id, sum(byte_in/1024/1024)+sum(byte_out/1024/1024) AS {dt_cp_name}_tfc_sum FROM hunan843.tmp_app_exp_event_tgps_202011_2 t1 WHERE t1.cpid = {dt_cp_id} AND t1.is_hlwk_user = 1 AND t1.day_id BETWEEN \"20201101\" AND \"20201130\" group by prod_inst_id' )\n",
    "#         monthly_trc_df= monthly_trc_df.join(dt_trc_df,'prod_inst_id',\"fullouter\")\n",
    "# check_spark_df(monthly_trc_df)\n",
    "\n",
    "# sparkdf2parquet(monthly_trc_df, \"ic\", \"seq\", \"202011\", \"7\")\n",
    "\n",
    "# monthly_trc_df = None\n",
    "# for i,(dt_cp_id, dt_cp_name) in enumerate(zip(dt_cp_id_tuple, dt_cp_name_list)):\n",
    "#     if i >= len(dt_cp_id_tuple) / 2:\n",
    "#         dt_trc_df = simple_hiveql2df(f'SELECT prod_inst_id, sum(byte_in/1024/1024)+sum(byte_out/1024/1024) AS {dt_cp_name}_tfc_sum FROM hunan843.tmp_app_exp_event_tgps_202011_2 t1 WHERE t1.cpid = {dt_cp_id} AND t1.is_hlwk_user = 1 AND t1.day_id BETWEEN \"20201101\" AND \"20201130\" group by prod_inst_id' )\n",
    "#         if monthly_trc_df is None:\n",
    "#             monthly_trc_df = dt_trc_df\n",
    "#         else:    \n",
    "#             monthly_trc_df= monthly_trc_df.join(dt_trc_df,'prod_inst_id',\"fullouter\")\n",
    "# check_spark_df(monthly_trc_df)\n",
    "\n",
    "# sparkdf2parquet(monthly_trc_df, \"ic\", \"seq\", \"202011\", \"8\")\n",
    "\n",
    "# # 4. merge \n",
    "# ic_seq_df1 = parquet2sparkdf(\"ic\", \"seq\", \"202011\", \"7\")\n",
    " \n",
    "# # ic_seq_df1 = ic_seq_df1.withColumnRenamed('shoutaotiaomao_tfc_sum','shoutaotianmao_tfc_sum') #???\n",
    "# ic_seq_df1 = ic_seq_df1.withColumnRenamed('tianyi_tfc_sum','tianyishixun_tfc_sum')\n",
    "\n",
    "# ic_seq_df2 = parquet2sparkdf(\"ic\", \"seq\", \"202011\", \"8\")\n",
    "# ic_seq_df = ic_seq_df1.join(ic_seq_df2,'prod_inst_id',\"fullouter\")\n",
    "# # check_spark_df(ic_seq_df)\n",
    "\n",
    "# # ic_seq_df = parquet2sparkdf(\"ic\", \"seq\", \"202011\", \"1\")\n",
    "# # ic_seq_df= ic_seq_df.join(cm_trc_df,'prod_inst_id',\"left_outer\")\n",
    "# # ic_seq_df= ic_seq_df.join(aqy_trc_df,'prod_inst_id',\"left_outer\")\n",
    "# # check_spark_df(ic_seq_df)\n",
    "# # sparkdf2parquet(ic_seq_df, \"ic\", \"seq\", \"202011\", \"4\")\n",
    "\n",
    "# 5. cal dt traffic match degree\n",
    "\n",
    "# # offer\n",
    "# # 1. extract offer \n",
    "# dt_series_name_list = []\n",
    "# for dt_series in dt_series_list:\n",
    "#     dt_series_name_list.append(dt_series.get_name())\n",
    "\n",
    "# print(f\"dt_series_name_list:{dt_series_name_list}\")\n",
    "\n",
    "# name_li = [\"offer_name\",\"src_offer_id\", \"price\", \"traffic\", \"comm\", \"traffic_cost\", \"comm_cost\"]\n",
    "# name_li.extend(dt_series_name_list)\n",
    "# print(f\"len(name_li):{len(name_li)}\")\n",
    "# print(f\"name_li:{name_li}\")\n",
    "\n",
    "# offer_matrix = []\n",
    "# for offer in offer_list:\n",
    "#     offer_matrix.append(offer.output_list())\n",
    "# print(f\"offer_matrix:{offer_matrix}\")    \n",
    "# offer_pd_df = pd.DataFrame(np.array(offer_matrix))\n",
    "# offer_pd_df.columns = name_li\n",
    "# # check_all(offer_pd_df,100)\n",
    "\n",
    "# offer_df,_ = pandasdf2sparkdf(offer_pd_df)\n",
    "# # check_spark_df(offer_df)\n",
    "\n",
    "# # 2. merge offer_df and user_df\n",
    "# ic_df = parquet2sparkdf(\"ic\",\"wide\",\"202011\",\"5\")\n",
    "# ic_df = ic_df.select(\"prod_inst_id\",\"own_age\",\"balance\",\"rel_amount\",\"offer_grade\",\"d_stop_cnt_sum_3m\",\"s_stop_cnt_sum_3m\",\"all_app_use_times\",\"src_offer_id\",\"halt_2012\")\n",
    "# ic_df= ic_df.join(offer_df, \"src_offer_id\", \"left_outer\")\n",
    "# # check_spark_df(ic_df)\n",
    "\n",
    "# # 3. contrast offer_df and seq_df\n",
    "# seq_li = list(ic_seq_df.toPandas().columns)\n",
    "# offer_li = list(offer_df.toPandas().columns)\n",
    "\n",
    "# # seq_li = ['kuaishou_tfc_sum', 'tianyi_tfc_sum', 'yixin_tfc_sum', 'alibaba_tfc_sum', 'aiqiyi_tfc_sum', 'youkutudou_tfc_sum', 'julishipin_tfc_sum', 'liuliangkong_tfc_sum', 'jinritoutiao_tfc_sum', 'shuqixiaoshuo_tfc_sum', 'xiamiyinyue_tfc_sum', 'baidu_tfc_sum', 'wangyi_tfc_sum', 'gaodeditu_tfc_sum', 'xinlangweibo_tfc_sum', 'shoutaotiaomao_tfc_sum', 'aliyun_tfc_sum']\n",
    "# # offer_li = ['baidu', 'wangyi', 'aiqiyi', 'yixin', 'kuaishou', 'jinritoutiao', 'pptv', 'tianyishixun', 'youkutudou', 'aliyun', 'alibaba', 'xiamiyinyue', 'shuqixiaoshuo', 'gaodeditu', 'shoutaotianmao', 'julishipin', 'xinlangweibo', 'liuliangkong']\n",
    "# inter_li = []\n",
    "# seq_ind_li = []\n",
    "\n",
    "# for seq in seq_li:\n",
    "#     if '_' in seq:\n",
    "#         seq = seq[:seq.index('_')]\n",
    "#         if seq in offer_li:\n",
    "#             inter_li.append(seq)\n",
    "#             offer_li.remove(seq)\n",
    "#         else:\n",
    "#             seq_ind_li.append(seq)\n",
    "        \n",
    "        \n",
    "# print(f\"inter_li:{inter_li}\") \n",
    "# print(f\"seq_ind_li:{seq_ind_li}\") \n",
    "# print(f\"offer_ind_li:{offer_li}\") \n",
    "\n",
    "# # Merge Feat Process\n",
    "# ex_ic_df = ic_df.join(ic_seq_df,'prod_inst_id',\"left_outer\")\n",
    "# # check_spark_df(ex_ic_df)\n",
    "# # sparkdf2parquet(ex_ic_df,\"ic\",\"ex_wide\",\"202011\",\"1\")\n",
    "\n",
    "\n",
    "\n",
    "# '''\n",
    "# net speed\n",
    "# '''\n",
    "# # tmp_set = simple_hiveql2df('SELECT t2.prod_inst_id as prod_inst_id, t2.net_speed as 201101_1st_speed from(SELECT t1.prod_inst_id as prod_inst_id, t1.net_speed as net_speed, row_number() over (PARTITION BY t1.prod_inst_id ORDER BY t1.net_speed DESC) as rn from(SELECT prod_inst_id, byte_in / duration as net_speed FROM hunan843.tmp_app_exp_event_tgps_202011_2 where is_hlwk_user = 1 and day_id = \"20201101\" and duration > 0 )t1 ) t2 where t2.rn = 1')\n",
    "# # tmp_set = simple_hiveql2df('SELECT t2.prod_inst_id as prod_inst_id, t2.net_speed as 201115_1st_speed from(SELECT t1.prod_inst_id as prod_inst_id, t1.net_speed as net_speed, row_number() over (PARTITION BY t1.prod_inst_id ORDER BY t1.net_speed DESC) as rn from(SELECT prod_inst_id, byte_in / duration as net_speed FROM hunan843.tmp_app_exp_event_tgps_202011_2 where is_hlwk_user = 1 and day_id = \"20201115\" and duration > 0 )t1 ) t2 where t2.rn = 1')\n",
    "# # check_spark_df(tmp_set,100)\n",
    "\n",
    "# ex_ic_df = parquet2sparkdf(\"ic\",\"ex_wide\",\"202011\",\"6\")\n",
    "# ex_ic_df = ex_ic_df.filter(ex_ic_df.halt_2012 == 1)\n",
    "\n",
    "# daily_net_speed_df = simple_hiveql2df(\"SELECT a.prod_inst_id, MAX((byte_in+byte_out)/1024/duration) as max_net_speed2 FROM hunan843.tmp_app_exp_event_tgps_202011_2 a inner join lr_pro2019_role2_45.ic_churner_id_202011 b on (a.prod_inst_id = b.prod_inst_id) WHERE a.is_hlwk_user = 1 AND a.duration > 0 AND a.day_id = '20201101' GROUP BY a.prod_inst_id\")\n",
    "# monthly_net_speed_df = simple_hiveql2df(\"SELECT a.prod_inst_id, AVG((byte_in+byte_out)/1024/duration) as avg_net_speed2 FROM hunan843.tmp_app_exp_event_tgps_202011_2 a inner join lr_pro2019_role2_45.ic_churner_id_202011 b on (a.prod_inst_id = b.prod_inst_id) WHERE a.is_hlwk_user = 1 AND a.duration > 0 AND a.byte_in > 10240 AND a.day_id BETWEEN '20201101' AND '20201130' GROUP BY a.prod_inst_id \")\n",
    "# # # # check_spark_df(day_net_speed_df,100)\n",
    "\n",
    "# # day_net_speed_pd_df = day_net_speed_df.toPandas()\n",
    "# # check_all(day_net_speed_pd_df) \n",
    "\n",
    "# net_speed_up = 400\n",
    "# # plot_cdf(day_net_speed_pd_df['max_net_speed'],'Day Max Net Speed(KB/S)',net_speed_up)\n",
    "# # plot_cdf(day_net_speed_pd_df['avg_net_speed'],'Monthly AVG Net Speed(KB/S)',net_speed_up)\n",
    "# # plot_hist(day_net_speed_pd_df['max_net_speed'],'day_max_net_speed',100,net_speed_up)\n",
    "# # plot_cdf_hist(day_net_speed_pd_df['max_net_speed'],'Day Max Net Speed(KB/S)',net_speed_up,100)\n",
    "\n",
    "# # merge\n",
    "# ex_ic_df = ex_ic_df.join(monthly_net_speed_df,'prod_inst_id',\"left_outer\")\n",
    "# ex_ic_df = ex_ic_df.join(daily_net_speed_df,'prod_inst_id',\"left_outer\")\n",
    "# sparkdf2parquet(ex_ic_df,\"ic_churn\",\"ex_wide\",\"202011\",\"1\")\n",
    "# # check_spark_df(ex_ic_df)\n",
    "# # sparkdf2parquet(ex_ic_df,\"ic\",\"ex_wide\",\"202011\",\"1\")\n",
    "# # sparkdf2parquet(ex_ic_df,\"ic\",\"ex_wide\",\"202011\",\"2\")\n",
    "\n",
    "# # check pile\n",
    "# check_cols = last_dt_name_li.copy()\n",
    "# check_cols.extend(last_dt_seq_li)\n",
    "# check_cols.append(\"in_offer_trf_sum\")\n",
    "# check_cols.append(\"out_offer_trf_sum\")\n",
    "# check_sp_pd_li(ex_ic_df, check_cols, 1)\n",
    "# # check_spark_df(ex_ic_df)\n",
    "\n",
    "\n",
    "'''\n",
    "remote card\n",
    "'''\n",
    "def trc_remote_modeling():\n",
    "    # comm\n",
    "    comm_area_code_li = [\"0730\",\"0731\",\"0732\",\"0733\",\"0734\",\"0735\",\"0736\",\"0737\",\"0738\",\"0739\",\"0743\",\"0744\",\"0745\",\"0746\"]\n",
    "    comm_area_code_tuple = tuple(comm_area_code_li)\n",
    "    \n",
    "    \n",
    "    # traffic\n",
    "    trc_remote_df = simple_hiveql2df(f\"SELECT a.prod_inst_id, count(a.prod_inst_id) AS monthly_trf_remote_record_num FROM hunan843.tmp_app_exp_event_tgps_202011_2 a inner join lr_pro2019_role2_45.ic_churner_id_202011 b on (a.prod_inst_id = b.prod_inst_id) WHERE a.is_hlwk_user = 1 AND a.p_day_id BETWEEN '20201101' AND '20201130' AND a.calling_visit_region not in {comm_area_code_tuple} GROUP BY a.prod_inst_id \")\n",
    "    # trc_remote_df = simple_hiveql2df(f\"SELECT a.prod_inst_id, count(a.prod_inst_id) AS monthly_trf_remote_record_num FROM hunan843.tmp_app_exp_event_tgps_202011_2 a inner join (select prd_inst_id as prod_inst_id FROM hunan843.tmp_app_exp_hlwk_label_info_202011 WHERE is_hlwk_offer ='1' AND prod_status_name='正常' AND std_stgy_sale_dept_cd='12002' and lan_id = '20' limit 1) b on (a.prod_inst_id = b.prod_inst_id) WHERE a.is_hlwk_user = 1 AND a.p_day_id BETWEEN '20201101' AND '20201130' AND a.calling_visit_region not in {comm_area_code_tuple} GROUP BY a.prod_inst_id \")\n",
    "    # check_spark_df(trc_remote_df)\n",
    "    \n",
    "    print(\"Feature Enginnering: #. of remote traffic records monthly has been modeled\")\n",
    "    return trc_remote_df\n",
    "\n",
    "'''\n",
    "multi card \n",
    "'''\n",
    "def quantify_card(card_val):\n",
    "    if card_val == '未知':\n",
    "        return 0\n",
    "    else:\n",
    "        return 10\n",
    "    # if card_val == '未知':\n",
    "    #     return 0\n",
    "    # elif card_val == '中国电信':\n",
    "    #     return 10    \n",
    "    # elif card_val == '中国移动':\n",
    "    #     return 8\n",
    "    # elif card_val == '中国联通':\n",
    "    #     return 6\n",
    "    # else:\n",
    "    #     return 4\n",
    "\n",
    "def multi_card_modeling(spark_df):\n",
    "    udf_quantify_card = udf(lambda x: quantify_card(x), returnType=IntegerType())        \n",
    "    spark_df = spark_df.withColumn(\"card_weight\", udf_quantify_card(col(\"second_card_slot_l1m\")))\n",
    "    \n",
    "    print(\"Feature Enginnering: encoding of multiple card has been modeled\")\n",
    "    \n",
    "    return spark_df\n",
    "\n",
    "# '''\n",
    "# lan_id\n",
    "# '''\n",
    "\n",
    "\n",
    "\n",
    "# # traffic\n",
    "# lan_df = simple_hiveql2df(f\"select prd_inst_id as prod_inst_id, lan_id FROM hunan843.tmp_app_exp_hlwk_label_info_202011 WHERE is_hlwk_offer ='1' AND prod_status_name='正常' AND std_stgy_sale_dept_cd='12002'\")\n",
    "# # check_spark_df(lan_df)\n",
    "\n",
    "# # merge\n",
    "\n",
    "# ic_df = ic_df.join(trc_remote_df,'prod_inst_id',\"left_outer\")\n",
    "\n",
    "\n",
    "# sparkdf2parquet(ic_df,\"ic_churn\",\"ex_wide\",\"202011\",\"2\")\n",
    "\n",
    "\n",
    "# # merge basic feature\n",
    "# basic_feat = [\"own_age\",\"balance\",\"rel_amount\",\"offer_grade\",\"d_stop_cnt_sum_3m\",\"s_stop_cnt_sum_3m\",\"all_app_use_times\"]\n",
    "\n",
    "\n",
    "# '''\n",
    "# merge all churn feature\n",
    "# '''\n",
    "# 1.\n",
    "# ex_ic_df = parquet2sparkdf(\"ic\",\"ex_wide\",\"202011\",\"6\")\n",
    "# # check_spark_df(ex_ic_df)\n",
    "# # ex_ic_df = parquet2sparkdf(\"ic_ex\",\"wide\",\"202011\",\"5\")\n",
    "# # ex_ic_df2 = parquet2sparkdf(\"ic_ex\",\"wide\",\"202011\",\"4\")\n",
    "# # ex_ic_df2 = ex_ic_df2.select(\"prod_inst_id\",\"avg_net_speed\",\"max_net_speed\",\"monthly_trf_remote_record_num\",\"second_card_slot_l1m\",\"card_weight\")\n",
    "# # ex_ic_df_all = ex_ic_df.join(ex_ic_df2,'prod_inst_id',\"leftouter\")\n",
    "# ex_ic_df_all = ex_ic_df.join(lan_df,'prod_inst_id',\"leftouter\")\n",
    "# check_spark_df(ex_ic_df_all)\n",
    "# sparkdf2parquet(ex_ic_df_all,\"ic\",\"ex_wide\",\"202011\",\"7\")\n",
    "\n",
    "# # 2. merge churners' feature\n",
    "# feat1 = parquet2sparkdf(\"ic_churn\",\"ex_wide\",\"202011\",\"1\")\n",
    "# feat1 = feat1.drop(\"second_card_slot_l1m\",\"card_weight\",\"monthly_trf_remote_record_num\",\"avg_net_speed\",\"max_net_speed\")\n",
    "# feat2 = parquet2sparkdf(\"ic_churn\",\"ex_wide\",\"202011\",\"2\")\n",
    "# feat_all = feat1.join(feat2,\"prod_inst_id\",'left_outer')\n",
    "# feat_all = feat_all.withColumnRenamed(\"avg_net_speed2\",\"avg_net_speed\")\n",
    "# feat_all = feat_all.withColumnRenamed(\"max_net_speed2\",\"max_net_speed\")\n",
    "# feat_all = feat_all.withColumnRenamed(\"monthly_trf_remote_record_num\",\"monthly_tfc_remote_record_num\")\n",
    "# check_spark_df(feat_all)\n",
    "# sparkdf2parquet(feat_all,\"ic_churn\",\"ex_wide\",\"202011\",\"3\")\n",
    "\n",
    "def get_next_version(old_version):\n",
    "    return str(int(old_version)+1)\n",
    "\n",
    "# # group 1    \n",
    "# '''\n",
    "# Get Dataset to deal with\n",
    "# '''\n",
    "# user_type = \"ic_churn\"\n",
    "# table_type = \"ex_wide\"\n",
    "# date = \"202011\"\n",
    "# version = \"1\"\n",
    "\n",
    "# churner_spark_df = parquet2sparkdf(user_type, table_type, date, version)\n",
    "\n",
    "# ic_user_type = \"ic\"\n",
    "# ic_table_type = \"ex_wide\"\n",
    "# ic_date = \"202011\"\n",
    "# ic_version = \"7\"\n",
    "\n",
    "# ic_spark_df = parquet2sparkdf(ic_user_type, ic_table_type, ic_date, ic_version) \n",
    "\n",
    "# aux_user_type = \"ic\"\n",
    "# aux_table_type = \"wide\"\n",
    "# aux_date = \"202011\"\n",
    "# aux_version = \"5\"\n",
    "# aux_spark_df = parquet2sparkdf(aux_user_type, aux_table_type, aux_date, aux_version)\n",
    "\n",
    "# '''\n",
    "# feature engineering\n",
    "# '''\n",
    "# # 1. drop invalid feature\n",
    "# churner_spark_df = spark_drop_column(churner_spark_df, \"avg_net_speed\")\n",
    "# churner_spark_df = spark_drop_column(churner_spark_df, \"max_net_speed\")\n",
    "# churner_spark_df = spark_drop_column(churner_spark_df, \"monthly_trf_remote_record_num\")\n",
    "# churner_spark_df = spark_drop_column(churner_spark_df, \"card_weight\")\n",
    "# # 2. rename deuplicate feature\n",
    "# churner_spark_df = spark_rename_column(churner_spark_df, \"max_net_speed2\", \"max_net_speed\")\n",
    "# churner_spark_df = spark_rename_column(churner_spark_df, \"avg_net_speed2\", \"avg_net_speed\")\n",
    "# # 3. new feat 1\n",
    "# trc_remote_df = trc_remote_modeling()\n",
    "# # # check_spark_df(trc_remote_df)\n",
    "# # li1, li2, common_li, is_subset_flag = check_spark_sample_intersection(trc_remote_df, ic_churn_ex_wide_df, \"prod_inst_id\")\n",
    "# churner_spark_df = spark_join(churner_spark_df, trc_remote_df, \"left_outer\", \"prod_inst_id\")\n",
    "# # 4. new feat 2\n",
    "# churner_spark_df = spark_drop_column(churner_spark_df, \"second_card_slot_l1m\")\n",
    "# tmp_spark_df = spark_select_columns(aux_spark_df, [\"prod_inst_id\", \"second_card_slot_l1m\"])\n",
    "# churner_spark_df = spark_join(churner_spark_df, tmp_spark_df, \"left_outer\", \"prod_inst_id\")\n",
    "# churner_spark_df = multi_card_modeling(churner_spark_df)\n",
    "# # 5. check dataset for churners\n",
    "# # ic_spark_df.printSchema()\n",
    "# # check_all_spark(ic_spark_df)\n",
    "# # 6. check common feature and sample \n",
    "# common_feat_li = check_spark_feature_intersection(churner_spark_df, ic_spark_df)\n",
    "# nonchurner_spark_df = ic_spark_df.filter(ic_spark_df.halt_2012 == 0)\n",
    "# # 7. union churner and non-churner\n",
    "# target_spark_df = spark_unionAll([churner_spark_df, nonchurner_spark_df], common_feat_li)\n",
    "# # check_all_spark(target_spark_df)\n",
    "\n",
    "# '''\n",
    "# save new dataset\n",
    "# '''\n",
    "# target_user_type = \"ic\"\n",
    "# target_table_type = \"ex_wide\"\n",
    "# target_date = \"202011\"\n",
    "# target_version = \"8\"\n",
    "# sparkdf2parquet(target_spark_df, target_user_type, target_table_type, target_date, target_version)\n",
    "\n",
    "# --e\n",
    "# --2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressed-claim",
   "metadata": {},
   "source": [
    "## Load User Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "interpreted-board",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename:./ic_37w_2011.csv\n",
      "csv2df:./ic_37w_2011.csv succeeded\n",
      "(379064, 337)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 379064 entries, 0 to 379063\n",
      "Data columns (total 337 columns):\n",
      " #   Column                               Non-Null Count   Dtype  \n",
      "---  ------                               --------------   -----  \n",
      " 0   Unnamed: 0                           379064 non-null  int64  \n",
      " 1   prod_inst_id                         379064 non-null  float64\n",
      " 2   jinritoutiao_tfc_sum                 379064 non-null  float64\n",
      " 3   monthly_tfc_remote_record_num        379064 non-null  float64\n",
      " 4   shuqixiaoshuo_tfc_sum                379064 non-null  float64\n",
      " 5   julishipin_offer                     379064 non-null  float64\n",
      " 6   yixin_tfc_sum                        379064 non-null  float64\n",
      " 7   in_offer_tfc_sum                     379064 non-null  float64\n",
      " 8   card_weight                          379064 non-null  float64\n",
      " 9   alibaba_offer                        379064 non-null  float64\n",
      " 10  out_offer_tfc_sum                    379064 non-null  float64\n",
      " 11  liuliangkong_tfc_sum                 379064 non-null  float64\n",
      " 12  shoutaotianmao_tfc_sum               379064 non-null  float64\n",
      " 13  tianyishixun_tfc_sum                 379064 non-null  float64\n",
      " 14  shoutaotianmao_offer                 379064 non-null  float64\n",
      " 15  wangyi_offer                         379064 non-null  float64\n",
      " 16  baidu_offer                          379064 non-null  float64\n",
      " 17  liuliangkong_offer                   379064 non-null  float64\n",
      " 18  avg_net_speed                        379064 non-null  float64\n",
      " 19  tianyishixun_offer                   379064 non-null  float64\n",
      " 20  traffic                              379064 non-null  float64\n",
      " 21  youkutudou_tfc_sum                   379064 non-null  float64\n",
      " 22  comm_cost                            379064 non-null  float64\n",
      " 23  alibaba_tfc_sum                      379064 non-null  float64\n",
      " 24  wangyi_tfc_sum                       379064 non-null  float64\n",
      " 25  xinlangweibo_tfc_sum                 379064 non-null  float64\n",
      " 26  kuaishou_tfc_sum                     379064 non-null  float64\n",
      " 27  gaodeditu_tfc_sum                    379064 non-null  float64\n",
      " 28  aiqiyi_offer                         379064 non-null  float64\n",
      " 29  aliyun_offer                         379064 non-null  float64\n",
      " 30  julishipin_tfc_sum                   379064 non-null  float64\n",
      " 31  baidu_tfc_sum                        379064 non-null  float64\n",
      " 32  aiqiyi_tfc_sum                       379064 non-null  float64\n",
      " 33  shuqixiaoshuo_offer                  379064 non-null  float64\n",
      " 34  gaodeditu_offer                      379064 non-null  float64\n",
      " 35  cm_tfc_sum                           379064 non-null  float64\n",
      " 36  youkutudou_offer                     379064 non-null  float64\n",
      " 37  price                                379064 non-null  float64\n",
      " 38  xinlangweibo_offer                   379064 non-null  float64\n",
      " 39  yixin_offer                          379064 non-null  float64\n",
      " 40  jinritoutiao_offer                   379064 non-null  float64\n",
      " 41  traffic_cost                         379064 non-null  float64\n",
      " 42  max_net_speed                        379064 non-null  float64\n",
      " 43  aliyun_tfc_sum                       379064 non-null  float64\n",
      " 44  xiamiyinyue_offer                    379064 non-null  float64\n",
      " 45  xiamiyinyue_tfc_sum                  379064 non-null  float64\n",
      " 46  kuaishou_offer                       379064 non-null  float64\n",
      " 47  hlwk_offer_aim_flow_cm_round         379064 non-null  float64\n",
      " 48  rel_amount_round                     379064 non-null  float64\n",
      " 49  lan_id                               379064 non-null  float64\n",
      " 50  innet_dur                            379064 non-null  float64\n",
      " 51  balance                              379064 non-null  float64\n",
      " 52  offer_grade                          379064 non-null  float64\n",
      " 53  user_terminal_dur                    379064 non-null  float64\n",
      " 54  reg_term_price                       379064 non-null  float64\n",
      " 55  exceed_flow_l1m                      379064 non-null  float64\n",
      " 56  surplus_flow_l1m                     379064 non-null  float64\n",
      " 57  total_offer_flow_l1m                 379064 non-null  float64\n",
      " 58  use_dura_l1m                         379064 non-null  float64\n",
      " 59  use_offer_percent                    379064 non-null  float64\n",
      " 60  use_flow_l1m                         379064 non-null  float64\n",
      " 61  exceed_flow_l2m                      379064 non-null  float64\n",
      " 62  surplus_flow_l2m                     379064 non-null  float64\n",
      " 63  use_dura_l2m                         379064 non-null  float64\n",
      " 64  use_flow_l2m                         379064 non-null  float64\n",
      " 65  l1_use_offer_percent                 379064 non-null  float64\n",
      " 66  use_dura_avg_3m                      379064 non-null  float64\n",
      " 67  use_flow_avg_3m                      379064 non-null  float64\n",
      " 68  surplus_flow_cm                      379064 non-null  float64\n",
      " 69  use_dura_cm                          379064 non-null  float64\n",
      " 70  use_flow_cm                          379064 non-null  float64\n",
      " 71  games_app_days_l1m                   379064 non-null  float64\n",
      " 72  music_app_days_l1m                   379064 non-null  float64\n",
      " 73  video_app_days_l1m                   379064 non-null  float64\n",
      " 74  shopping_app_days_l1m                379064 non-null  float64\n",
      " 75  own_age                              379064 non-null  float64\n",
      " 76  avg_over_flow                        379064 non-null  float64\n",
      " 77  s_stop_cnt_sum_3m                    379064 non-null  float64\n",
      " 78  d_stop_cnt_sum_3m                    379064 non-null  float64\n",
      " 79  hlwk_offer_aim_flow_cm               379064 non-null  float64\n",
      " 80  l2_use_offer_percent                 379064 non-null  float64\n",
      " 81  rel_amount                           379064 non-null  float64\n",
      " 82  halt_2012                            379064 non-null  float64\n",
      " 83  own_gender_cd_index                  379064 non-null  float64\n",
      " 84  src_offer_id_index                   379064 non-null  float64\n",
      " 85  second_card_slot_l1m_index           379064 non-null  float64\n",
      " 86  rel_amount_round_target              379064 non-null  float64\n",
      " 87  hlwk_offer_aim_flow_cm_round_target  379064 non-null  float64\n",
      " 88  20201101_calling_duration            379064 non-null  float64\n",
      " 89  20201101_calling_record_num          379064 non-null  float64\n",
      " 90  20201101_called_duration             379064 non-null  float64\n",
      " 91  20201101_called_record_num           379064 non-null  float64\n",
      " 92  20201102_calling_duration            379064 non-null  float64\n",
      " 93  20201102_calling_record_num          379064 non-null  float64\n",
      " 94  20201102_called_duration             379064 non-null  float64\n",
      " 95  20201102_called_record_num           379064 non-null  float64\n",
      " 96  20201103_calling_duration            379064 non-null  float64\n",
      " 97  20201103_calling_record_num          379064 non-null  float64\n",
      " 98  20201103_called_duration             379064 non-null  float64\n",
      " 99  20201103_called_record_num           379064 non-null  float64\n",
      " 100 20201104_calling_duration            379064 non-null  float64\n",
      " 101 20201104_calling_record_num          379064 non-null  float64\n",
      " 102 20201104_called_duration             379064 non-null  float64\n",
      " 103 20201104_called_record_num           379064 non-null  float64\n",
      " 104 20201105_calling_duration            379064 non-null  float64\n",
      " 105 20201105_calling_record_num          379064 non-null  float64\n",
      " 106 20201105_called_duration             379064 non-null  float64\n",
      " 107 20201105_called_record_num           379064 non-null  float64\n",
      " 108 20201106_calling_duration            379064 non-null  float64\n",
      " 109 20201106_calling_record_num          379064 non-null  float64\n",
      " 110 20201106_called_duration             379064 non-null  float64\n",
      " 111 20201106_called_record_num           379064 non-null  float64\n",
      " 112 20201107_calling_duration            379064 non-null  float64\n",
      " 113 20201107_calling_record_num          379064 non-null  float64\n",
      " 114 20201107_called_duration             379064 non-null  float64\n",
      " 115 20201107_called_record_num           379064 non-null  float64\n",
      " 116 20201108_calling_duration            379064 non-null  float64\n",
      " 117 20201108_calling_record_num          379064 non-null  float64\n",
      " 118 20201108_called_duration             379064 non-null  float64\n",
      " 119 20201108_called_record_num           379064 non-null  float64\n",
      " 120 20201109_calling_duration            379064 non-null  float64\n",
      " 121 20201109_calling_record_num          379064 non-null  float64\n",
      " 122 20201109_called_duration             379064 non-null  float64\n",
      " 123 20201109_called_record_num           379064 non-null  float64\n",
      " 124 20201110_calling_duration            379064 non-null  float64\n",
      " 125 20201110_calling_record_num          379064 non-null  float64\n",
      " 126 20201110_called_duration             379064 non-null  float64\n",
      " 127 20201110_called_record_num           379064 non-null  float64\n",
      " 128 20201111_calling_duration            379064 non-null  float64\n",
      " 129 20201111_calling_record_num          379064 non-null  float64\n",
      " 130 20201111_called_duration             379064 non-null  float64\n",
      " 131 20201111_called_record_num           379064 non-null  float64\n",
      " 132 20201112_calling_duration            379064 non-null  float64\n",
      " 133 20201112_calling_record_num          379064 non-null  float64\n",
      " 134 20201112_called_duration             379064 non-null  float64\n",
      " 135 20201112_called_record_num           379064 non-null  float64\n",
      " 136 20201113_calling_duration            379064 non-null  float64\n",
      " 137 20201113_calling_record_num          379064 non-null  float64\n",
      " 138 20201113_called_duration             379064 non-null  float64\n",
      " 139 20201113_called_record_num           379064 non-null  float64\n",
      " 140 20201114_calling_duration            379064 non-null  float64\n",
      " 141 20201114_calling_record_num          379064 non-null  float64\n",
      " 142 20201114_called_duration             379064 non-null  float64\n",
      " 143 20201114_called_record_num           379064 non-null  float64\n",
      " 144 20201115_calling_duration            379064 non-null  float64\n",
      " 145 20201115_calling_record_num          379064 non-null  float64\n",
      " 146 20201115_called_duration             379064 non-null  float64\n",
      " 147 20201115_called_record_num           379064 non-null  float64\n",
      " 148 20201116_calling_duration            379064 non-null  float64\n",
      " 149 20201116_calling_record_num          379064 non-null  float64\n",
      " 150 20201116_called_duration             379064 non-null  float64\n",
      " 151 20201116_called_record_num           379064 non-null  float64\n",
      " 152 20201117_calling_duration            379064 non-null  float64\n",
      " 153 20201117_calling_record_num          379064 non-null  float64\n",
      " 154 20201117_called_duration             379064 non-null  float64\n",
      " 155 20201117_called_record_num           379064 non-null  float64\n",
      " 156 20201118_calling_duration            379064 non-null  float64\n",
      " 157 20201118_calling_record_num          379064 non-null  float64\n",
      " 158 20201118_called_duration             379064 non-null  float64\n",
      " 159 20201118_called_record_num           379064 non-null  float64\n",
      " 160 20201119_calling_duration            379064 non-null  float64\n",
      " 161 20201119_calling_record_num          379064 non-null  float64\n",
      " 162 20201119_called_duration             379064 non-null  float64\n",
      " 163 20201119_called_record_num           379064 non-null  float64\n",
      " 164 20201120_calling_duration            379064 non-null  float64\n",
      " 165 20201120_calling_record_num          379064 non-null  float64\n",
      " 166 20201120_called_duration             379064 non-null  float64\n",
      " 167 20201120_called_record_num           379064 non-null  float64\n",
      " 168 20201121_calling_duration            379064 non-null  float64\n",
      " 169 20201121_calling_record_num          379064 non-null  float64\n",
      " 170 20201121_called_duration             379064 non-null  float64\n",
      " 171 20201121_called_record_num           379064 non-null  float64\n",
      " 172 20201122_calling_duration            379064 non-null  float64\n",
      " 173 20201122_calling_record_num          379064 non-null  float64\n",
      " 174 20201122_called_duration             379064 non-null  float64\n",
      " 175 20201122_called_record_num           379064 non-null  float64\n",
      " 176 20201123_calling_duration            379064 non-null  float64\n",
      " 177 20201123_calling_record_num          379064 non-null  float64\n",
      " 178 20201123_called_duration             379064 non-null  float64\n",
      " 179 20201123_called_record_num           379064 non-null  float64\n",
      " 180 20201124_calling_duration            379064 non-null  float64\n",
      " 181 20201124_calling_record_num          379064 non-null  float64\n",
      " 182 20201124_called_duration             379064 non-null  float64\n",
      " 183 20201124_called_record_num           379064 non-null  float64\n",
      " 184 20201125_calling_duration            379064 non-null  float64\n",
      " 185 20201125_calling_record_num          379064 non-null  float64\n",
      " 186 20201125_called_duration             379064 non-null  float64\n",
      " 187 20201125_called_record_num           379064 non-null  float64\n",
      " 188 20201126_calling_duration            379064 non-null  float64\n",
      " 189 20201126_calling_record_num          379064 non-null  float64\n",
      " 190 20201126_called_duration             379064 non-null  float64\n",
      " 191 20201126_called_record_num           379064 non-null  float64\n",
      " 192 20201127_calling_duration            379064 non-null  float64\n",
      " 193 20201127_calling_record_num          379064 non-null  float64\n",
      " 194 20201127_called_duration             379064 non-null  float64\n",
      " 195 20201127_called_record_num           379064 non-null  float64\n",
      " 196 20201128_calling_duration            379064 non-null  float64\n",
      " 197 20201128_calling_record_num          379064 non-null  float64\n",
      " 198 20201128_called_duration             379064 non-null  float64\n",
      " 199 20201128_called_record_num           379064 non-null  float64\n",
      " 200 20201129_calling_duration            379064 non-null  float64\n",
      " 201 20201129_calling_record_num          379064 non-null  float64\n",
      " 202 20201129_called_duration             379064 non-null  float64\n",
      " 203 20201129_called_record_num           379064 non-null  float64\n",
      " 204 20201130_calling_duration            379064 non-null  float64\n",
      " 205 20201130_calling_record_num          379064 non-null  float64\n",
      " 206 20201130_called_duration             379064 non-null  float64\n",
      " 207 20201130_called_record_num           379064 non-null  float64\n",
      " 208 20201101_byte_in                     379064 non-null  float64\n",
      " 209 20201101_record_num                  379064 non-null  float64\n",
      " 210 20201101_byte_out                    379064 non-null  float64\n",
      " 211 20201101_duration                    379064 non-null  float64\n",
      " 212 20201102_byte_in                     379064 non-null  float64\n",
      " 213 20201102_record_num                  379064 non-null  float64\n",
      " 214 20201102_byte_out                    379064 non-null  float64\n",
      " 215 20201102_duration                    379064 non-null  float64\n",
      " 216 20201103_byte_in                     379064 non-null  float64\n",
      " 217 20201103_record_num                  379064 non-null  float64\n",
      " 218 20201103_byte_out                    379064 non-null  float64\n",
      " 219 20201103_duration                    379064 non-null  float64\n",
      " 220 20201104_byte_in                     379064 non-null  float64\n",
      " 221 20201104_record_num                  379064 non-null  float64\n",
      " 222 20201104_byte_out                    379064 non-null  float64\n",
      " 223 20201104_duration                    379064 non-null  float64\n",
      " 224 20201105_byte_in                     379064 non-null  float64\n",
      " 225 20201105_record_num                  379064 non-null  float64\n",
      " 226 20201105_byte_out                    379064 non-null  float64\n",
      " 227 20201105_duration                    379064 non-null  float64\n",
      " 228 20201106_byte_in                     379064 non-null  float64\n",
      " 229 20201106_record_num                  379064 non-null  float64\n",
      " 230 20201106_byte_out                    379064 non-null  float64\n",
      " 231 20201106_duration                    379064 non-null  float64\n",
      " 232 20201107_byte_in                     379064 non-null  float64\n",
      " 233 20201107_record_num                  379064 non-null  float64\n",
      " 234 20201107_byte_out                    379064 non-null  float64\n",
      " 235 20201107_duration                    379064 non-null  float64\n",
      " 236 20201108_byte_in                     379064 non-null  float64\n",
      " 237 20201108_record_num                  379064 non-null  float64\n",
      " 238 20201108_byte_out                    379064 non-null  float64\n",
      " 239 20201108_duration                    379064 non-null  float64\n",
      " 240 20201109_byte_in                     379064 non-null  float64\n",
      " 241 20201109_record_num                  379064 non-null  float64\n",
      " 242 20201109_byte_out                    379064 non-null  float64\n",
      " 243 20201109_duration                    379064 non-null  float64\n",
      " 244 20201110_byte_in                     379064 non-null  float64\n",
      " 245 20201110_record_num                  379064 non-null  float64\n",
      " 246 20201110_byte_out                    379064 non-null  float64\n",
      " 247 20201110_duration                    379064 non-null  float64\n",
      " 248 20201111_byte_in                     379064 non-null  float64\n",
      " 249 20201111_record_num                  379064 non-null  float64\n",
      " 250 20201111_byte_out                    379064 non-null  float64\n",
      " 251 20201111_duration                    379064 non-null  float64\n",
      " 252 20201112_byte_in                     379064 non-null  float64\n",
      " 253 20201112_record_num                  379064 non-null  float64\n",
      " 254 20201112_byte_out                    379064 non-null  float64\n",
      " 255 20201112_duration                    379064 non-null  float64\n",
      " 256 20201113_byte_in                     379064 non-null  float64\n",
      " 257 20201113_record_num                  379064 non-null  float64\n",
      " 258 20201113_byte_out                    379064 non-null  float64\n",
      " 259 20201113_duration                    379064 non-null  float64\n",
      " 260 20201114_byte_in                     379064 non-null  float64\n",
      " 261 20201114_record_num                  379064 non-null  float64\n",
      " 262 20201114_byte_out                    379064 non-null  float64\n",
      " 263 20201114_duration                    379064 non-null  float64\n",
      " 264 20201115_byte_in                     379064 non-null  float64\n",
      " 265 20201115_record_num                  379064 non-null  float64\n",
      " 266 20201115_byte_out                    379064 non-null  float64\n",
      " 267 20201115_duration                    379064 non-null  float64\n",
      " 268 20201116_byte_in                     379064 non-null  float64\n",
      " 269 20201116_record_num                  379064 non-null  float64\n",
      " 270 20201116_byte_out                    379064 non-null  float64\n",
      " 271 20201116_duration                    379064 non-null  float64\n",
      " 272 20201117_byte_in                     379064 non-null  float64\n",
      " 273 20201117_record_num                  379064 non-null  float64\n",
      " 274 20201117_byte_out                    379064 non-null  float64\n",
      " 275 20201117_duration                    379064 non-null  float64\n",
      " 276 20201118_byte_in                     379064 non-null  float64\n",
      " 277 20201118_record_num                  379064 non-null  float64\n",
      " 278 20201118_byte_out                    379064 non-null  float64\n",
      " 279 20201118_duration                    379064 non-null  float64\n",
      " 280 20201119_byte_in                     379064 non-null  float64\n",
      " 281 20201119_record_num                  379064 non-null  float64\n",
      " 282 20201119_byte_out                    379064 non-null  float64\n",
      " 283 20201119_duration                    379064 non-null  float64\n",
      " 284 20201120_byte_in                     379064 non-null  float64\n",
      " 285 20201120_record_num                  379064 non-null  float64\n",
      " 286 20201120_byte_out                    379064 non-null  float64\n",
      " 287 20201120_duration                    379064 non-null  float64\n",
      " 288 20201121_byte_in                     379064 non-null  float64\n",
      " 289 20201121_record_num                  379064 non-null  float64\n",
      " 290 20201121_byte_out                    379064 non-null  float64\n",
      " 291 20201121_duration                    379064 non-null  float64\n",
      " 292 20201122_byte_in                     379064 non-null  float64\n",
      " 293 20201122_record_num                  379064 non-null  float64\n",
      " 294 20201122_byte_out                    379064 non-null  float64\n",
      " 295 20201122_duration                    379064 non-null  float64\n",
      " 296 20201123_byte_in                     379064 non-null  float64\n",
      " 297 20201123_record_num                  379064 non-null  float64\n",
      " 298 20201123_byte_out                    379064 non-null  float64\n",
      " 299 20201123_duration                    379064 non-null  float64\n",
      " 300 20201124_byte_in                     379064 non-null  float64\n",
      " 301 20201124_record_num                  379064 non-null  float64\n",
      " 302 20201124_byte_out                    379064 non-null  float64\n",
      " 303 20201124_duration                    379064 non-null  float64\n",
      " 304 20201125_byte_in                     379064 non-null  float64\n",
      " 305 20201125_record_num                  379064 non-null  float64\n",
      " 306 20201125_byte_out                    379064 non-null  float64\n",
      " 307 20201125_duration                    379064 non-null  float64\n",
      " 308 20201126_byte_in                     379064 non-null  float64\n",
      " 309 20201126_record_num                  379064 non-null  float64\n",
      " 310 20201126_byte_out                    379064 non-null  float64\n",
      " 311 20201126_duration                    379064 non-null  float64\n",
      " 312 20201127_byte_in                     379064 non-null  float64\n",
      " 313 20201127_record_num                  379064 non-null  float64\n",
      " 314 20201127_byte_out                    379064 non-null  float64\n",
      " 315 20201127_duration                    379064 non-null  float64\n",
      " 316 20201128_byte_in                     379064 non-null  float64\n",
      " 317 20201128_record_num                  379064 non-null  float64\n",
      " 318 20201128_byte_out                    379064 non-null  float64\n",
      " 319 20201128_duration                    379064 non-null  float64\n",
      " 320 20201129_byte_in                     379064 non-null  float64\n",
      " 321 20201129_record_num                  379064 non-null  float64\n",
      " 322 20201129_byte_out                    379064 non-null  float64\n",
      " 323 20201129_duration                    379064 non-null  float64\n",
      " 324 20201130_byte_in                     379064 non-null  float64\n",
      " 325 20201130_record_num                  379064 non-null  float64\n",
      " 326 20201130_byte_out                    379064 non-null  float64\n",
      " 327 20201130_duration                    379064 non-null  float64\n",
      " 328 all_app_use_times                    379064 non-null  float64\n",
      " 329 202011_byte_in_list_entropy          379064 non-null  float64\n",
      " 330 202011_byte_out_list_entropy         379064 non-null  float64\n",
      " 331 202011_duration_list_entropy         379064 non-null  float64\n",
      " 332 202011_record_num_list_entropy       379064 non-null  float64\n",
      " 333 202011_byte_in_list_anomaly          379064 non-null  float64\n",
      " 334 202011_byte_out_list_anomaly         379064 non-null  float64\n",
      " 335 202011_duration_list_anomaly         379064 non-null  float64\n",
      " 336 202011_record_num_list_anomaly       379064 non-null  float64\n",
      "dtypes: float64(336), int64(1)\n",
      "memory usage: 974.6 MB\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Unnamed: 0  prod_inst_id  jinritoutiao_tfc_sum  \\\n",
      "count  379064.000000  3.790640e+05         379064.000000   \n",
      "mean   189531.500000  9.209584e+11          11108.061449   \n",
      "std    109426.495558  1.737385e+10          18491.999430   \n",
      "min         0.000000  3.373541e+07              0.000000   \n",
      "25%     94765.750000  9.216180e+11             44.776636   \n",
      "50%    189531.500000  9.219805e+11           2067.797807   \n",
      "75%    284297.250000  9.221594e+11          15053.961532   \n",
      "max    379063.000000  9.224257e+11         303248.347200   \n",
      "\n",
      "       monthly_tfc_remote_record_num  shuqixiaoshuo_tfc_sum  julishipin_offer  \\\n",
      "count                  379064.000000          379064.000000     379064.000000   \n",
      "mean                     2531.844361               0.209907          0.083535   \n",
      "std                      2417.412530              24.916996          0.276689   \n",
      "min                         0.000000               0.000000          0.000000   \n",
      "25%                       412.000000               0.000000          0.000000   \n",
      "50%                      2042.000000               0.000000          0.000000   \n",
      "75%                      3985.000000               0.000000          0.000000   \n",
      "max                     58105.000000           14800.065979          1.000000   \n",
      "\n",
      "       yixin_tfc_sum  in_offer_tfc_sum   card_weight  alibaba_offer  \\\n",
      "count  379064.000000     379064.000000  379064.00000  379064.000000   \n",
      "mean        0.012390      18743.762304       6.30722       0.035648   \n",
      "std         3.747356      25709.028479       4.82610       0.185412   \n",
      "min         0.000000          0.000000       0.00000       0.000000   \n",
      "25%         0.000000        726.656130       0.00000       0.000000   \n",
      "50%         0.000000       7867.709319      10.00000       0.000000   \n",
      "75%         0.000000      27813.112296      10.00000       0.000000   \n",
      "max      2167.712420     347905.091992      10.00000       1.000000   \n",
      "\n",
      "       out_offer_tfc_sum  liuliangkong_tfc_sum  shoutaotianmao_tfc_sum  \\\n",
      "count      379064.000000         379064.000000           379064.000000   \n",
      "mean         4373.075739              5.951189              211.883362   \n",
      "std         12668.184843            257.383519              594.115107   \n",
      "min             0.000000              0.000000                0.000000   \n",
      "25%            34.809433              0.000000                0.990159   \n",
      "50%           284.977301              0.000000               32.219560   \n",
      "75%          1420.537995              0.000000              180.707165   \n",
      "max        296034.941192          41916.399792            39158.911964   \n",
      "\n",
      "       tianyishixun_tfc_sum  shoutaotianmao_offer   wangyi_offer  \\\n",
      "count         379064.000000         379064.000000  379064.000000   \n",
      "mean               7.892646              0.001741       0.937821   \n",
      "std              195.729080              0.041691       0.241482   \n",
      "min                0.000000              0.000000       0.000000   \n",
      "25%                0.000000              0.000000       1.000000   \n",
      "50%                0.000000              0.000000       1.000000   \n",
      "75%                0.000000              0.000000       1.000000   \n",
      "max            40903.288150              1.000000       1.000000   \n",
      "\n",
      "         baidu_offer  liuliangkong_offer  avg_net_speed  tianyishixun_offer  \\\n",
      "count  379064.000000       379064.000000  379064.000000       379064.000000   \n",
      "mean        0.937821            0.085276      47.547377            0.085276   \n",
      "std         0.241482            0.279292      91.081156            0.279292   \n",
      "min         0.000000            0.000000       0.000000            0.000000   \n",
      "25%         1.000000            0.000000      14.933398            0.000000   \n",
      "50%         1.000000            0.000000      31.728890            0.000000   \n",
      "75%         1.000000            0.000000      55.870188            0.000000   \n",
      "max         1.000000            1.000000   12091.559889            1.000000   \n",
      "\n",
      "             traffic  youkutudou_tfc_sum  comm_cost  alibaba_tfc_sum  \\\n",
      "count  379064.000000       379064.000000   379064.0    379064.000000   \n",
      "mean        0.984900         1141.222457        0.1        97.326250   \n",
      "std         0.249261         6136.987132        0.0       267.680612   \n",
      "min         0.000000            0.000000        0.1         0.000000   \n",
      "25%         1.000000            0.130732        0.1         6.033685   \n",
      "50%         1.000000            7.121220        0.1        40.158868   \n",
      "75%         1.000000          131.710041        0.1       115.276034   \n",
      "max         3.000000       292998.535988        0.1     39980.677899   \n",
      "\n",
      "       wangyi_tfc_sum  xinlangweibo_tfc_sum  kuaishou_tfc_sum  \\\n",
      "count   379064.000000         379064.000000     379064.000000   \n",
      "mean       180.219313             52.675210       1149.583193   \n",
      "std       1164.800025            386.826812       4734.565981   \n",
      "min          0.000000              0.000000          0.000000   \n",
      "25%          0.000000              0.000000          0.000000   \n",
      "50%          0.598645              0.172616         33.067612   \n",
      "75%         38.582884              3.738042        573.098287   \n",
      "max     172094.427816          39624.967665     185547.961977   \n",
      "\n",
      "       gaodeditu_tfc_sum   aiqiyi_offer   aliyun_offer  julishipin_tfc_sum  \\\n",
      "count      379064.000000  379064.000000  379064.000000       379064.000000   \n",
      "mean            2.574059       0.937821       0.004247            4.241971   \n",
      "std            25.450839       0.241482       0.065033          196.743102   \n",
      "min             0.000000       0.000000       0.000000            0.000000   \n",
      "25%             0.000000       1.000000       0.000000            0.000000   \n",
      "50%             0.000000       1.000000       0.000000            0.000000   \n",
      "75%             0.738836       1.000000       0.000000            0.000000   \n",
      "max         11239.360658       1.000000       1.000000        41623.623341   \n",
      "\n",
      "       baidu_tfc_sum  aiqiyi_tfc_sum  shuqixiaoshuo_offer  gaodeditu_offer  \\\n",
      "count  379064.000000   379064.000000        379064.000000    379064.000000   \n",
      "mean     7473.181594     1633.109363             0.035648         0.035648   \n",
      "std     15297.719826     6538.745572             0.185412         0.185412   \n",
      "min         0.000000        0.000000             0.000000         0.000000   \n",
      "25%       121.084245        0.000000             0.000000         0.000000   \n",
      "50%       908.970131        7.693236             0.000000         0.000000   \n",
      "75%      6844.080443      295.056256             0.000000         0.000000   \n",
      "max    249639.907599   287674.053784             1.000000         1.000000   \n",
      "\n",
      "          cm_tfc_sum  youkutudou_offer     price  xinlangweibo_offer  \\\n",
      "count  379064.000000     379064.000000  379064.0       379064.000000   \n",
      "mean     8855.503296          0.702396      19.0            0.002506   \n",
      "std     10956.621153          0.457205       0.0            0.049999   \n",
      "min         0.000000          0.000000      19.0            0.000000   \n",
      "25%      1457.358442          0.000000      19.0            0.000000   \n",
      "50%      5394.254858          1.000000      19.0            0.000000   \n",
      "75%     12323.594022          1.000000      19.0            0.000000   \n",
      "max    373796.938527          1.000000      19.0            1.000000   \n",
      "\n",
      "         yixin_offer  jinritoutiao_offer   traffic_cost  max_net_speed  \\\n",
      "count  379064.000000       379064.000000  379064.000000  379064.000000   \n",
      "mean        0.906419            0.666748     801.465716     690.030747   \n",
      "std         0.291245            0.471377      17.058592    1914.928330   \n",
      "min         0.000000            0.000000     800.000000       0.000000   \n",
      "25%         1.000000            0.000000     800.000000       0.000000   \n",
      "50%         1.000000            1.000000     800.000000     215.652368   \n",
      "75%         1.000000            1.000000     800.000000     659.513086   \n",
      "max         1.000000            1.000000    1000.000000  102450.411133   \n",
      "\n",
      "       aliyun_tfc_sum  xiamiyinyue_offer  xiamiyinyue_tfc_sum  kuaishou_offer  \\\n",
      "count   379064.000000      379064.000000        379064.000000   379064.000000   \n",
      "mean        48.282114           0.035648             0.411577        0.937821   \n",
      "std        171.574094           0.185412            22.284023        0.241482   \n",
      "min          0.000000           0.000000             0.000000        0.000000   \n",
      "25%          0.042747           0.000000             0.000000        1.000000   \n",
      "50%         12.593612           0.000000             0.000000        1.000000   \n",
      "75%         48.869627           0.000000             0.000000        1.000000   \n",
      "max      27823.663847           1.000000          4845.722657        1.000000   \n",
      "\n",
      "       hlwk_offer_aim_flow_cm_round  rel_amount_round         lan_id  \\\n",
      "count                 379064.000000     379064.000000  379064.000000   \n",
      "mean                   23986.921243       3373.148088      15.467251   \n",
      "std                    33365.374660       1982.341161       4.113373   \n",
      "min                        0.000000          0.000000      10.000000   \n",
      "25%                      590.000000       1900.000000      11.000000   \n",
      "50%                     9606.500000       2700.000000      15.000000   \n",
      "75%                    35311.250000       4339.250000      19.000000   \n",
      "max                   475151.000000      58290.000000      23.000000   \n",
      "\n",
      "           innet_dur        balance  offer_grade  user_terminal_dur  \\\n",
      "count  379064.000000  379064.000000     379064.0      379064.000000   \n",
      "mean        7.191886      56.239931         19.0          86.168935   \n",
      "std         6.176156      40.141591          0.0         111.621956   \n",
      "min         0.000000      -0.360000         19.0           0.000000   \n",
      "25%         3.000000      36.670000         19.0          27.000000   \n",
      "50%         5.000000      49.900000         19.0          53.000000   \n",
      "75%         9.000000      66.400000         19.0         105.000000   \n",
      "max       219.000000   10904.000000         19.0        2474.000000   \n",
      "\n",
      "       reg_term_price  exceed_flow_l1m  surplus_flow_l1m  \\\n",
      "count   379064.000000    379064.000000     379064.000000   \n",
      "mean       597.486182       393.722591        251.730941   \n",
      "std       1414.867582       896.340394        666.901082   \n",
      "min         -1.000000         0.000000      -1024.000000   \n",
      "25%          0.000000         0.000000          0.000000   \n",
      "50%          0.000000        55.110000          0.000000   \n",
      "75%          0.000000       403.812500          0.000000   \n",
      "max      20000.000000     40489.130000      51243.300000   \n",
      "\n",
      "       total_offer_flow_l1m   use_dura_l1m  use_offer_percent   use_flow_l1m  \\\n",
      "count         379064.000000  379064.000000      379064.000000  379064.000000   \n",
      "mean             518.780898      49.968367          21.711271   33822.573106   \n",
      "std              826.255763     119.435794          40.328635   36427.059488   \n",
      "min                0.000000       0.000000           0.000000       0.000000   \n",
      "25%                0.000000       0.000000           0.000000    5582.750000   \n",
      "50%                0.000000       8.000000           0.000000   22675.500000   \n",
      "75%             1024.000000      48.000000           0.000000   50391.000000   \n",
      "max            61440.000000    6179.000000         200.000000  546416.000000   \n",
      "\n",
      "       exceed_flow_l2m  surplus_flow_l2m   use_dura_l2m   use_flow_l2m  \\\n",
      "count    379064.000000     379064.000000  379064.000000  379064.000000   \n",
      "mean        330.755840        244.632543      43.126897   27450.564406   \n",
      "std         818.877445        661.588646     111.999786   34195.951227   \n",
      "min           0.000000      -1024.000000       0.000000       0.000000   \n",
      "25%           0.000000          0.000000       0.000000    1000.000000   \n",
      "50%           0.000000          0.000000       4.000000   14852.500000   \n",
      "75%         314.422500          0.000000      37.000000   41882.250000   \n",
      "max       45884.670000      38825.310000    9133.000000  927028.000000   \n",
      "\n",
      "       l1_use_offer_percent  use_dura_avg_3m  use_flow_avg_3m  \\\n",
      "count         379064.000000    379064.000000    379064.000000   \n",
      "mean              22.124002        43.817628     27996.015747   \n",
      "std               40.582094       100.269785     29959.604379   \n",
      "min                0.000000         0.000000         0.000000   \n",
      "25%                0.000000         1.000000      5353.175000   \n",
      "50%                0.000000         8.670000     18428.925000   \n",
      "75%                0.210000        43.330000     41212.935000   \n",
      "max              200.000000      8302.670000    677684.060000   \n",
      "\n",
      "       surplus_flow_cm    use_dura_cm    use_flow_cm  games_app_days_l1m  \\\n",
      "count    379064.000000  379064.000000  379064.000000       379064.000000   \n",
      "mean        288.073885      50.793246   35118.688082            2.032913   \n",
      "std         696.369638     125.485156   37366.222434            5.960684   \n",
      "min       -1024.000000       0.000000       0.000000            0.000000   \n",
      "25%           0.000000       0.000000    5531.000000            0.000000   \n",
      "50%           0.000000       8.000000   23802.000000            0.000000   \n",
      "75%           0.000000      48.000000   53124.250000            1.000000   \n",
      "max       20480.000000    7051.000000  441644.000000           31.000000   \n",
      "\n",
      "       music_app_days_l1m  video_app_days_l1m  shopping_app_days_l1m  \\\n",
      "count       379064.000000       379064.000000          379064.000000   \n",
      "mean             1.607035            2.154457               2.307486   \n",
      "std              5.380331            6.197567               6.369393   \n",
      "min              0.000000            0.000000               0.000000   \n",
      "25%              0.000000            0.000000               0.000000   \n",
      "50%              0.000000            0.000000               0.000000   \n",
      "75%              0.000000            1.000000               1.000000   \n",
      "max             31.000000           31.000000              31.000000   \n",
      "\n",
      "             own_age  avg_over_flow  s_stop_cnt_sum_3m  d_stop_cnt_sum_3m  \\\n",
      "count  379064.000000  379064.000000      379064.000000      379064.000000   \n",
      "mean       29.935330     335.187111           0.134241           1.564543   \n",
      "std        11.794462     704.157803           0.408887           2.587786   \n",
      "min        16.000000       0.000000           0.000000           0.000000   \n",
      "25%        20.000000       0.000000           0.000000           0.000000   \n",
      "50%        27.000000      82.250000           0.000000           1.000000   \n",
      "75%        37.000000     366.930000           0.000000           2.000000   \n",
      "max        85.000000   31732.410000           6.000000         115.000000   \n",
      "\n",
      "       hlwk_offer_aim_flow_cm  l2_use_offer_percent     rel_amount  \\\n",
      "count           379064.000000         379064.000000  379064.000000   \n",
      "mean             23987.321875             22.195954      33.731481   \n",
      "std              33365.441644             40.645355      19.823412   \n",
      "min                  0.000000              0.000000       0.000000   \n",
      "25%                590.570000              0.000000      19.000000   \n",
      "50%               9607.100000              0.000000      27.000000   \n",
      "75%              35311.877500              0.100000      43.392500   \n",
      "max             475151.840000            132.230000     582.900000   \n",
      "\n",
      "           halt_2012  own_gender_cd_index  src_offer_id_index  \\\n",
      "count  379064.000000        379064.000000       379064.000000   \n",
      "mean        0.077372             0.254458            0.914959   \n",
      "std         0.267181             0.435557            2.248129   \n",
      "min         0.000000             0.000000            0.000000   \n",
      "25%         0.000000             0.000000            0.000000   \n",
      "50%         0.000000             0.000000            0.000000   \n",
      "75%         0.000000             1.000000            1.000000   \n",
      "max         1.000000             1.000000           30.000000   \n",
      "\n",
      "       second_card_slot_l1m_index  rel_amount_round_target  \\\n",
      "count               379064.000000            379064.000000   \n",
      "mean                     0.864247                 0.083914   \n",
      "std                      0.923215                 0.047557   \n",
      "min                      0.000000                 0.000000   \n",
      "25%                      0.000000                 0.042357   \n",
      "50%                      1.000000                 0.078570   \n",
      "75%                      1.000000                 0.125210   \n",
      "max                      3.000000                 1.000000   \n",
      "\n",
      "       hlwk_offer_aim_flow_cm_round_target  20201101_calling_duration  \\\n",
      "count                        379064.000000              379064.000000   \n",
      "mean                              0.081173                  45.661081   \n",
      "std                               0.058238                 236.684615   \n",
      "min                               0.000000                   0.000000   \n",
      "25%                               0.036364                   0.000000   \n",
      "50%                               0.092784                   0.000000   \n",
      "75%                               0.112742                   0.000000   \n",
      "max                               1.000000               24398.000000   \n",
      "\n",
      "       20201101_calling_record_num  20201101_called_duration  \\\n",
      "count                379064.000000             379064.000000   \n",
      "mean                      0.496486                 34.329628   \n",
      "std                       1.866613                208.772532   \n",
      "min                       0.000000                  0.000000   \n",
      "25%                       0.000000                  0.000000   \n",
      "50%                       0.000000                  0.000000   \n",
      "75%                       0.000000                  0.000000   \n",
      "max                     164.000000              22384.000000   \n",
      "\n",
      "       20201101_called_record_num  20201102_calling_duration  \\\n",
      "count               379064.000000              379064.000000   \n",
      "mean                     0.410144                  43.647669   \n",
      "std                      1.434446                 235.219304   \n",
      "min                      0.000000                   0.000000   \n",
      "25%                      0.000000                   0.000000   \n",
      "50%                      0.000000                   0.000000   \n",
      "75%                      0.000000                   0.000000   \n",
      "max                    287.000000               24296.000000   \n",
      "\n",
      "       20201102_calling_record_num  20201102_called_duration  \\\n",
      "count                379064.000000             379064.000000   \n",
      "mean                      0.466578                 31.363448   \n",
      "std                       1.900430                190.423248   \n",
      "min                       0.000000                  0.000000   \n",
      "25%                       0.000000                  0.000000   \n",
      "50%                       0.000000                  0.000000   \n",
      "75%                       0.000000                  0.000000   \n",
      "max                     148.000000              12410.000000   \n",
      "\n",
      "       20201102_called_record_num  20201103_calling_duration  \\\n",
      "count               379064.000000              379064.000000   \n",
      "mean                     0.370740                  41.982330   \n",
      "std                      1.356048                 221.052916   \n",
      "min                      0.000000                   0.000000   \n",
      "25%                      0.000000                   0.000000   \n",
      "50%                      0.000000                   0.000000   \n",
      "75%                      0.000000                   0.000000   \n",
      "max                    164.000000               10892.000000   \n",
      "\n",
      "       20201103_calling_record_num  20201103_called_duration  \\\n",
      "count                379064.000000             379064.000000   \n",
      "mean                      0.456063                 31.033810   \n",
      "std                       1.866145                192.633705   \n",
      "min                       0.000000                  0.000000   \n",
      "25%                       0.000000                  0.000000   \n",
      "50%                       0.000000                  0.000000   \n",
      "75%                       0.000000                  0.000000   \n",
      "max                     117.000000              20188.000000   \n",
      "\n",
      "       20201103_called_record_num  20201104_calling_duration  \\\n",
      "count               379064.000000              379064.000000   \n",
      "mean                     0.364693                  42.050891   \n",
      "std                      1.405931                 220.797195   \n",
      "min                      0.000000                   0.000000   \n",
      "25%                      0.000000                   0.000000   \n",
      "50%                      0.000000                   0.000000   \n",
      "75%                      0.000000                   0.000000   \n",
      "max                    267.000000               15196.000000   \n",
      "\n",
      "       20201104_calling_record_num  20201104_called_duration  \\\n",
      "count                379064.000000             379064.000000   \n",
      "mean                      0.453818                 31.026969   \n",
      "std                       1.875585                192.489352   \n",
      "min                       0.000000                  0.000000   \n",
      "25%                       0.000000                  0.000000   \n",
      "50%                       0.000000                  0.000000   \n",
      "75%                       0.000000                  0.000000   \n",
      "max                     114.000000              12995.000000   \n",
      "\n",
      "       20201104_called_record_num  20201105_calling_duration  \\\n",
      "count               379064.000000              379064.000000   \n",
      "mean                     0.359390                  41.609079   \n",
      "std                      1.332999                 217.707453   \n",
      "min                      0.000000                   0.000000   \n",
      "25%                      0.000000                   0.000000   \n",
      "50%                      0.000000                   0.000000   \n",
      "75%                      0.000000                   0.000000   \n",
      "max                    171.000000               12586.000000   \n",
      "\n",
      "       20201105_calling_record_num  20201105_called_duration  \\\n",
      "count                379064.000000             379064.000000   \n",
      "mean                      0.451726                 30.859731   \n",
      "std                       1.873436                187.904903   \n",
      "min                       0.000000                  0.000000   \n",
      "25%                       0.000000                  0.000000   \n",
      "50%                       0.000000                  0.000000   \n",
      "75%                       0.000000                  0.000000   \n",
      "max                     113.000000              11880.000000   \n",
      "\n",
      "       20201105_called_record_num  20201106_calling_duration  \\\n",
      "count               379064.000000              379064.000000   \n",
      "mean                     0.361235                  45.567126   \n",
      "std                      1.467514                 231.395653   \n",
      "min                      0.000000                   0.000000   \n",
      "25%                      0.000000                   0.000000   \n",
      "50%                      0.000000                   0.000000   \n",
      "75%                      0.000000                   0.000000   \n",
      "max                    307.000000               13191.000000   \n",
      "\n",
      "       20201106_calling_record_num  20201106_called_duration  \\\n",
      "count                379064.000000             379064.000000   \n",
      "mean                      0.496549                 33.923058   \n",
      "std                       1.969604                206.591319   \n",
      "min                       0.000000                  0.000000   \n",
      "25%                       0.000000                  0.000000   \n",
      "50%                       0.000000                  0.000000   \n",
      "75%                       0.000000                  0.000000   \n",
      "max                     138.000000              17371.000000   \n",
      "\n",
      "       20201106_called_record_num  20201107_calling_duration  \\\n",
      "count               379064.000000              379064.000000   \n",
      "mean                     0.397928                  44.156296   \n",
      "std                      1.506776                 230.522678   \n",
      "min                      0.000000                   0.000000   \n",
      "25%                      0.000000                   0.000000   \n",
      "50%                      0.000000                   0.000000   \n",
      "75%                      0.000000                   0.000000   \n",
      "max                    253.000000               13904.000000   \n",
      "\n",
      "       20201107_calling_record_num  20201107_called_duration  \\\n",
      "count                379064.000000             379064.000000   \n",
      "mean                      0.491814                 33.805737   \n",
      "std                       1.977716                197.993308   \n",
      "min                       0.000000                  0.000000   \n",
      "25%                       0.000000                  0.000000   \n",
      "50%                       0.000000                  0.000000   \n",
      "75%                       0.000000                  0.000000   \n",
      "max                     120.000000              14392.000000   \n",
      "\n",
      "       20201107_called_record_num  20201108_calling_duration  \\\n",
      "count               379064.000000              379064.000000   \n",
      "mean                     0.407417                  44.415162   \n",
      "std                      1.439756                 231.036788   \n",
      "min                      0.000000                   0.000000   \n",
      "25%                      0.000000                   0.000000   \n",
      "50%                      0.000000                   0.000000   \n",
      "75%                      0.000000                   0.000000   \n",
      "max                    114.000000               12359.000000   \n",
      "\n",
      "       20201108_calling_record_num  20201108_called_duration  \\\n",
      "count                379064.000000             379064.000000   \n",
      "mean                      0.477410                 33.242595   \n",
      "std                       1.900149                203.442501   \n",
      "min                       0.000000                  0.000000   \n",
      "25%                       0.000000                  0.000000   \n",
      "50%                       0.000000                  0.000000   \n",
      "75%                       0.000000                  0.000000   \n",
      "max                     145.000000              15490.000000   \n",
      "\n",
      "       20201108_called_record_num  20201109_calling_duration  \\\n",
      "count               379064.000000              379064.000000   \n",
      "mean                     0.395384                  41.889364   \n",
      "std                      1.366889                 227.904898   \n",
      "min                      0.000000                   0.000000   \n",
      "25%                      0.000000                   0.000000   \n",
      "50%                      0.000000                   0.000000   \n",
      "75%                      0.000000                   0.000000   \n",
      "max                     99.000000               15300.000000   \n",
      "\n",
      "       20201109_calling_record_num  20201109_called_duration  \\\n",
      "count                379064.000000             379064.000000   \n",
      "mean                      0.452135                 31.254986   \n",
      "std                       1.858023                193.575085   \n",
      "min                       0.000000                  0.000000   \n",
      "25%                       0.000000                  0.000000   \n",
      "50%                       0.000000                  0.000000   \n",
      "75%                       0.000000                  0.000000   \n",
      "max                     116.000000              18262.000000   \n",
      "\n",
      "       20201109_called_record_num  20201110_calling_duration  \\\n",
      "count               379064.000000              379064.000000   \n",
      "mean                     0.365585                  42.498565   \n",
      "std                      1.410097                 225.221780   \n",
      "min                      0.000000                   0.000000   \n",
      "25%                      0.000000                   0.000000   \n",
      "50%                      0.000000                   0.000000   \n",
      "75%                      0.000000                   0.000000   \n",
      "max                    242.000000               12034.000000   \n",
      "\n",
      "       20201110_calling_record_num  20201110_called_duration  \\\n",
      "count                379064.000000             379064.000000   \n",
      "mean                      0.450887                 31.753284   \n",
      "std                       1.861251                199.076281   \n",
      "min                       0.000000                  0.000000   \n",
      "25%                       0.000000                  0.000000   \n",
      "50%                       0.000000                  0.000000   \n",
      "75%                       0.000000                  0.000000   \n",
      "max                     102.000000              18366.000000   \n",
      "\n",
      "       20201110_called_record_num  20201111_calling_duration  \\\n",
      "count               379064.000000              379064.000000   \n",
      "mean                     0.370982                  42.394670   \n",
      "std                      1.387189                 234.117914   \n",
      "min                      0.000000                   0.000000   \n",
      "25%                      0.000000                   0.000000   \n",
      "50%                      0.000000                   0.000000   \n",
      "75%                      0.000000                   0.000000   \n",
      "max                    152.000000               31752.000000   \n",
      "\n",
      "       20201111_calling_record_num  20201111_called_duration  \\\n",
      "count                379064.000000             379064.000000   \n",
      "mean                      0.456419                 31.810992   \n",
      "std                       1.902555                197.565166   \n",
      "min                       0.000000                  0.000000   \n",
      "25%                       0.000000                  0.000000   \n",
      "50%                       0.000000                  0.000000   \n",
      "75%                       0.000000                  0.000000   \n",
      "max                     124.000000              13527.000000   \n",
      "\n",
      "       20201111_called_record_num  20201112_calling_duration  \\\n",
      "count               379064.000000              379064.000000   \n",
      "mean                     0.373866                  42.330427   \n",
      "std                      1.484726                 231.474821   \n",
      "min                      0.000000                   0.000000   \n",
      "25%                      0.000000                   0.000000   \n",
      "50%                      0.000000                   0.000000   \n",
      "75%                      0.000000                   0.000000   \n",
      "max                    283.000000               19881.000000   \n",
      "\n",
      "       20201112_calling_record_num  20201112_called_duration  \\\n",
      "count                379064.000000             379064.000000   \n",
      "mean                      0.454116                 31.189828   \n",
      "std                       1.943791                196.317229   \n",
      "min                       0.000000                  0.000000   \n",
      "25%                       0.000000                  0.000000   \n",
      "50%                       0.000000                  0.000000   \n",
      "75%                       0.000000                  0.000000   \n",
      "max                     127.000000              22605.000000   \n",
      "\n",
      "       20201112_called_record_num  20201113_calling_duration  \\\n",
      "count               379064.000000              379064.000000   \n",
      "mean                     0.366621                  43.821526   \n",
      "std                      1.507868                 226.676659   \n",
      "min                      0.000000                   0.000000   \n",
      "25%                      0.000000                   0.000000   \n",
      "50%                      0.000000                   0.000000   \n",
      "75%                      0.000000                   0.000000   \n",
      "max                    250.000000               13181.000000   \n",
      "\n",
      "       20201113_calling_record_num  20201113_called_duration  \\\n",
      "count                379064.000000             379064.000000   \n",
      "mean                      0.490313                 33.406543   \n",
      "std                       2.003196                196.247003   \n",
      "min                       0.000000                  0.000000   \n",
      "25%                       0.000000                  0.000000   \n",
      "50%                       0.000000                  0.000000   \n",
      "75%                       0.000000                  0.000000   \n",
      "max                     129.000000              19952.000000   \n",
      "\n",
      "       20201113_called_record_num  20201114_calling_duration  \\\n",
      "count               379064.000000              379064.000000   \n",
      "mean                     0.405712                  44.271202   \n",
      "std                      1.442558                 229.325047   \n",
      "min                      0.000000                   0.000000   \n",
      "25%                      0.000000                   0.000000   \n",
      "50%                      0.000000                   0.000000   \n",
      "75%                      0.000000                   0.000000   \n",
      "max                    143.000000               13844.000000   \n",
      "\n",
      "       20201114_calling_record_num  20201114_called_duration  \\\n",
      "count                379064.000000             379064.000000   \n",
      "mean                      0.491189                 33.451557   \n",
      "std                       1.969439                198.537399   \n",
      "min                       0.000000                  0.000000   \n",
      "25%                       0.000000                  0.000000   \n",
      "50%                       0.000000                  0.000000   \n",
      "75%                       0.000000                  0.000000   \n",
      "max                     122.000000              21611.000000   \n",
      "\n",
      "       20201114_called_record_num  20201115_calling_duration  \\\n",
      "count               379064.000000              379064.000000   \n",
      "mean                     0.410237                  44.461608   \n",
      "std                      1.432888                 235.508823   \n",
      "min                      0.000000                   0.000000   \n",
      "25%                      0.000000                   0.000000   \n",
      "50%                      0.000000                   0.000000   \n",
      "75%                      0.000000                   0.000000   \n",
      "max                    102.000000               27326.000000   \n",
      "\n",
      "       20201115_calling_record_num  20201115_called_duration  \\\n",
      "count                379064.000000             379064.000000   \n",
      "mean                      0.491002                 34.256571   \n",
      "std                       1.931176                208.732988   \n",
      "min                       0.000000                  0.000000   \n",
      "25%                       0.000000                  0.000000   \n",
      "50%                       0.000000                  0.000000   \n",
      "75%                       0.000000                  0.000000   \n",
      "max                     152.000000              26414.000000   \n",
      "\n",
      "       20201115_called_record_num  20201116_calling_duration  \\\n",
      "count               379064.000000              379064.000000   \n",
      "mean                     0.410796                  42.511017   \n",
      "std                      1.417526                 234.147401   \n",
      "min                      0.000000                   0.000000   \n",
      "25%                      0.000000                   0.000000   \n",
      "50%                      0.000000                   0.000000   \n",
      "75%                      0.000000                   0.000000   \n",
      "max                    113.000000               35379.000000   \n",
      "\n",
      "       20201116_calling_record_num  20201116_called_duration  \\\n",
      "count                379064.000000             379064.000000   \n",
      "mean                      0.463106                 32.153874   \n",
      "std                       1.953947                193.846226   \n",
      "min                       0.000000                  0.000000   \n",
      "25%                       0.000000                  0.000000   \n",
      "50%                       0.000000                  0.000000   \n",
      "75%                       0.000000                  0.000000   \n",
      "max                     176.000000              16369.000000   \n",
      "\n",
      "       20201116_called_record_num  20201117_calling_duration  \\\n",
      "count               379064.000000              379064.000000   \n",
      "mean                     0.379981                  41.358367   \n",
      "std                      1.403033                 222.685902   \n",
      "min                      0.000000                   0.000000   \n",
      "25%                      0.000000                   0.000000   \n",
      "50%                      0.000000                   0.000000   \n",
      "75%                      0.000000                   0.000000   \n",
      "max                     89.000000               12478.000000   \n",
      "\n",
      "       20201117_calling_record_num  20201117_called_duration  \\\n",
      "count                379064.000000             379064.000000   \n",
      "mean                      0.449718                 31.406720   \n",
      "std                       1.902115                194.148705   \n",
      "min                       0.000000                  0.000000   \n",
      "25%                       0.000000                  0.000000   \n",
      "50%                       0.000000                  0.000000   \n",
      "75%                       0.000000                  0.000000   \n",
      "max                     141.000000              14878.000000   \n",
      "\n",
      "       20201117_called_record_num  20201118_calling_duration  \\\n",
      "count               379064.000000              379064.000000   \n",
      "mean                     0.367202                  41.953931   \n",
      "std                      1.372124                 236.791536   \n",
      "min                      0.000000                   0.000000   \n",
      "25%                      0.000000                   0.000000   \n",
      "50%                      0.000000                   0.000000   \n",
      "75%                      0.000000                   0.000000   \n",
      "max                    105.000000               34819.000000   \n",
      "\n",
      "       20201118_calling_record_num  20201118_called_duration  \\\n",
      "count                379064.000000             379064.000000   \n",
      "mean                      0.449615                 31.646516   \n",
      "std                       1.886330                198.827009   \n",
      "min                       0.000000                  0.000000   \n",
      "25%                       0.000000                  0.000000   \n",
      "50%                       0.000000                  0.000000   \n",
      "75%                       0.000000                  0.000000   \n",
      "max                     136.000000              21663.000000   \n",
      "\n",
      "       20201118_called_record_num  20201119_calling_duration  \\\n",
      "count               379064.000000              379064.000000   \n",
      "mean                     0.369890                  41.948323   \n",
      "std                      1.410516                 233.145134   \n",
      "min                      0.000000                   0.000000   \n",
      "25%                      0.000000                   0.000000   \n",
      "50%                      0.000000                   0.000000   \n",
      "75%                      0.000000                   0.000000   \n",
      "max                    188.000000               34195.000000   \n",
      "\n",
      "       20201119_calling_record_num  20201119_called_duration  \\\n",
      "count                379064.000000             379064.000000   \n",
      "mean                      0.451808                 31.642641   \n",
      "std                       1.862521                198.001861   \n",
      "min                       0.000000                  0.000000   \n",
      "25%                       0.000000                  0.000000   \n",
      "50%                       0.000000                  0.000000   \n",
      "75%                       0.000000                  0.000000   \n",
      "max                     102.000000              21929.000000   \n",
      "\n",
      "       20201119_called_record_num  20201120_calling_duration  \\\n",
      "count               379064.000000              379064.000000   \n",
      "mean                     0.371510                  44.617460   \n",
      "std                      1.421898                 236.078826   \n",
      "min                      0.000000                   0.000000   \n",
      "25%                      0.000000                   0.000000   \n",
      "50%                      0.000000                   0.000000   \n",
      "75%                      0.000000                   0.000000   \n",
      "max                    138.000000               14874.000000   \n",
      "\n",
      "       20201120_calling_record_num  20201120_called_duration  \\\n",
      "count                379064.000000             379064.000000   \n",
      "mean                      0.489558                 33.610498   \n",
      "std                       1.958717                199.921660   \n",
      "min                       0.000000                  0.000000   \n",
      "25%                       0.000000                  0.000000   \n",
      "50%                       0.000000                  0.000000   \n",
      "75%                       0.000000                  0.000000   \n",
      "max                     178.000000              16191.000000   \n",
      "\n",
      "       20201120_called_record_num  20201121_calling_duration  \\\n",
      "count               379064.000000              379064.000000   \n",
      "mean                     0.400539                  43.242260   \n",
      "std                      1.466513                 233.983329   \n",
      "min                      0.000000                   0.000000   \n",
      "25%                      0.000000                   0.000000   \n",
      "50%                      0.000000                   0.000000   \n",
      "75%                      0.000000                   0.000000   \n",
      "max                    206.000000               25360.000000   \n",
      "\n",
      "       20201121_calling_record_num  20201121_called_duration  \\\n",
      "count                379064.000000             379064.000000   \n",
      "mean                      0.468578                 33.422968   \n",
      "std                       1.872852                207.801168   \n",
      "min                       0.000000                  0.000000   \n",
      "25%                       0.000000                  0.000000   \n",
      "50%                       0.000000                  0.000000   \n",
      "75%                       0.000000                  0.000000   \n",
      "max                     117.000000              21453.000000   \n",
      "\n",
      "       20201121_called_record_num  20201122_calling_duration  \\\n",
      "count               379064.000000              379064.000000   \n",
      "mean                     0.395785                  43.031707   \n",
      "std                      1.381641                 238.443366   \n",
      "min                      0.000000                   0.000000   \n",
      "25%                      0.000000                   0.000000   \n",
      "50%                      0.000000                   0.000000   \n",
      "75%                      0.000000                   0.000000   \n",
      "max                    124.000000               17180.000000   \n",
      "\n",
      "       20201122_calling_record_num  20201122_called_duration  \\\n",
      "count                379064.000000             379064.000000   \n",
      "mean                      0.461357                 32.845775   \n",
      "std                       1.844847                201.182385   \n",
      "min                       0.000000                  0.000000   \n",
      "25%                       0.000000                  0.000000   \n",
      "50%                       0.000000                  0.000000   \n",
      "75%                       0.000000                  0.000000   \n",
      "max                     174.000000              21142.000000   \n",
      "\n",
      "       20201122_called_record_num  20201123_calling_duration  \\\n",
      "count               379064.000000              379064.000000   \n",
      "mean                     0.388478                  40.666827   \n",
      "std                      1.331268                 220.544870   \n",
      "min                      0.000000                   0.000000   \n",
      "25%                      0.000000                   0.000000   \n",
      "50%                      0.000000                   0.000000   \n",
      "75%                      0.000000                   0.000000   \n",
      "max                     95.000000               15010.000000   \n",
      "\n",
      "       20201123_calling_record_num  20201123_called_duration  \\\n",
      "count                379064.000000             379064.000000   \n",
      "mean                      0.439182                 31.680621   \n",
      "std                       1.847863                202.573474   \n",
      "min                       0.000000                  0.000000   \n",
      "25%                       0.000000                  0.000000   \n",
      "50%                       0.000000                  0.000000   \n",
      "75%                       0.000000                  0.000000   \n",
      "max                     172.000000              17226.000000   \n",
      "\n",
      "       20201123_called_record_num  20201124_calling_duration  \\\n",
      "count               379064.000000              379064.000000   \n",
      "mean                     0.367576                  40.660791   \n",
      "std                      1.368132                 224.215410   \n",
      "min                      0.000000                   0.000000   \n",
      "25%                      0.000000                   0.000000   \n",
      "50%                      0.000000                   0.000000   \n",
      "75%                      0.000000                   0.000000   \n",
      "max                    128.000000               14687.000000   \n",
      "\n",
      "       20201124_calling_record_num  20201124_called_duration  \\\n",
      "count                379064.000000             379064.000000   \n",
      "mean                      0.438910                 31.815564   \n",
      "std                       1.853347                201.449664   \n",
      "min                       0.000000                  0.000000   \n",
      "25%                       0.000000                  0.000000   \n",
      "50%                       0.000000                  0.000000   \n",
      "75%                       0.000000                  0.000000   \n",
      "max                     118.000000              21245.000000   \n",
      "\n",
      "       20201124_called_record_num  20201125_calling_duration  \\\n",
      "count               379064.000000              379064.000000   \n",
      "mean                     0.366447                  38.972770   \n",
      "std                      1.348990                 214.236967   \n",
      "min                      0.000000                   0.000000   \n",
      "25%                      0.000000                   0.000000   \n",
      "50%                      0.000000                   0.000000   \n",
      "75%                      0.000000                   0.000000   \n",
      "max                     90.000000               11731.000000   \n",
      "\n",
      "       20201125_calling_record_num  20201125_called_duration  \\\n",
      "count                379064.000000             379064.000000   \n",
      "mean                      0.425738                 29.968443   \n",
      "std                       1.816132                191.542853   \n",
      "min                       0.000000                  0.000000   \n",
      "25%                       0.000000                  0.000000   \n",
      "50%                       0.000000                  0.000000   \n",
      "75%                       0.000000                  0.000000   \n",
      "max                     131.000000              15679.000000   \n",
      "\n",
      "       20201125_called_record_num  20201126_calling_duration  \\\n",
      "count               379064.000000              379064.000000   \n",
      "mean                     0.353560                  42.717214   \n",
      "std                      1.435797                 236.827082   \n",
      "min                      0.000000                   0.000000   \n",
      "25%                      0.000000                   0.000000   \n",
      "50%                      0.000000                   0.000000   \n",
      "75%                      0.000000                   0.000000   \n",
      "max                    245.000000               20924.000000   \n",
      "\n",
      "       20201126_calling_record_num  20201126_called_duration  \\\n",
      "count                379064.000000             379064.000000   \n",
      "mean                      0.448758                 32.476811   \n",
      "std                       1.938308                208.420878   \n",
      "min                       0.000000                  0.000000   \n",
      "25%                       0.000000                  0.000000   \n",
      "50%                       0.000000                  0.000000   \n",
      "75%                       0.000000                  0.000000   \n",
      "max                     137.000000              15743.000000   \n",
      "\n",
      "       20201126_called_record_num  20201127_calling_duration  \\\n",
      "count               379064.000000              379064.000000   \n",
      "mean                     0.369566                  41.758819   \n",
      "std                      1.453078                 225.047059   \n",
      "min                      0.000000                   0.000000   \n",
      "25%                      0.000000                   0.000000   \n",
      "50%                      0.000000                   0.000000   \n",
      "75%                      0.000000                   0.000000   \n",
      "max                    225.000000               17953.000000   \n",
      "\n",
      "       20201127_calling_record_num  20201127_called_duration  \\\n",
      "count                379064.000000             379064.000000   \n",
      "mean                      0.462706                 32.485775   \n",
      "std                       1.939997                200.081047   \n",
      "min                       0.000000                  0.000000   \n",
      "25%                       0.000000                  0.000000   \n",
      "50%                       0.000000                  0.000000   \n",
      "75%                       0.000000                  0.000000   \n",
      "max                     180.000000              29331.000000   \n",
      "\n",
      "       20201127_called_record_num  20201128_calling_duration  \\\n",
      "count               379064.000000              379064.000000   \n",
      "mean                     0.411107                  44.225672   \n",
      "std                      1.463507                 233.544808   \n",
      "min                      0.000000                   0.000000   \n",
      "25%                      0.000000                   0.000000   \n",
      "50%                      0.000000                   0.000000   \n",
      "75%                      0.000000                   0.000000   \n",
      "max                    268.000000               14734.000000   \n",
      "\n",
      "       20201128_calling_record_num  20201128_called_duration  \\\n",
      "count                379064.000000             379064.000000   \n",
      "mean                      0.487973                 34.832867   \n",
      "std                       1.979105                214.413696   \n",
      "min                       0.000000                  0.000000   \n",
      "25%                       0.000000                  0.000000   \n",
      "50%                       0.000000                  0.000000   \n",
      "75%                       0.000000                  0.000000   \n",
      "max                     173.000000              20610.000000   \n",
      "\n",
      "       20201128_called_record_num  20201129_calling_duration  \\\n",
      "count               379064.000000              379064.000000   \n",
      "mean                     0.448470                  43.264625   \n",
      "std                      1.507515                 231.598883   \n",
      "min                      0.000000                   0.000000   \n",
      "25%                      0.000000                   0.000000   \n",
      "50%                      0.000000                   0.000000   \n",
      "75%                      0.000000                   0.000000   \n",
      "max                    287.000000               15314.000000   \n",
      "\n",
      "       20201129_calling_record_num  20201129_called_duration  \\\n",
      "count                379064.000000             379064.000000   \n",
      "mean                      0.473453                 32.950689   \n",
      "std                       1.930019                203.031399   \n",
      "min                       0.000000                  0.000000   \n",
      "25%                       0.000000                  0.000000   \n",
      "50%                       0.000000                  0.000000   \n",
      "75%                       0.000000                  0.000000   \n",
      "max                     150.000000              31287.000000   \n",
      "\n",
      "       20201129_called_record_num  20201130_calling_duration  \\\n",
      "count               379064.000000              379064.000000   \n",
      "mean                     0.401742                  43.043887   \n",
      "std                      1.446352                 229.535813   \n",
      "min                      0.000000                   0.000000   \n",
      "25%                      0.000000                   0.000000   \n",
      "50%                      0.000000                   0.000000   \n",
      "75%                      0.000000                   0.000000   \n",
      "max                    296.000000               14978.000000   \n",
      "\n",
      "       20201130_calling_record_num  20201130_called_duration  \\\n",
      "count                379064.000000             379064.000000   \n",
      "mean                      0.455240                 32.987875   \n",
      "std                       1.918355                237.930939   \n",
      "min                       0.000000                  0.000000   \n",
      "25%                       0.000000                  0.000000   \n",
      "50%                       0.000000                  0.000000   \n",
      "75%                       0.000000                  0.000000   \n",
      "max                     263.000000              60756.000000   \n",
      "\n",
      "       20201130_called_record_num  20201101_byte_in  20201101_record_num  \\\n",
      "count               379064.000000      3.790640e+05        379064.000000   \n",
      "mean                     0.384975      1.275942e+09            99.646698   \n",
      "std                      1.398610      1.844028e+09           105.309165   \n",
      "min                      0.000000      0.000000e+00             0.000000   \n",
      "25%                      0.000000      1.889445e+06             7.000000   \n",
      "50%                      0.000000      5.353035e+08            75.000000   \n",
      "75%                      0.000000      1.877083e+09           156.000000   \n",
      "max                    117.000000      5.793151e+10          3516.000000   \n",
      "\n",
      "       20201101_byte_out  20201101_duration  20201102_byte_in  \\\n",
      "count       3.790640e+05      379064.000000      3.790640e+05   \n",
      "mean        8.562776e+07      127003.693672      1.125350e+09   \n",
      "std         1.863706e+08      118380.827149      1.683806e+09   \n",
      "min         0.000000e+00           0.000000      0.000000e+00   \n",
      "25%         5.662362e+05        5513.000000      1.374978e+05   \n",
      "50%         3.511179e+07      108044.000000      4.423188e+08   \n",
      "75%         1.043346e+08      209825.000000      1.634716e+09   \n",
      "max         1.385456e+10      915335.000000      5.154370e+10   \n",
      "\n",
      "       20201102_record_num  20201102_byte_out  20201102_duration  \\\n",
      "count        379064.000000       3.790640e+05      379064.000000   \n",
      "mean             91.326739       7.394054e+07      120639.991701   \n",
      "std              97.878854       1.640095e+08      115358.457395   \n",
      "min               0.000000       0.000000e+00           0.000000   \n",
      "25%               2.000000       6.930650e+04         796.750000   \n",
      "50%              69.000000       2.921446e+07      100941.500000   \n",
      "75%             144.000000       8.877843e+07      201038.750000   \n",
      "max            3314.000000       1.641083e+10      854478.000000   \n",
      "\n",
      "       20201103_byte_in  20201103_record_num  20201103_byte_out  \\\n",
      "count      3.790640e+05        379064.000000       3.790640e+05   \n",
      "mean       1.113965e+09            89.111211       7.149634e+07   \n",
      "std        1.671303e+09            97.393747       1.640838e+08   \n",
      "min        0.000000e+00             0.000000       0.000000e+00   \n",
      "25%        0.000000e+00             0.000000       0.000000e+00   \n",
      "50%        4.287421e+08            66.000000       2.717728e+07   \n",
      "75%        1.630488e+09           142.000000       8.570009e+07   \n",
      "max        5.596984e+10          3134.000000       1.974783e+10   \n",
      "\n",
      "       20201103_duration  20201104_byte_in  20201104_record_num  \\\n",
      "count       379064.00000      3.790640e+05        379064.000000   \n",
      "mean        116368.30732      1.102018e+09            87.520675   \n",
      "std         114088.58952      1.683681e+09            96.578251   \n",
      "min              0.00000      0.000000e+00             0.000000   \n",
      "25%              0.00000      0.000000e+00             0.000000   \n",
      "50%          95047.00000      4.090123e+08            65.000000   \n",
      "75%         195704.25000      1.608846e+09           140.000000   \n",
      "max         817823.00000      9.691715e+10          3709.000000   \n",
      "\n",
      "       20201104_byte_out  20201104_duration  20201105_byte_in  \\\n",
      "count       3.790640e+05      379064.000000      3.790640e+05   \n",
      "mean        7.147705e+07      115541.856942      1.109382e+09   \n",
      "std         1.587379e+08      114370.879166      1.669880e+09   \n",
      "min         0.000000e+00           0.000000      0.000000e+00   \n",
      "25%         0.000000e+00           0.000000      0.000000e+00   \n",
      "50%         2.692811e+07       93724.000000      4.249645e+08   \n",
      "75%         8.599234e+07      194991.000000      1.626918e+09   \n",
      "max         8.236326e+09      811809.000000      1.015150e+11   \n",
      "\n",
      "       20201105_record_num  20201105_byte_out  20201105_duration  \\\n",
      "count        379064.000000       3.790640e+05      379064.000000   \n",
      "mean             88.651774       7.237470e+07      117191.571194   \n",
      "std              96.845412       1.646944e+08      114952.917543   \n",
      "min               0.000000       0.000000e+00           0.000000   \n",
      "25%               0.000000       0.000000e+00           0.000000   \n",
      "50%              66.000000       2.795816e+07       96422.000000   \n",
      "75%             141.000000       8.697938e+07      197459.500000   \n",
      "max            3358.000000       2.571769e+10      791367.000000   \n",
      "\n",
      "       20201106_byte_in  20201106_record_num  20201106_byte_out  \\\n",
      "count      3.790640e+05        379064.000000       3.790640e+05   \n",
      "mean       1.112885e+09            89.160667       7.298401e+07   \n",
      "std        1.668174e+09            97.693146       1.649785e+08   \n",
      "min        0.000000e+00             0.000000       0.000000e+00   \n",
      "25%        0.000000e+00             0.000000       0.000000e+00   \n",
      "50%        4.344523e+08            67.000000       2.850352e+07   \n",
      "75%        1.628504e+09           141.000000       8.774721e+07   \n",
      "max        5.713721e+10          4592.000000       1.686029e+10   \n",
      "\n",
      "       20201106_duration  20201107_byte_in  20201107_record_num  \\\n",
      "count      379064.000000      3.790640e+05        379064.000000   \n",
      "mean       116827.057436      1.166672e+09            90.525977   \n",
      "std        114373.373087      1.775948e+09           100.896078   \n",
      "min             0.000000      0.000000e+00             0.000000   \n",
      "25%             0.000000      0.000000e+00             0.000000   \n",
      "50%         95727.000000      4.195723e+08            65.000000   \n",
      "75%        196108.500000      1.696435e+09           144.000000   \n",
      "max        843909.000000      4.744062e+10          3878.000000   \n",
      "\n",
      "       20201107_byte_out  20201107_duration  20201108_byte_in  \\\n",
      "count       3.790640e+05      379064.000000      3.790640e+05   \n",
      "mean        7.660759e+07      116586.498557      1.192534e+09   \n",
      "std         1.735723e+08      115315.656845      1.799539e+09   \n",
      "min         0.000000e+00           0.000000      0.000000e+00   \n",
      "25%         0.000000e+00           0.000000      0.000000e+00   \n",
      "50%         2.839961e+07       94527.000000      4.414329e+08   \n",
      "75%         9.241704e+07      197456.250000      1.751159e+09   \n",
      "max         2.435544e+10      796453.000000      6.110486e+10   \n",
      "\n",
      "       20201108_record_num  20201108_byte_out  20201108_duration  \\\n",
      "count        379064.000000       3.790640e+05      379064.000000   \n",
      "mean             91.930938       7.908690e+07      118349.237786   \n",
      "std             101.922570       1.833068e+08      115697.861952   \n",
      "min               0.000000       0.000000e+00           0.000000   \n",
      "25%               0.000000       0.000000e+00           0.000000   \n",
      "50%              67.000000       2.969438e+07       96816.000000   \n",
      "75%             146.000000       9.578254e+07      199604.000000   \n",
      "max            3977.000000       1.675144e+10      881069.000000   \n",
      "\n",
      "       20201109_byte_in  20201109_record_num  20201109_byte_out  \\\n",
      "count      3.790640e+05        379064.000000       3.790640e+05   \n",
      "mean       1.096816e+09            89.498655       7.202158e+07   \n",
      "std        1.675581e+09            98.491723       1.665129e+08   \n",
      "min        0.000000e+00             0.000000       0.000000e+00   \n",
      "25%        0.000000e+00             0.000000       0.000000e+00   \n",
      "50%        4.027207e+08            66.000000       2.707175e+07   \n",
      "75%        1.597608e+09           143.000000       8.604525e+07   \n",
      "max        5.836597e+10          3514.000000       1.058590e+10   \n",
      "\n",
      "       20201109_duration  20201110_byte_in  20201110_record_num  \\\n",
      "count      379064.000000      3.790640e+05        379064.000000   \n",
      "mean       118241.258022      1.100378e+09            90.552078   \n",
      "std        116664.510187      1.651999e+09            99.243479   \n",
      "min             0.000000      0.000000e+00             0.000000   \n",
      "25%             0.000000      0.000000e+00             0.000000   \n",
      "50%         96430.000000      4.211871e+08            68.000000   \n",
      "75%        199910.250000      1.614563e+09           145.000000   \n",
      "max        785184.000000      5.795751e+10          4754.000000   \n",
      "\n",
      "       20201110_byte_out  20201110_duration  20201111_byte_in  \\\n",
      "count       3.790640e+05      379064.000000      3.790640e+05   \n",
      "mean        7.218024e+07      119478.137201      1.111182e+09   \n",
      "std         1.750789e+08      117217.717123      1.667313e+09   \n",
      "min         0.000000e+00           0.000000      0.000000e+00   \n",
      "25%         0.000000e+00           0.000000      0.000000e+00   \n",
      "50%         2.790924e+07       98165.000000      4.292758e+08   \n",
      "75%         8.655321e+07      201841.000000      1.627173e+09   \n",
      "max         3.407541e+10      805939.000000      4.540837e+10   \n",
      "\n",
      "       20201111_record_num  20201111_byte_out  20201111_duration  \\\n",
      "count        379064.000000       3.790640e+05      379064.000000   \n",
      "mean             92.066585       7.364225e+07      121566.768292   \n",
      "std              99.771514       1.648449e+08      118676.659131   \n",
      "min               0.000000       0.000000e+00           0.000000   \n",
      "25%               0.000000       0.000000e+00           0.000000   \n",
      "50%              69.000000       2.876075e+07      100341.000000   \n",
      "75%             146.000000       8.891627e+07      204506.000000   \n",
      "max            3830.000000       1.266319e+10      819050.000000   \n",
      "\n",
      "       20201112_byte_in  20201112_record_num  20201112_byte_out  \\\n",
      "count      3.790640e+05        379064.000000       3.790640e+05   \n",
      "mean       1.029909e+09            82.519527       6.684532e+07   \n",
      "std        1.553318e+09            89.629929       1.543335e+08   \n",
      "min        0.000000e+00             0.000000       0.000000e+00   \n",
      "25%        0.000000e+00             0.000000       0.000000e+00   \n",
      "50%        3.890753e+08            62.000000       2.557257e+07   \n",
      "75%        1.506280e+09           132.000000       7.915166e+07   \n",
      "max        7.110356e+10          3198.000000       1.011598e+10   \n",
      "\n",
      "       20201112_duration  20201113_byte_in  20201113_record_num  \\\n",
      "count       3.790640e+05      3.790640e+05        379064.000000   \n",
      "mean        1.100825e+05      1.124957e+09            89.482958   \n",
      "std         1.081489e+05      1.672275e+09            97.369454   \n",
      "min         0.000000e+00      0.000000e+00             0.000000   \n",
      "25%         0.000000e+00      9.796000e+03             1.000000   \n",
      "50%         9.018300e+04      4.548788e+08            68.000000   \n",
      "75%         1.849692e+05      1.650521e+09           141.000000   \n",
      "max         1.323339e+06      5.382664e+10          7121.000000   \n",
      "\n",
      "       20201113_byte_out  20201113_duration  20201114_byte_in  \\\n",
      "count       3.790640e+05      379064.000000      3.790640e+05   \n",
      "mean        7.394495e+07      118393.592779      1.213974e+09   \n",
      "std         1.662998e+08      114209.670256      1.809392e+09   \n",
      "min         0.000000e+00           0.000000      0.000000e+00   \n",
      "25%         0.000000e+00           6.000000      3.838725e+04   \n",
      "50%         2.936989e+07       98731.500000      4.683560e+08   \n",
      "75%         8.823383e+07      197349.250000      1.771703e+09   \n",
      "max         1.021235e+10      828703.000000      5.636494e+10   \n",
      "\n",
      "       20201114_record_num  20201114_byte_out  20201114_duration  \\\n",
      "count        379064.000000       3.790640e+05      379064.000000   \n",
      "mean             92.832432       7.987634e+07      120116.270382   \n",
      "std             100.860562       1.801248e+08      115176.423719   \n",
      "min               0.000000       0.000000e+00           0.000000   \n",
      "25%               1.000000       2.128425e+04         139.000000   \n",
      "50%              69.000000       3.076563e+07      100059.500000   \n",
      "75%             147.000000       9.547410e+07      201710.750000   \n",
      "max            4493.000000       1.415499e+10      865597.000000   \n",
      "\n",
      "       20201115_byte_in  20201115_record_num  20201115_byte_out  \\\n",
      "count      3.790640e+05        379064.000000       3.790640e+05   \n",
      "mean       1.234026e+09            93.089737       8.181578e+07   \n",
      "std        1.822340e+09           100.767326       1.848404e+08   \n",
      "min        0.000000e+00             0.000000       0.000000e+00   \n",
      "25%        1.158970e+05             2.000000       5.758150e+04   \n",
      "50%        4.794131e+08            69.000000       3.174659e+07   \n",
      "75%        1.821330e+09           147.000000       9.836106e+07   \n",
      "max        5.819467e+10          2495.000000       2.031967e+10   \n",
      "\n",
      "       20201115_duration  20201116_byte_in  20201116_record_num  \\\n",
      "count      379064.000000      3.790640e+05        379064.000000   \n",
      "mean       119944.675458      1.133555e+09            89.750113   \n",
      "std        114771.875311      1.710690e+09            97.794831   \n",
      "min             0.000000      0.000000e+00             0.000000   \n",
      "25%           629.750000      0.000000e+00             0.000000   \n",
      "50%         99873.000000      4.276461e+08            67.000000   \n",
      "75%        200854.250000      1.651971e+09           142.000000   \n",
      "max        808543.000000      4.190508e+10          3502.000000   \n",
      "\n",
      "       20201116_byte_out  20201116_duration  20201117_byte_in  \\\n",
      "count       3.790640e+05      379064.000000      3.790640e+05   \n",
      "mean        7.404654e+07      118862.152607      1.162142e+09   \n",
      "std         1.720356e+08      115343.389715      1.738337e+09   \n",
      "min         0.000000e+00           0.000000      0.000000e+00   \n",
      "25%         0.000000e+00           0.000000      0.000000e+00   \n",
      "50%         2.833524e+07       98624.500000      4.524094e+08   \n",
      "75%         8.837682e+07      199567.000000      1.704698e+09   \n",
      "max         2.291112e+10      828565.000000      4.634440e+10   \n",
      "\n",
      "       20201117_record_num  20201117_byte_out  20201117_duration  \\\n",
      "count        379064.000000       3.790640e+05      379064.000000   \n",
      "mean             91.175894       7.524061e+07      119463.169156   \n",
      "std              99.377923       1.779828e+08      116104.675566   \n",
      "min               0.000000       0.000000e+00           0.000000   \n",
      "25%               0.000000       0.000000e+00           0.000000   \n",
      "50%              68.000000       2.919528e+07       98477.500000   \n",
      "75%             144.000000       8.995154e+07      201226.500000   \n",
      "max            2300.000000       3.155727e+10      904825.000000   \n",
      "\n",
      "       20201118_byte_in  20201118_record_num  20201118_byte_out  \\\n",
      "count      3.790640e+05        379064.000000       3.790640e+05   \n",
      "mean       1.191474e+09            90.940451       7.479567e+07   \n",
      "std        1.784484e+09           100.782299       1.704919e+08   \n",
      "min        0.000000e+00             0.000000       0.000000e+00   \n",
      "25%        0.000000e+00             0.000000       0.000000e+00   \n",
      "50%        4.551823e+08            67.000000       2.903695e+07   \n",
      "75%        1.739297e+09           144.000000       8.997185e+07   \n",
      "max        5.597898e+10          7241.000000       1.985450e+10   \n",
      "\n",
      "       20201118_duration  20201119_byte_in  20201119_record_num  \\\n",
      "count      379064.000000      3.790640e+05        379064.000000   \n",
      "mean       117474.616595      1.112622e+09            87.134207   \n",
      "std        114928.511856      1.685993e+09            95.890085   \n",
      "min             0.000000      0.000000e+00             0.000000   \n",
      "25%             0.000000      0.000000e+00             0.000000   \n",
      "50%         96237.000000      4.253271e+08            64.000000   \n",
      "75%        197178.000000      1.613574e+09           138.000000   \n",
      "max        956956.000000      4.681317e+10          3098.000000   \n",
      "\n",
      "       20201119_byte_out  20201119_duration  20201120_byte_in  \\\n",
      "count       3.790640e+05      379064.000000      3.790640e+05   \n",
      "mean        7.179382e+07      114782.461935      1.186522e+09   \n",
      "std         1.688348e+08      112702.831504      1.714515e+09   \n",
      "min         0.000000e+00           0.000000      0.000000e+00   \n",
      "25%         0.000000e+00           0.000000      1.084005e+05   \n",
      "50%         2.755225e+07       93389.000000      5.035097e+08   \n",
      "75%         8.564273e+07      192337.500000      1.758751e+09   \n",
      "max         2.441962e+10      780368.000000      4.573329e+10   \n",
      "\n",
      "       20201120_record_num  20201120_byte_out  20201120_duration  \\\n",
      "count        379064.000000       3.790640e+05      379064.000000   \n",
      "mean             93.916705       7.428221e+07      122110.440158   \n",
      "std              99.886664       1.468489e+08      116763.663923   \n",
      "min               0.000000       0.000000e+00           0.000000   \n",
      "25%               2.000000       5.316400e+04         553.000000   \n",
      "50%              71.000000       3.196249e+07      102571.000000   \n",
      "75%             149.000000       9.396957e+07      203338.500000   \n",
      "max            3159.000000       1.005224e+10      826600.000000   \n",
      "\n",
      "       20201121_byte_in  20201121_record_num  20201121_byte_out  \\\n",
      "count      3.790640e+05        379064.000000       3.790640e+05   \n",
      "mean       1.234603e+09            93.657580       7.653788e+07   \n",
      "std        1.828482e+09           102.039362       1.583293e+08   \n",
      "min        0.000000e+00             0.000000       0.000000e+00   \n",
      "25%        7.444025e+04             2.000000       3.697350e+04   \n",
      "50%        4.763191e+08            68.000000       3.102852e+07   \n",
      "75%        1.818775e+09           149.000000       9.709016e+07   \n",
      "max        6.838243e+10          3439.000000       2.611311e+10   \n",
      "\n",
      "       20201121_duration  20201122_byte_in  20201122_record_num  \\\n",
      "count      379064.000000      3.790640e+05        379064.000000   \n",
      "mean       119677.966660      1.208398e+09            91.346970   \n",
      "std        116038.580491      1.786699e+09            99.906949   \n",
      "min             0.000000      0.000000e+00             0.000000   \n",
      "25%           348.000000      8.663000e+04             2.000000   \n",
      "50%         98287.000000      4.671580e+08            66.000000   \n",
      "75%        200747.250000      1.794389e+09           146.000000   \n",
      "max        803974.000000      8.693253e+10          4010.000000   \n",
      "\n",
      "       20201122_byte_out  20201122_duration  20201123_byte_in  \\\n",
      "count       3.790640e+05      379064.000000      3.790640e+05   \n",
      "mean        7.540174e+07      117147.933478      1.158708e+09   \n",
      "std         1.722576e+08      113029.051643      1.760811e+09   \n",
      "min         0.000000e+00           0.000000      0.000000e+00   \n",
      "25%         4.309225e+04         461.000000      0.000000e+00   \n",
      "50%         3.049377e+07       96481.500000      4.258277e+08   \n",
      "75%         9.588317e+07      196523.250000      1.687284e+09   \n",
      "max         5.270704e+10      883864.000000      7.008911e+10   \n",
      "\n",
      "       20201123_record_num  20201123_byte_out  20201123_duration  \\\n",
      "count        379064.000000       3.790640e+05      379064.000000   \n",
      "mean             89.962866       7.749423e+07      117874.622671   \n",
      "std              99.947212       1.897380e+08      115286.290770   \n",
      "min               0.000000       0.000000e+00           0.000000   \n",
      "25%               0.000000       0.000000e+00           0.000000   \n",
      "50%              65.000000       2.825593e+07       96343.000000   \n",
      "75%             142.000000       9.089914e+07      198655.000000   \n",
      "max            3554.000000       3.889178e+10      838316.000000   \n",
      "\n",
      "       20201124_byte_in  20201124_record_num  20201124_byte_out  \\\n",
      "count      3.790640e+05        379064.000000       3.790640e+05   \n",
      "mean       1.263745e+09            93.092554       9.468328e+07   \n",
      "std        2.056769e+09           107.761075       2.910776e+08   \n",
      "min        0.000000e+00             0.000000       0.000000e+00   \n",
      "25%        0.000000e+00             0.000000       0.000000e+00   \n",
      "50%        4.465921e+08            66.000000       2.902876e+07   \n",
      "75%        1.760640e+09           145.000000       9.534377e+07   \n",
      "max        8.589732e+10          7346.000000       2.532553e+10   \n",
      "\n",
      "       20201124_duration  20201125_byte_in  20201125_record_num  \\\n",
      "count       3.790640e+05      3.790640e+05        379064.000000   \n",
      "mean        1.206558e+05      1.078924e+09            84.569133   \n",
      "std         1.207078e+05      1.657965e+09            94.467880   \n",
      "min         0.000000e+00      0.000000e+00             0.000000   \n",
      "25%         0.000000e+00      0.000000e+00             0.000000   \n",
      "50%         9.714100e+04      3.855009e+08            61.000000   \n",
      "75%         2.017140e+05      1.558782e+09           134.000000   \n",
      "max         9.504438e+06      4.935291e+10          2895.000000   \n",
      "\n",
      "       20201125_byte_out  20201125_duration  20201126_byte_in  \\\n",
      "count       3.790640e+05      379064.000000      3.790640e+05   \n",
      "mean        7.089037e+07      112311.905723      1.247723e+09   \n",
      "std         1.779252e+08      110789.454242      1.862011e+09   \n",
      "min         0.000000e+00           0.000000      0.000000e+00   \n",
      "25%         0.000000e+00           0.000000      0.000000e+00   \n",
      "50%         2.569742e+07       91079.000000      4.763722e+08   \n",
      "75%         8.306606e+07      189464.000000      1.836023e+09   \n",
      "max         3.285804e+10      844555.000000      7.506156e+10   \n",
      "\n",
      "       20201126_record_num  20201126_byte_out  20201126_duration  \\\n",
      "count        379064.000000       3.790640e+05      379064.000000   \n",
      "mean             96.278562       8.240205e+07      123534.112939   \n",
      "std             105.967832       1.912569e+08      120553.488845   \n",
      "min               0.000000       0.000000e+00           0.000000   \n",
      "25%               0.000000       0.000000e+00           0.000000   \n",
      "50%              70.000000       3.054682e+07      100986.500000   \n",
      "75%             153.000000       9.740672e+07      208346.000000   \n",
      "max            4167.000000       2.689433e+10      853403.000000   \n",
      "\n",
      "       20201127_byte_in  20201127_record_num  20201127_byte_out  \\\n",
      "count      3.790640e+05        379064.000000       3.790640e+05   \n",
      "mean       1.176717e+09            91.943134       7.831819e+07   \n",
      "std        1.756729e+09           100.476638       1.801712e+08   \n",
      "min        0.000000e+00             0.000000       0.000000e+00   \n",
      "25%        4.872375e+04             1.000000       2.294625e+04   \n",
      "50%        4.609534e+08            68.000000       2.993854e+07   \n",
      "75%        1.717240e+09           145.000000       9.237365e+07   \n",
      "max        6.401328e+10          2746.000000       2.393414e+10   \n",
      "\n",
      "       20201127_duration  20201128_byte_in  20201128_record_num  \\\n",
      "count      379064.000000      3.790640e+05        379064.000000   \n",
      "mean       119215.532731      1.222879e+09            93.418080   \n",
      "std        114956.168059      1.842958e+09           103.608153   \n",
      "min             0.000000      0.000000e+00             0.000000   \n",
      "25%           188.000000      5.574850e+04             2.000000   \n",
      "50%         98861.000000      4.482373e+08            67.000000   \n",
      "75%        199367.000000      1.778623e+09           147.000000   \n",
      "max        835192.000000      5.023848e+10          3355.000000   \n",
      "\n",
      "       20201128_byte_out  20201128_duration  20201129_byte_in  \\\n",
      "count       3.790640e+05       3.790640e+05      3.790640e+05   \n",
      "mean        8.211244e+07       1.191368e+05      1.214022e+09   \n",
      "std         1.838538e+08       1.156933e+05      1.831997e+09   \n",
      "min         0.000000e+00       0.000000e+00      0.000000e+00   \n",
      "25%         2.876625e+04       2.410000e+02      8.542225e+04   \n",
      "50%         2.985755e+07       9.753250e+04      4.470817e+08   \n",
      "75%         9.666824e+07       2.002735e+05      1.772761e+09   \n",
      "max         1.037155e+10       3.505344e+06      5.559734e+10   \n",
      "\n",
      "       20201129_record_num  20201129_byte_out  20201129_duration  \\\n",
      "count        379064.000000       3.790640e+05      379064.000000   \n",
      "mean             92.638322       8.240934e+07      119126.967705   \n",
      "std             102.430089       1.914356e+08      114989.589167   \n",
      "min               0.000000       0.000000e+00           0.000000   \n",
      "25%               2.000000       4.372150e+04         501.000000   \n",
      "50%              66.000000       3.031769e+07       97838.000000   \n",
      "75%             146.000000       9.792318e+07      199763.000000   \n",
      "max            3857.000000       2.224979e+10      812263.000000   \n",
      "\n",
      "       20201130_byte_in  20201130_record_num  20201130_byte_out  \\\n",
      "count      3.790640e+05        379064.000000       3.790640e+05   \n",
      "mean       1.133081e+09            90.404298       7.729575e+07   \n",
      "std        1.729431e+09            99.701826       1.793590e+08   \n",
      "min        0.000000e+00             0.000000       0.000000e+00   \n",
      "25%        0.000000e+00             0.000000       0.000000e+00   \n",
      "50%        4.116103e+08            66.000000       2.806203e+07   \n",
      "75%        1.640356e+09           143.000000       9.040509e+07   \n",
      "max        7.127202e+10          2739.000000       1.300253e+10   \n",
      "\n",
      "       20201130_duration  all_app_use_times  202011_byte_in_list_entropy  \\\n",
      "count      379064.000000       3.790640e+05                379064.000000   \n",
      "mean       119151.175216       1.071765e+04                     1.740797   \n",
      "std        116332.888232       4.711061e+04                     0.827266   \n",
      "min             0.000000       0.000000e+00                     0.000000   \n",
      "25%             0.000000       0.000000e+00                     1.177219   \n",
      "50%         97216.000000       0.000000e+00                     2.004936   \n",
      "75%        200363.250000       9.832500e+02                     2.413758   \n",
      "max        822618.000000       3.528356e+06                     2.907752   \n",
      "\n",
      "       202011_byte_out_list_entropy  202011_duration_list_entropy  \\\n",
      "count                 379064.000000                 379064.000000   \n",
      "mean                       1.649505                      2.024525   \n",
      "std                        0.802847                      0.823619   \n",
      "min                        0.000000                      0.000000   \n",
      "25%                        1.105587                      1.792785   \n",
      "50%                        1.846439                      2.354610   \n",
      "75%                        2.304962                      2.597417   \n",
      "max                        2.919903                      2.923231   \n",
      "\n",
      "       202011_record_num_list_entropy  202011_byte_in_list_anomaly  \\\n",
      "count                   379064.000000                379064.000000   \n",
      "mean                         2.018843                     1.893667   \n",
      "std                          0.827310                     1.996798   \n",
      "min                          0.000000                     0.000000   \n",
      "25%                          1.791374                     0.000000   \n",
      "50%                          2.342367                     1.000000   \n",
      "75%                          2.586569                     3.000000   \n",
      "max                          3.644414                    11.000000   \n",
      "\n",
      "       202011_byte_out_list_anomaly  202011_duration_list_anomaly  \\\n",
      "count                 379064.000000                 379064.000000   \n",
      "mean                       2.072492                      1.397149   \n",
      "std                        1.919736                      1.962258   \n",
      "min                        0.000000                      0.000000   \n",
      "25%                        0.000000                      0.000000   \n",
      "50%                        2.000000                      0.000000   \n",
      "75%                        3.000000                      2.000000   \n",
      "max                       13.000000                     14.000000   \n",
      "\n",
      "       202011_record_num_list_anomaly  \n",
      "count                   379064.000000  \n",
      "mean                         1.416054  \n",
      "std                          1.864860  \n",
      "min                          0.000000  \n",
      "25%                          0.000000  \n",
      "50%                          1.000000  \n",
      "75%                          2.000000  \n",
      "max                         14.000000  \n",
      "**********************************\n",
      "   Unnamed: 0  prod_inst_id  jinritoutiao_tfc_sum  \\\n",
      "0           0    33735411.0              0.000000   \n",
      "1           1    33796848.0            360.622108   \n",
      "2           2    34412178.0              0.000000   \n",
      "3           3    34632667.0              0.173120   \n",
      "4           4    34737443.0              0.000000   \n",
      "5           5    34784425.0              0.000000   \n",
      "6           6    56275671.0              0.000000   \n",
      "7           7    57558916.0           1766.082967   \n",
      "8           8    59316003.0            305.448681   \n",
      "9           9    59954656.0             88.706315   \n",
      "\n",
      "   monthly_tfc_remote_record_num  shuqixiaoshuo_tfc_sum  julishipin_offer  \\\n",
      "0                            0.0                    0.0               1.0   \n",
      "1                            0.0                    0.0               1.0   \n",
      "2                            0.0                    0.0               1.0   \n",
      "3                            0.0                    0.0               1.0   \n",
      "4                            5.0                    0.0               1.0   \n",
      "5                            0.0                    0.0               1.0   \n",
      "6                            0.0                    0.0               1.0   \n",
      "7                            0.0                    0.0               1.0   \n",
      "8                            0.0                    0.0               1.0   \n",
      "9                            0.0                    0.0               1.0   \n",
      "\n",
      "   yixin_tfc_sum  in_offer_tfc_sum  card_weight  alibaba_offer  \\\n",
      "0            0.0       1658.251314          0.0            1.0   \n",
      "1            0.0         12.510484          0.0            1.0   \n",
      "2            0.0         78.552678          0.0            1.0   \n",
      "3            0.0          4.323922         10.0            1.0   \n",
      "4            0.0          0.000000          0.0            1.0   \n",
      "5            0.0          0.000000         10.0            1.0   \n",
      "6            0.0          0.000000          0.0            1.0   \n",
      "7            0.0          9.054453         10.0            1.0   \n",
      "8            0.0       5199.206011          0.0            1.0   \n",
      "9            0.0        363.954306          0.0            1.0   \n",
      "\n",
      "   out_offer_tfc_sum  liuliangkong_tfc_sum  shoutaotianmao_tfc_sum  \\\n",
      "0          36.550119              0.000000               36.550119   \n",
      "1         363.292318              0.000000                0.000000   \n",
      "2           5.737586              0.000000                5.737586   \n",
      "3           0.173120              0.000000                0.000000   \n",
      "4           0.000000              0.000000                0.000000   \n",
      "5           0.000000              0.000000                0.000000   \n",
      "6           0.000000              0.000000                0.000000   \n",
      "7        1767.180210              0.000000                0.000000   \n",
      "8         316.056033              0.000000                0.000000   \n",
      "9          97.760597            245.532939                0.000000   \n",
      "\n",
      "   tianyishixun_tfc_sum  shoutaotianmao_offer  wangyi_offer  baidu_offer  \\\n",
      "0                   0.0                   0.0           0.0          0.0   \n",
      "1                   0.0                   0.0           1.0          1.0   \n",
      "2                   0.0                   0.0           0.0          0.0   \n",
      "3                   0.0                   0.0           1.0          1.0   \n",
      "4                   0.0                   0.0           1.0          1.0   \n",
      "5                   0.0                   0.0           1.0          1.0   \n",
      "6                   0.0                   0.0           1.0          1.0   \n",
      "7                   0.0                   0.0           1.0          1.0   \n",
      "8                   0.0                   0.0           1.0          1.0   \n",
      "9                   0.0                   0.0           1.0          1.0   \n",
      "\n",
      "   liuliangkong_offer  avg_net_speed  tianyishixun_offer  traffic  \\\n",
      "0                 1.0      21.091193                 1.0      1.0   \n",
      "1                 1.0       4.550475                 1.0      0.0   \n",
      "2                 1.0       9.299007                 1.0      1.0   \n",
      "3                 1.0       7.177593                 1.0      0.0   \n",
      "4                 1.0       0.000000                 1.0      0.0   \n",
      "5                 1.0       0.000000                 1.0      0.0   \n",
      "6                 1.0       0.000000                 1.0      0.0   \n",
      "7                 1.0     111.250907                 1.0      0.0   \n",
      "8                 1.0      10.361828                 1.0      0.0   \n",
      "9                 1.0       6.873042                 1.0      0.0   \n",
      "\n",
      "   youkutudou_tfc_sum  comm_cost  alibaba_tfc_sum  wangyi_tfc_sum  \\\n",
      "0            1.430820        0.1      1632.651359        0.000000   \n",
      "1            0.000000        0.1         7.120826        0.000000   \n",
      "2            0.010518        0.1        41.971141        0.000000   \n",
      "3            0.000000        0.1         2.038849        0.000000   \n",
      "4            0.000000        0.1         0.000000        0.000000   \n",
      "5            0.000000        0.1         0.000000        0.000000   \n",
      "6            0.000000        0.1         0.000000        0.000000   \n",
      "7            0.000000        0.1         1.023041        0.151176   \n",
      "8            0.900500        0.1      3854.779549        0.059286   \n",
      "9            0.141984        0.1        26.576269        0.000000   \n",
      "\n",
      "   xinlangweibo_tfc_sum  kuaishou_tfc_sum  gaodeditu_tfc_sum  aiqiyi_offer  \\\n",
      "0              2.089882          0.000000           0.000000           0.0   \n",
      "1              0.000000          0.000000           0.000000           1.0   \n",
      "2              0.032864          0.000000          23.100998           0.0   \n",
      "3              0.000000          0.000000           2.150902           1.0   \n",
      "4              0.000000          0.000000           0.000000           1.0   \n",
      "5              0.000000          0.000000           0.000000           1.0   \n",
      "6              0.000000          0.000000           0.000000           1.0   \n",
      "7              0.000000          4.002735           0.000000           1.0   \n",
      "8              0.000000          0.000000           0.000000           1.0   \n",
      "9              0.000000          0.275435          17.635478           1.0   \n",
      "\n",
      "   aliyun_offer  julishipin_tfc_sum  baidu_tfc_sum  aiqiyi_tfc_sum  \\\n",
      "0           1.0            0.069620       0.000000         0.00000   \n",
      "1           0.0            0.000000       5.389658         0.00000   \n",
      "2           1.0            0.180759       0.000000         0.00000   \n",
      "3           0.0            0.000000       0.134171         0.00000   \n",
      "4           0.0            0.000000       0.000000         0.00000   \n",
      "5           0.0            0.000000       0.000000         0.00000   \n",
      "6           0.0            0.000000       0.000000         0.00000   \n",
      "7           0.0            0.000000       3.877501         0.00000   \n",
      "8           0.0            0.000000    1283.349406        60.11727   \n",
      "9           0.0            0.000000      73.792200         0.00000   \n",
      "\n",
      "   shuqixiaoshuo_offer  gaodeditu_offer   cm_tfc_sum  youkutudou_offer  price  \\\n",
      "0                  1.0              1.0  3692.180623               1.0   19.0   \n",
      "1                  1.0              1.0   225.580913               1.0   19.0   \n",
      "2                  1.0              1.0  1364.670557               1.0   19.0   \n",
      "3                  1.0              1.0    37.108617               1.0   19.0   \n",
      "4                  1.0              1.0     0.000000               1.0   19.0   \n",
      "5                  1.0              1.0     0.000000               1.0   19.0   \n",
      "6                  1.0              1.0     0.000000               1.0   19.0   \n",
      "7                  1.0              1.0   741.558182               1.0   19.0   \n",
      "8                  1.0              1.0  2488.383275               1.0   19.0   \n",
      "9                  1.0              1.0  1736.505820               1.0   19.0   \n",
      "\n",
      "   xinlangweibo_offer  yixin_offer  jinritoutiao_offer  traffic_cost  \\\n",
      "0                 1.0          0.0                 0.0         800.0   \n",
      "1                 0.0          0.0                 0.0         800.0   \n",
      "2                 1.0          0.0                 0.0         800.0   \n",
      "3                 0.0          0.0                 0.0         800.0   \n",
      "4                 0.0          0.0                 0.0         800.0   \n",
      "5                 0.0          0.0                 0.0         800.0   \n",
      "6                 0.0          0.0                 0.0         800.0   \n",
      "7                 0.0          0.0                 0.0         800.0   \n",
      "8                 0.0          0.0                 0.0         800.0   \n",
      "9                 0.0          0.0                 0.0         800.0   \n",
      "\n",
      "   max_net_speed  aliyun_tfc_sum  xiamiyinyue_offer  xiamiyinyue_tfc_sum  \\\n",
      "0      59.070925       22.009633                1.0                  0.0   \n",
      "1       0.000000        2.670211                1.0                  0.0   \n",
      "2      10.627930       13.256398                1.0                  0.0   \n",
      "3       0.000000        0.000000                1.0                  0.0   \n",
      "4       0.000000        0.000000                1.0                  0.0   \n",
      "5       0.000000        0.000000                1.0                  0.0   \n",
      "6       0.000000        0.000000                1.0                  0.0   \n",
      "7       0.000000        1.097243                1.0                  0.0   \n",
      "8     164.924609       10.607352                1.0                  0.0   \n",
      "9       0.000000        9.054282                1.0                  0.0   \n",
      "\n",
      "   kuaishou_offer  hlwk_offer_aim_flow_cm_round  rel_amount_round  lan_id  \\\n",
      "0             0.0                        5377.0            5160.0    11.0   \n",
      "1             1.0                         604.0            4900.0    11.0   \n",
      "2             0.0                        1445.0            1900.0    15.0   \n",
      "3             1.0                          41.0            2580.0    19.0   \n",
      "4             1.0                           0.0            1970.0    20.0   \n",
      "5             1.0                           0.0            1970.0    21.0   \n",
      "6             1.0                           0.0            2790.0    11.0   \n",
      "7             1.0                        2517.0            2470.0    22.0   \n",
      "8             1.0                        8005.0            5450.0    14.0   \n",
      "9             1.0                        2198.0            3800.0    11.0   \n",
      "\n",
      "   innet_dur  balance  offer_grade  user_terminal_dur  reg_term_price  \\\n",
      "0      216.0   163.60         19.0              240.0          3899.0   \n",
      "1      178.0   142.84         19.0              904.0          2290.0   \n",
      "2      161.0    30.82         19.0              557.0          2499.0   \n",
      "3      219.0    45.41         19.0              530.0             0.0   \n",
      "4      150.0   171.91         19.0              907.0          1999.0   \n",
      "5      155.0     2.26         19.0              119.0             0.0   \n",
      "6      122.0   119.22         19.0                4.0             0.0   \n",
      "7      122.0    50.33         19.0               32.0             0.0   \n",
      "8      120.0    93.84         19.0               37.0             0.0   \n",
      "9      120.0   108.67         19.0               18.0             0.0   \n",
      "\n",
      "   exceed_flow_l1m  surplus_flow_l1m  total_offer_flow_l1m  use_dura_l1m  \\\n",
      "0          1154.23              0.00                1024.0         261.0   \n",
      "1             0.00              0.00                   0.0          45.0   \n",
      "2             0.00            803.29                2048.0         268.0   \n",
      "3            35.37              0.00                   0.0         635.0   \n",
      "4             0.00           2952.79                3072.0         319.0   \n",
      "5             0.00              0.00                   0.0        1022.0   \n",
      "6           719.79              0.00                   0.0         294.0   \n",
      "7            66.56              0.00                   0.0          74.0   \n",
      "8           109.27              0.00                   0.0         127.0   \n",
      "9             1.14              0.00                   0.0           0.0   \n",
      "\n",
      "   use_offer_percent  use_flow_l1m  exceed_flow_l2m  surplus_flow_l2m  \\\n",
      "0             100.00        5259.0           854.85              0.00   \n",
      "1               0.00           0.0             0.00           2048.00   \n",
      "2              60.78        1305.0             0.00           1285.03   \n",
      "3               0.00          55.0            43.11              0.00   \n",
      "4               3.88           0.0             0.00           2982.79   \n",
      "5               0.00           0.0             0.00              0.00   \n",
      "6               0.00        9149.0           884.91              0.00   \n",
      "7               0.00        1277.0            97.34              0.00   \n",
      "8               0.00        8955.0           132.84              0.00   \n",
      "9               0.00           6.0           244.03              0.00   \n",
      "\n",
      "   use_dura_l2m  use_flow_l2m  l1_use_offer_percent  use_dura_avg_3m  \\\n",
      "0         100.0        5781.0                100.00           154.67   \n",
      "1         618.0          61.0                  0.00           346.00   \n",
      "2         115.0         827.0                 37.25           201.33   \n",
      "3         602.0          78.0                  0.00           555.67   \n",
      "4         218.0           0.0                  2.90           250.00   \n",
      "5        1051.0           0.0                  0.00          1066.33   \n",
      "6         450.0        6958.0                  0.00           343.33   \n",
      "7         104.0        1913.0                  0.00            87.00   \n",
      "8         116.0        5657.0                  0.00           152.00   \n",
      "9         125.0        3245.0                  0.00           113.67   \n",
      "\n",
      "   use_flow_avg_3m  surplus_flow_cm  use_dura_cm  use_flow_cm  \\\n",
      "0          4993.45             0.00        288.0       5378.0   \n",
      "1            74.53             0.00         70.0        604.0   \n",
      "2          1012.54           456.75        153.0       1448.0   \n",
      "3            65.50             0.00        341.0         42.0   \n",
      "4             0.19             0.00        246.0          0.0   \n",
      "5             0.02             0.00        892.0          0.0   \n",
      "6          5369.02             0.00        281.0          0.0   \n",
      "7          1132.34             0.00         72.0       2518.0   \n",
      "8          7907.34             0.00        182.0       8003.0   \n",
      "9          2890.16             0.00         94.0       2198.0   \n",
      "\n",
      "   games_app_days_l1m  music_app_days_l1m  video_app_days_l1m  \\\n",
      "0                21.0                28.0                17.0   \n",
      "1                 0.0                 0.0                 0.0   \n",
      "2                25.0                25.0                 9.0   \n",
      "3                 5.0                 2.0                 0.0   \n",
      "4                 0.0                 0.0                 0.0   \n",
      "5                 0.0                 0.0                 1.0   \n",
      "6                 1.0                 3.0                20.0   \n",
      "7                 5.0                 5.0                 5.0   \n",
      "8                31.0                31.0                31.0   \n",
      "9                 0.0                 0.0                 0.0   \n",
      "\n",
      "   shopping_app_days_l1m  own_age  avg_over_flow  s_stop_cnt_sum_3m  \\\n",
      "0                   29.0     65.0         916.11                0.0   \n",
      "1                    0.0     56.0           0.00                0.0   \n",
      "2                   30.0     49.0           0.00                0.0   \n",
      "3                    5.0     57.0          41.56                0.0   \n",
      "4                    0.0     58.0           0.00                0.0   \n",
      "5                    0.0     47.0           0.01                0.0   \n",
      "6                   19.0     52.0         534.90                0.0   \n",
      "7                    7.0     65.0          56.92                0.0   \n",
      "8                    7.0     54.0         240.27                0.0   \n",
      "9                    2.0     59.0         139.42                0.0   \n",
      "\n",
      "   d_stop_cnt_sum_3m  hlwk_offer_aim_flow_cm  l2_use_offer_percent  \\\n",
      "0                0.0                 5377.96                100.00   \n",
      "1                0.0                  604.24                  0.00   \n",
      "2                0.0                 1445.98                 41.46   \n",
      "3                1.0                   41.63                  0.00   \n",
      "4                0.0                    0.02                  3.03   \n",
      "5                0.0                    0.00                  0.00   \n",
      "6                0.0                    0.00                  0.00   \n",
      "7                0.0                 2517.86                  0.00   \n",
      "8                4.0                 8005.08                  0.00   \n",
      "9                0.0                 2198.42                  0.00   \n",
      "\n",
      "   rel_amount  halt_2012  own_gender_cd_index  src_offer_id_index  \\\n",
      "0        51.6        0.0                  0.0                14.0   \n",
      "1        49.0        0.0                  0.0                 6.0   \n",
      "2        19.0        0.0                  0.0                14.0   \n",
      "3        25.8        0.0                  0.0                 6.0   \n",
      "4        19.7        0.0                  0.0                 6.0   \n",
      "5        19.7        0.0                  1.0                 6.0   \n",
      "6        27.9        0.0                  1.0                 6.0   \n",
      "7        24.7        0.0                  0.0                 6.0   \n",
      "8        54.5        0.0                  1.0                 6.0   \n",
      "9        38.0        0.0                  1.0                 6.0   \n",
      "\n",
      "   second_card_slot_l1m_index  rel_amount_round_target  \\\n",
      "0                         1.0                 0.033074   \n",
      "1                         1.0                 0.141241   \n",
      "2                         1.0                 0.125210   \n",
      "3                         0.0                 0.093973   \n",
      "4                         1.0                 0.131704   \n",
      "5                         0.0                 0.131704   \n",
      "6                         1.0                 0.088083   \n",
      "7                         0.0                 0.408494   \n",
      "8                         1.0                 0.031088   \n",
      "9                         1.0                 0.066215   \n",
      "\n",
      "   hlwk_offer_aim_flow_cm_round_target  20201101_calling_duration  \\\n",
      "0                             0.123457                       27.0   \n",
      "1                             0.126582                        0.0   \n",
      "2                             0.088542                       42.0   \n",
      "3                             0.157895                       67.0   \n",
      "4                             0.112742                      199.0   \n",
      "5                             0.112742                       29.0   \n",
      "6                             0.112742                       23.0   \n",
      "7                             0.094017                        0.0   \n",
      "8                             0.092593                       53.0   \n",
      "9                             0.107143                        0.0   \n",
      "\n",
      "   20201101_calling_record_num  20201101_called_duration  \\\n",
      "0                          1.0                       0.0   \n",
      "1                          0.0                       0.0   \n",
      "2                          1.0                     122.0   \n",
      "3                          2.0                     811.0   \n",
      "4                          6.0                     195.0   \n",
      "5                          2.0                     542.0   \n",
      "6                          1.0                      16.0   \n",
      "7                          0.0                       0.0   \n",
      "8                          1.0                     105.0   \n",
      "9                          0.0                       0.0   \n",
      "\n",
      "   20201101_called_record_num  20201102_calling_duration  \\\n",
      "0                         0.0                        0.0   \n",
      "1                         0.0                        0.0   \n",
      "2                         3.0                      281.0   \n",
      "3                         6.0                     2664.0   \n",
      "4                         5.0                        0.0   \n",
      "5                        29.0                       38.0   \n",
      "6                         1.0                        0.0   \n",
      "7                         0.0                        0.0   \n",
      "8                         1.0                      264.0   \n",
      "9                         0.0                        0.0   \n",
      "\n",
      "   20201102_calling_record_num  20201102_called_duration  \\\n",
      "0                          0.0                       0.0   \n",
      "1                          0.0                       0.0   \n",
      "2                          2.0                      64.0   \n",
      "3                         20.0                    1342.0   \n",
      "4                          0.0                     208.0   \n",
      "5                          2.0                     707.0   \n",
      "6                          0.0                      10.0   \n",
      "7                          0.0                      45.0   \n",
      "8                          3.0                     181.0   \n",
      "9                          0.0                       0.0   \n",
      "\n",
      "   20201102_called_record_num  20201103_calling_duration  \\\n",
      "0                         0.0                       24.0   \n",
      "1                         0.0                        0.0   \n",
      "2                         2.0                       46.0   \n",
      "3                        19.0                      807.0   \n",
      "4                         4.0                        0.0   \n",
      "5                        35.0                       18.0   \n",
      "6                         1.0                        9.0   \n",
      "7                         1.0                        0.0   \n",
      "8                         2.0                       67.0   \n",
      "9                         0.0                        0.0   \n",
      "\n",
      "   20201103_calling_record_num  20201103_called_duration  \\\n",
      "0                          1.0                     805.0   \n",
      "1                          0.0                       0.0   \n",
      "2                          1.0                      57.0   \n",
      "3                          6.0                     274.0   \n",
      "4                          0.0                      55.0   \n",
      "5                          1.0                     802.0   \n",
      "6                          1.0                      16.0   \n",
      "7                          0.0                      13.0   \n",
      "8                          1.0                       0.0   \n",
      "9                          0.0                       0.0   \n",
      "\n",
      "   20201103_called_record_num  20201104_calling_duration  \\\n",
      "0                         3.0                     2142.0   \n",
      "1                         0.0                        0.0   \n",
      "2                         1.0                      146.0   \n",
      "3                         5.0                      158.0   \n",
      "4                         2.0                       35.0   \n",
      "5                        33.0                       16.0   \n",
      "6                         1.0                        0.0   \n",
      "7                         1.0                        0.0   \n",
      "8                         0.0                      216.0   \n",
      "9                         0.0                        0.0   \n",
      "\n",
      "   20201104_calling_record_num  20201104_called_duration  \\\n",
      "0                          6.0                     701.0   \n",
      "1                          0.0                       0.0   \n",
      "2                          2.0                      63.0   \n",
      "3                          1.0                     846.0   \n",
      "4                          2.0                     327.0   \n",
      "5                          1.0                     561.0   \n",
      "6                          0.0                      21.0   \n",
      "7                          0.0                       0.0   \n",
      "8                          5.0                       0.0   \n",
      "9                          0.0                       0.0   \n",
      "\n",
      "   20201104_called_record_num  20201105_calling_duration  \\\n",
      "0                         1.0                     2899.0   \n",
      "1                         0.0                        0.0   \n",
      "2                         4.0                        0.0   \n",
      "3                         2.0                        0.0   \n",
      "4                         3.0                       29.0   \n",
      "5                        30.0                       45.0   \n",
      "6                         1.0                      516.0   \n",
      "7                         0.0                        0.0   \n",
      "8                         0.0                       41.0   \n",
      "9                         0.0                        0.0   \n",
      "\n",
      "   20201105_calling_record_num  20201105_called_duration  \\\n",
      "0                          3.0                     341.0   \n",
      "1                          0.0                       0.0   \n",
      "2                          0.0                      60.0   \n",
      "3                          0.0                     168.0   \n",
      "4                          2.0                     219.0   \n",
      "5                          1.0                     352.0   \n",
      "6                          3.0                      99.0   \n",
      "7                          0.0                       0.0   \n",
      "8                          2.0                     266.0   \n",
      "9                          0.0                       0.0   \n",
      "\n",
      "   20201105_called_record_num  20201106_calling_duration  \\\n",
      "0                         1.0                      430.0   \n",
      "1                         0.0                        0.0   \n",
      "2                         2.0                        0.0   \n",
      "3                         3.0                      494.0   \n",
      "4                         4.0                       14.0   \n",
      "5                        22.0                       58.0   \n",
      "6                         3.0                     1129.0   \n",
      "7                         0.0                       36.0   \n",
      "8                         1.0                       84.0   \n",
      "9                         0.0                        0.0   \n",
      "\n",
      "   20201106_calling_record_num  20201106_called_duration  \\\n",
      "0                          4.0                     266.0   \n",
      "1                          0.0                       0.0   \n",
      "2                          0.0                      33.0   \n",
      "3                          3.0                     203.0   \n",
      "4                          1.0                     159.0   \n",
      "5                          2.0                     521.0   \n",
      "6                          6.0                     989.0   \n",
      "7                          2.0                      91.0   \n",
      "8                          2.0                       0.0   \n",
      "9                          0.0                       0.0   \n",
      "\n",
      "   20201106_called_record_num  20201107_calling_duration  \\\n",
      "0                         6.0                      473.0   \n",
      "1                         0.0                        0.0   \n",
      "2                         1.0                      118.0   \n",
      "3                         5.0                      188.0   \n",
      "4                         3.0                       38.0   \n",
      "5                        35.0                        0.0   \n",
      "6                         5.0                        0.0   \n",
      "7                         1.0                        0.0   \n",
      "8                         0.0                      135.0   \n",
      "9                         0.0                        0.0   \n",
      "\n",
      "   20201107_calling_record_num  20201107_called_duration  \\\n",
      "0                          2.0                     106.0   \n",
      "1                          0.0                       0.0   \n",
      "2                          2.0                       0.0   \n",
      "3                          4.0                       0.0   \n",
      "4                          1.0                      69.0   \n",
      "5                          0.0                     415.0   \n",
      "6                          0.0                      73.0   \n",
      "7                          0.0                       0.0   \n",
      "8                          1.0                      41.0   \n",
      "9                          0.0                       0.0   \n",
      "\n",
      "   20201107_called_record_num  20201108_calling_duration  \\\n",
      "0                         2.0                      187.0   \n",
      "1                         0.0                        0.0   \n",
      "2                         0.0                       41.0   \n",
      "3                         0.0                      274.0   \n",
      "4                         2.0                        0.0   \n",
      "5                        26.0                        0.0   \n",
      "6                         3.0                        0.0   \n",
      "7                         0.0                        0.0   \n",
      "8                         2.0                      153.0   \n",
      "9                         0.0                        0.0   \n",
      "\n",
      "   20201108_calling_record_num  20201108_called_duration  \\\n",
      "0                          2.0                     492.0   \n",
      "1                          0.0                       0.0   \n",
      "2                          1.0                       1.0   \n",
      "3                          3.0                     920.0   \n",
      "4                          0.0                       0.0   \n",
      "5                          0.0                     342.0   \n",
      "6                          0.0                      23.0   \n",
      "7                          0.0                       0.0   \n",
      "8                          3.0                     280.0   \n",
      "9                          0.0                       0.0   \n",
      "\n",
      "   20201108_called_record_num  20201109_calling_duration  \\\n",
      "0                         3.0                       28.0   \n",
      "1                         0.0                        0.0   \n",
      "2                         1.0                        0.0   \n",
      "3                         5.0                      449.0   \n",
      "4                         0.0                      135.0   \n",
      "5                        21.0                        0.0   \n",
      "6                         1.0                      841.0   \n",
      "7                         0.0                        0.0   \n",
      "8                         2.0                      356.0   \n",
      "9                         0.0                        0.0   \n",
      "\n",
      "   20201109_calling_record_num  20201109_called_duration  \\\n",
      "0                          1.0                     213.0   \n",
      "1                          0.0                       0.0   \n",
      "2                          0.0                       0.0   \n",
      "3                         11.0                     342.0   \n",
      "4                          5.0                     374.0   \n",
      "5                          0.0                     370.0   \n",
      "6                          8.0                     331.0   \n",
      "7                          0.0                      69.0   \n",
      "8                          6.0                       0.0   \n",
      "9                          0.0                       0.0   \n",
      "\n",
      "   20201109_called_record_num  20201110_calling_duration  \\\n",
      "0                         1.0                     3049.0   \n",
      "1                         0.0                       23.0   \n",
      "2                         0.0                      136.0   \n",
      "3                         4.0                       75.0   \n",
      "4                         7.0                       18.0   \n",
      "5                        23.0                       43.0   \n",
      "6                         6.0                      449.0   \n",
      "7                         1.0                        0.0   \n",
      "8                         0.0                       43.0   \n",
      "9                         0.0                        0.0   \n",
      "\n",
      "   20201110_calling_record_num  20201110_called_duration  \\\n",
      "0                          2.0                       0.0   \n",
      "1                          1.0                       0.0   \n",
      "2                          2.0                     130.0   \n",
      "3                          1.0                     194.0   \n",
      "4                          1.0                     336.0   \n",
      "5                          4.0                     320.0   \n",
      "6                          5.0                       1.0   \n",
      "7                          0.0                       0.0   \n",
      "8                          2.0                     166.0   \n",
      "9                          0.0                       0.0   \n",
      "\n",
      "   20201110_called_record_num  20201111_calling_duration  \\\n",
      "0                         0.0                     1097.0   \n",
      "1                         0.0                      238.0   \n",
      "2                         3.0                      122.0   \n",
      "3                         3.0                       14.0   \n",
      "4                         4.0                      167.0   \n",
      "5                        23.0                        0.0   \n",
      "6                         1.0                      253.0   \n",
      "7                         0.0                        0.0   \n",
      "8                         5.0                       55.0   \n",
      "9                         0.0                        0.0   \n",
      "\n",
      "   20201111_calling_record_num  20201111_called_duration  \\\n",
      "0                          4.0                      11.0   \n",
      "1                          3.0                      27.0   \n",
      "2                          4.0                     706.0   \n",
      "3                          1.0                     484.0   \n",
      "4                          4.0                      85.0   \n",
      "5                          0.0                     448.0   \n",
      "6                          1.0                      19.0   \n",
      "7                          0.0                     122.0   \n",
      "8                          2.0                       0.0   \n",
      "9                          0.0                       0.0   \n",
      "\n",
      "   20201111_called_record_num  20201112_calling_duration  \\\n",
      "0                         2.0                        0.0   \n",
      "1                         1.0                     2426.0   \n",
      "2                         6.0                       19.0   \n",
      "3                         5.0                       57.0   \n",
      "4                         5.0                        0.0   \n",
      "5                        25.0                       41.0   \n",
      "6                         1.0                       38.0   \n",
      "7                         3.0                        0.0   \n",
      "8                         0.0                      234.0   \n",
      "9                         0.0                        0.0   \n",
      "\n",
      "   20201112_calling_record_num  20201112_called_duration  \\\n",
      "0                          0.0                       0.0   \n",
      "1                          9.0                     219.0   \n",
      "2                          1.0                     352.0   \n",
      "3                          1.0                     789.0   \n",
      "4                          0.0                     250.0   \n",
      "5                          1.0                     470.0   \n",
      "6                          1.0                     344.0   \n",
      "7                          0.0                       0.0   \n",
      "8                          3.0                       0.0   \n",
      "9                          0.0                       0.0   \n",
      "\n",
      "   20201112_called_record_num  20201113_calling_duration  \\\n",
      "0                         0.0                       21.0   \n",
      "1                         5.0                       39.0   \n",
      "2                         3.0                      196.0   \n",
      "3                         1.0                      577.0   \n",
      "4                         3.0                      229.0   \n",
      "5                        29.0                        0.0   \n",
      "6                         2.0                      557.0   \n",
      "7                         0.0                        0.0   \n",
      "8                         0.0                       35.0   \n",
      "9                         0.0                      293.0   \n",
      "\n",
      "   20201113_calling_record_num  20201113_called_duration  \\\n",
      "0                          1.0                     478.0   \n",
      "1                          1.0                       0.0   \n",
      "2                          2.0                       0.0   \n",
      "3                          3.0                     125.0   \n",
      "4                          6.0                     149.0   \n",
      "5                          0.0                     772.0   \n",
      "6                          4.0                     205.0   \n",
      "7                          0.0                       0.0   \n",
      "8                          1.0                       0.0   \n",
      "9                          1.0                      78.0   \n",
      "\n",
      "   20201113_called_record_num  20201114_calling_duration  \\\n",
      "0                         1.0                        0.0   \n",
      "1                         0.0                        0.0   \n",
      "2                         0.0                       55.0   \n",
      "3                         2.0                      178.0   \n",
      "4                         1.0                      103.0   \n",
      "5                        34.0                        0.0   \n",
      "6                         3.0                       71.0   \n",
      "7                         0.0                       15.0   \n",
      "8                         0.0                       71.0   \n",
      "9                         1.0                        0.0   \n",
      "\n",
      "   20201114_calling_record_num  20201114_called_duration  \\\n",
      "0                          0.0                      99.0   \n",
      "1                          0.0                       0.0   \n",
      "2                          2.0                      25.0   \n",
      "3                          1.0                     268.0   \n",
      "4                          4.0                     332.0   \n",
      "5                          0.0                     399.0   \n",
      "6                          3.0                     637.0   \n",
      "7                          1.0                       0.0   \n",
      "8                          1.0                       0.0   \n",
      "9                          0.0                      74.0   \n",
      "\n",
      "   20201114_called_record_num  20201115_calling_duration  \\\n",
      "0                         1.0                      216.0   \n",
      "1                         0.0                        0.0   \n",
      "2                         1.0                        0.0   \n",
      "3                         6.0                       41.0   \n",
      "4                         8.0                       51.0   \n",
      "5                        28.0                      168.0   \n",
      "6                         3.0                        0.0   \n",
      "7                         0.0                        0.0   \n",
      "8                         0.0                      374.0   \n",
      "9                         1.0                      486.0   \n",
      "\n",
      "   20201115_calling_record_num  20201115_called_duration  \\\n",
      "0                          7.0                     256.0   \n",
      "1                          0.0                       0.0   \n",
      "2                          0.0                     492.0   \n",
      "3                          1.0                     117.0   \n",
      "4                          2.0                      26.0   \n",
      "5                          6.0                     738.0   \n",
      "6                          0.0                      58.0   \n",
      "7                          0.0                       0.0   \n",
      "8                          5.0                      47.0   \n",
      "9                          4.0                    1059.0   \n",
      "\n",
      "   20201115_called_record_num  20201116_calling_duration  \\\n",
      "0                         1.0                      113.0   \n",
      "1                         0.0                        0.0   \n",
      "2                         3.0                      223.0   \n",
      "3                         1.0                        0.0   \n",
      "4                         1.0                      108.0   \n",
      "5                        35.0                       12.0   \n",
      "6                         1.0                       42.0   \n",
      "7                         0.0                        0.0   \n",
      "8                         2.0                       71.0   \n",
      "9                         5.0                      242.0   \n",
      "\n",
      "   20201116_calling_record_num  20201116_called_duration  \\\n",
      "0                          3.0                       0.0   \n",
      "1                          0.0                       0.0   \n",
      "2                          2.0                     194.0   \n",
      "3                          0.0                      14.0   \n",
      "4                          1.0                    1064.0   \n",
      "5                          1.0                     346.0   \n",
      "6                          1.0                     375.0   \n",
      "7                          0.0                       8.0   \n",
      "8                          2.0                     257.0   \n",
      "9                          1.0                       0.0   \n",
      "\n",
      "   20201116_called_record_num  20201117_calling_duration  \\\n",
      "0                         0.0                        0.0   \n",
      "1                         0.0                        0.0   \n",
      "2                         3.0                        0.0   \n",
      "3                         1.0                        7.0   \n",
      "4                         5.0                       71.0   \n",
      "5                        24.0                       39.0   \n",
      "6                         3.0                       38.0   \n",
      "7                         1.0                       94.0   \n",
      "8                         3.0                       25.0   \n",
      "9                         0.0                        0.0   \n",
      "\n",
      "   20201117_calling_record_num  20201117_called_duration  \\\n",
      "0                          0.0                       0.0   \n",
      "1                          0.0                       0.0   \n",
      "2                          0.0                       0.0   \n",
      "3                          1.0                     328.0   \n",
      "4                          1.0                     110.0   \n",
      "5                          1.0                     361.0   \n",
      "6                          1.0                       0.0   \n",
      "7                          2.0                     131.0   \n",
      "8                          2.0                      26.0   \n",
      "9                          0.0                     161.0   \n",
      "\n",
      "   20201117_called_record_num  20201118_calling_duration  \\\n",
      "0                         0.0                       25.0   \n",
      "1                         0.0                        0.0   \n",
      "2                         0.0                        0.0   \n",
      "3                         2.0                        0.0   \n",
      "4                         5.0                        0.0   \n",
      "5                        25.0                      146.0   \n",
      "6                         0.0                      184.0   \n",
      "7                         1.0                      210.0   \n",
      "8                         1.0                      225.0   \n",
      "9                         2.0                      711.0   \n",
      "\n",
      "   20201118_calling_record_num  20201118_called_duration  \\\n",
      "0                          1.0                       0.0   \n",
      "1                          0.0                       0.0   \n",
      "2                          0.0                      21.0   \n",
      "3                          0.0                     460.0   \n",
      "4                          0.0                      10.0   \n",
      "5                          6.0                     499.0   \n",
      "6                          3.0                       0.0   \n",
      "7                          4.0                      42.0   \n",
      "8                          3.0                      35.0   \n",
      "9                          5.0                      53.0   \n",
      "\n",
      "   20201118_called_record_num  20201119_calling_duration  \\\n",
      "0                         0.0                        0.0   \n",
      "1                         0.0                        0.0   \n",
      "2                         1.0                       23.0   \n",
      "3                         3.0                        0.0   \n",
      "4                         1.0                        0.0   \n",
      "5                        28.0                       47.0   \n",
      "6                         0.0                      110.0   \n",
      "7                         1.0                        0.0   \n",
      "8                         1.0                      127.0   \n",
      "9                         3.0                        0.0   \n",
      "\n",
      "   20201119_calling_record_num  20201119_called_duration  \\\n",
      "0                          0.0                      96.0   \n",
      "1                          0.0                       0.0   \n",
      "2                          1.0                     567.0   \n",
      "3                          0.0                      72.0   \n",
      "4                          0.0                      84.0   \n",
      "5                          2.0                     475.0   \n",
      "6                          2.0                     130.0   \n",
      "7                          0.0                       0.0   \n",
      "8                          7.0                       0.0   \n",
      "9                          0.0                     266.0   \n",
      "\n",
      "   20201119_called_record_num  20201120_calling_duration  \\\n",
      "0                         1.0                        0.0   \n",
      "1                         0.0                        0.0   \n",
      "2                         1.0                      200.0   \n",
      "3                         1.0                        0.0   \n",
      "4                         2.0                      513.0   \n",
      "5                        28.0                       13.0   \n",
      "6                         3.0                       21.0   \n",
      "7                         0.0                        0.0   \n",
      "8                         0.0                        0.0   \n",
      "9                         3.0                        0.0   \n",
      "\n",
      "   20201120_calling_record_num  20201120_called_duration  \\\n",
      "0                          0.0                       0.0   \n",
      "1                          0.0                       0.0   \n",
      "2                          2.0                       0.0   \n",
      "3                          0.0                     336.0   \n",
      "4                          4.0                     222.0   \n",
      "5                          1.0                     337.0   \n",
      "6                          1.0                      18.0   \n",
      "7                          0.0                       0.0   \n",
      "8                          0.0                      49.0   \n",
      "9                          0.0                      66.0   \n",
      "\n",
      "   20201120_called_record_num  20201121_calling_duration  \\\n",
      "0                         0.0                        0.0   \n",
      "1                         0.0                        0.0   \n",
      "2                         0.0                        0.0   \n",
      "3                         3.0                        0.0   \n",
      "4                         4.0                      358.0   \n",
      "5                        21.0                        5.0   \n",
      "6                         1.0                      814.0   \n",
      "7                         0.0                        0.0   \n",
      "8                         3.0                      157.0   \n",
      "9                         1.0                        0.0   \n",
      "\n",
      "   20201121_calling_record_num  20201121_called_duration  \\\n",
      "0                          0.0                     133.0   \n",
      "1                          0.0                       0.0   \n",
      "2                          0.0                     144.0   \n",
      "3                          0.0                       0.0   \n",
      "4                          3.0                     108.0   \n",
      "5                          1.0                     535.0   \n",
      "6                          2.0                      30.0   \n",
      "7                          0.0                     228.0   \n",
      "8                          3.0                     210.0   \n",
      "9                          0.0                     213.0   \n",
      "\n",
      "   20201121_called_record_num  20201122_calling_duration  \\\n",
      "0                         1.0                       30.0   \n",
      "1                         0.0                       36.0   \n",
      "2                         4.0                      657.0   \n",
      "3                         0.0                        0.0   \n",
      "4                         3.0                        0.0   \n",
      "5                        29.0                      119.0   \n",
      "6                         1.0                      112.0   \n",
      "7                         3.0                        0.0   \n",
      "8                         4.0                       81.0   \n",
      "9                         3.0                       48.0   \n",
      "\n",
      "   20201122_calling_record_num  20201122_called_duration  \\\n",
      "0                          2.0                      55.0   \n",
      "1                          1.0                      72.0   \n",
      "2                          3.0                      76.0   \n",
      "3                          0.0                       0.0   \n",
      "4                          0.0                      31.0   \n",
      "5                          1.0                     655.0   \n",
      "6                          1.0                     135.0   \n",
      "7                          0.0                      22.0   \n",
      "8                          3.0                      27.0   \n",
      "9                          1.0                      10.0   \n",
      "\n",
      "   20201122_called_record_num  20201123_calling_duration  \\\n",
      "0                         1.0                        0.0   \n",
      "1                         1.0                        0.0   \n",
      "2                         1.0                        0.0   \n",
      "3                         0.0                        0.0   \n",
      "4                         1.0                        0.0   \n",
      "5                        33.0                        5.0   \n",
      "6                         2.0                      190.0   \n",
      "7                         1.0                        0.0   \n",
      "8                         1.0                        0.0   \n",
      "9                         1.0                        0.0   \n",
      "\n",
      "   20201123_calling_record_num  20201123_called_duration  \\\n",
      "0                          0.0                      88.0   \n",
      "1                          0.0                       0.0   \n",
      "2                          0.0                      29.0   \n",
      "3                          0.0                       0.0   \n",
      "4                          0.0                       0.0   \n",
      "5                          1.0                     997.0   \n",
      "6                          4.0                       0.0   \n",
      "7                          0.0                     116.0   \n",
      "8                          0.0                       0.0   \n",
      "9                          0.0                       0.0   \n",
      "\n",
      "   20201123_called_record_num  20201124_calling_duration  \\\n",
      "0                         1.0                        0.0   \n",
      "1                         0.0                        0.0   \n",
      "2                         2.0                       51.0   \n",
      "3                         0.0                      176.0   \n",
      "4                         0.0                        0.0   \n",
      "5                        26.0                       12.0   \n",
      "6                         0.0                      140.0   \n",
      "7                         3.0                      157.0   \n",
      "8                         0.0                      251.0   \n",
      "9                         0.0                        0.0   \n",
      "\n",
      "   20201124_calling_record_num  20201124_called_duration  \\\n",
      "0                          0.0                       0.0   \n",
      "1                          0.0                       0.0   \n",
      "2                          2.0                     124.0   \n",
      "3                          1.0                       0.0   \n",
      "4                          0.0                     158.0   \n",
      "5                          1.0                     424.0   \n",
      "6                          3.0                      19.0   \n",
      "7                          2.0                     214.0   \n",
      "8                          2.0                     238.0   \n",
      "9                          0.0                      76.0   \n",
      "\n",
      "   20201124_called_record_num  20201125_calling_duration  \\\n",
      "0                         0.0                        0.0   \n",
      "1                         0.0                        0.0   \n",
      "2                         1.0                        0.0   \n",
      "3                         0.0                        0.0   \n",
      "4                         2.0                       99.0   \n",
      "5                        24.0                        4.0   \n",
      "6                         1.0                      116.0   \n",
      "7                         1.0                        0.0   \n",
      "8                         3.0                      117.0   \n",
      "9                         2.0                        0.0   \n",
      "\n",
      "   20201125_calling_record_num  20201125_called_duration  \\\n",
      "0                          0.0                       0.0   \n",
      "1                          0.0                       0.0   \n",
      "2                          0.0                      16.0   \n",
      "3                          0.0                      28.0   \n",
      "4                          1.0                     812.0   \n",
      "5                          1.0                     385.0   \n",
      "6                          3.0                      22.0   \n",
      "7                          0.0                     441.0   \n",
      "8                          4.0                      14.0   \n",
      "9                          0.0                      94.0   \n",
      "\n",
      "   20201125_called_record_num  20201126_calling_duration  \\\n",
      "0                         0.0                       16.0   \n",
      "1                         0.0                        0.0   \n",
      "2                         2.0                       71.0   \n",
      "3                         1.0                       40.0   \n",
      "4                         4.0                      356.0   \n",
      "5                        25.0                        0.0   \n",
      "6                         1.0                     1894.0   \n",
      "7                         1.0                        0.0   \n",
      "8                         2.0                       76.0   \n",
      "9                         3.0                        0.0   \n",
      "\n",
      "   20201126_calling_record_num  20201126_called_duration  \\\n",
      "0                          1.0                       0.0   \n",
      "1                          0.0                       0.0   \n",
      "2                          1.0                       0.0   \n",
      "3                          1.0                     121.0   \n",
      "4                          8.0                     196.0   \n",
      "5                          0.0                     691.0   \n",
      "6                          7.0                     147.0   \n",
      "7                          0.0                     342.0   \n",
      "8                          2.0                     216.0   \n",
      "9                          0.0                      35.0   \n",
      "\n",
      "   20201126_called_record_num  20201127_calling_duration  \\\n",
      "0                         0.0                        0.0   \n",
      "1                         0.0                        0.0   \n",
      "2                         0.0                      214.0   \n",
      "3                         3.0                        0.0   \n",
      "4                         5.0                      139.0   \n",
      "5                        27.0                       70.0   \n",
      "6                         1.0                      102.0   \n",
      "7                         1.0                       56.0   \n",
      "8                         1.0                       79.0   \n",
      "9                         2.0                        0.0   \n",
      "\n",
      "   20201127_calling_record_num  20201127_called_duration  \\\n",
      "0                          0.0                      44.0   \n",
      "1                          0.0                     401.0   \n",
      "2                          1.0                       0.0   \n",
      "3                          0.0                      58.0   \n",
      "4                          2.0                     230.0   \n",
      "5                          3.0                     220.0   \n",
      "6                          2.0                      39.0   \n",
      "7                          1.0                       2.0   \n",
      "8                          4.0                       6.0   \n",
      "9                          0.0                      88.0   \n",
      "\n",
      "   20201127_called_record_num  20201128_calling_duration  \\\n",
      "0                         1.0                        0.0   \n",
      "1                         1.0                        0.0   \n",
      "2                         0.0                        0.0   \n",
      "3                         1.0                        0.0   \n",
      "4                         5.0                      263.0   \n",
      "5                        18.0                        0.0   \n",
      "6                         1.0                      469.0   \n",
      "7                         1.0                        0.0   \n",
      "8                         1.0                       21.0   \n",
      "9                         2.0                        0.0   \n",
      "\n",
      "   20201128_calling_record_num  20201128_called_duration  \\\n",
      "0                          0.0                       0.0   \n",
      "1                          0.0                       0.0   \n",
      "2                          0.0                       0.0   \n",
      "3                          0.0                     365.0   \n",
      "4                          8.0                     228.0   \n",
      "5                          0.0                     143.0   \n",
      "6                          8.0                       0.0   \n",
      "7                          0.0                     123.0   \n",
      "8                          1.0                      14.0   \n",
      "9                          0.0                      18.0   \n",
      "\n",
      "   20201128_called_record_num  20201129_calling_duration  \\\n",
      "0                         0.0                        0.0   \n",
      "1                         0.0                        0.0   \n",
      "2                         0.0                        0.0   \n",
      "3                         3.0                        0.0   \n",
      "4                         7.0                       27.0   \n",
      "5                        16.0                       14.0   \n",
      "6                         0.0                      254.0   \n",
      "7                         2.0                        0.0   \n",
      "8                         1.0                      510.0   \n",
      "9                         1.0                        0.0   \n",
      "\n",
      "   20201129_calling_record_num  20201129_called_duration  \\\n",
      "0                          0.0                       0.0   \n",
      "1                          0.0                       0.0   \n",
      "2                          0.0                       0.0   \n",
      "3                          0.0                      63.0   \n",
      "4                          1.0                      56.0   \n",
      "5                          1.0                     456.0   \n",
      "6                          8.0                     358.0   \n",
      "7                          0.0                      13.0   \n",
      "8                          8.0                     294.0   \n",
      "9                          0.0                      55.0   \n",
      "\n",
      "   20201129_called_record_num  20201130_calling_duration  \\\n",
      "0                         0.0                       43.0   \n",
      "1                         0.0                        0.0   \n",
      "2                         0.0                      905.0   \n",
      "3                         1.0                      142.0   \n",
      "4                         1.0                       60.0   \n",
      "5                        32.0                       25.0   \n",
      "6                         6.0                      130.0   \n",
      "7                         1.0                      211.0   \n",
      "8                         6.0                      174.0   \n",
      "9                         1.0                        0.0   \n",
      "\n",
      "   20201130_calling_record_num  20201130_called_duration  \\\n",
      "0                          1.0                       0.0   \n",
      "1                          0.0                       0.0   \n",
      "2                          4.0                     168.0   \n",
      "3                          1.0                     305.0   \n",
      "4                          3.0                      11.0   \n",
      "5                          2.0                     508.0   \n",
      "6                          2.0                       0.0   \n",
      "7                          1.0                     158.0   \n",
      "8                          4.0                     114.0   \n",
      "9                          0.0                       0.0   \n",
      "\n",
      "   20201130_called_record_num  20201101_byte_in  20201101_record_num  \\\n",
      "0                         0.0       194346851.0                 19.0   \n",
      "1                         0.0               0.0                  0.0   \n",
      "2                         5.0        50780667.0                 21.0   \n",
      "3                         6.0               0.0                  0.0   \n",
      "4                         1.0            3582.0                  1.0   \n",
      "5                        31.0               0.0                  0.0   \n",
      "6                         0.0               0.0                  0.0   \n",
      "7                         4.0               0.0                  0.0   \n",
      "8                         1.0       410992126.0                 81.0   \n",
      "9                         0.0               0.0                  0.0   \n",
      "\n",
      "   20201101_byte_out  20201101_duration  20201102_byte_in  \\\n",
      "0         10041132.0            23374.0         2825039.0   \n",
      "1                0.0                0.0           21840.0   \n",
      "2         13349812.0            39937.0       138354713.0   \n",
      "3                0.0                0.0        20474912.0   \n",
      "4             2978.0              300.0            3582.0   \n",
      "5                0.0                0.0               0.0   \n",
      "6                0.0                0.0               0.0   \n",
      "7                0.0                0.0               0.0   \n",
      "8         18899900.0           156481.0       549462966.0   \n",
      "9                0.0                0.0               0.0   \n",
      "\n",
      "   20201102_record_num  20201102_byte_out  20201102_duration  \\\n",
      "0                  7.0           524979.0             5152.0   \n",
      "1                  1.0                0.0              193.0   \n",
      "2                 17.0          6496666.0            22157.0   \n",
      "3                 11.0          2539558.0            22922.0   \n",
      "4                  1.0             2978.0              300.0   \n",
      "5                  0.0                0.0                0.0   \n",
      "6                  0.0                0.0                0.0   \n",
      "7                  0.0                0.0                0.0   \n",
      "8                 71.0         22987057.0           156910.0   \n",
      "9                  0.0                0.0                0.0   \n",
      "\n",
      "   20201103_byte_in  20201103_record_num  20201103_byte_out  \\\n",
      "0       101050835.0                 17.0         26920834.0   \n",
      "1               0.0                  0.0                0.0   \n",
      "2        22045880.0                 15.0          2209411.0   \n",
      "3          954424.0                 18.0           567128.0   \n",
      "4            4074.0                  1.0             3310.0   \n",
      "5               0.0                  0.0                0.0   \n",
      "6               0.0                  0.0                0.0   \n",
      "7               0.0                  0.0                0.0   \n",
      "8       565008342.0                 68.0         24936588.0   \n",
      "9               0.0                  0.0                0.0   \n",
      "\n",
      "   20201103_duration  20201104_byte_in  20201104_record_num  \\\n",
      "0            16312.0       185824515.0                 25.0   \n",
      "1                0.0               0.0                  0.0   \n",
      "2            21108.0        13957138.0                 13.0   \n",
      "3            31212.0               0.0                  0.0   \n",
      "4              298.0            3653.0                  1.0   \n",
      "5                0.0               0.0                  0.0   \n",
      "6                0.0               0.0                  0.0   \n",
      "7                0.0               0.0                  0.0   \n",
      "8           173273.0       303028720.0                 59.0   \n",
      "9                0.0               0.0                  0.0   \n",
      "\n",
      "   20201104_byte_out  20201104_duration  20201105_byte_in  \\\n",
      "0         14728132.0            19910.0         1465236.0   \n",
      "1                0.0                0.0               0.0   \n",
      "2          3106946.0            12832.0        13723790.0   \n",
      "3                0.0                0.0               0.0   \n",
      "4             3022.0              301.0            3684.0   \n",
      "5                0.0                0.0               0.0   \n",
      "6                0.0                0.0               0.0   \n",
      "7                0.0                0.0               0.0   \n",
      "8         14180439.0           152011.0       269537957.0   \n",
      "9                0.0                0.0               0.0   \n",
      "\n",
      "   20201105_record_num  20201105_byte_out  20201105_duration  \\\n",
      "0                  4.0           356005.0             5282.0   \n",
      "1                  0.0                0.0                0.0   \n",
      "2                 18.0          3340970.0            26836.0   \n",
      "3                  0.0                0.0                0.0   \n",
      "4                  1.0             2978.0              298.0   \n",
      "5                  0.0                0.0                0.0   \n",
      "6                  0.0                0.0                0.0   \n",
      "7                  0.0                0.0                0.0   \n",
      "8                 58.0         13515774.0           159465.0   \n",
      "9                  0.0                0.0                0.0   \n",
      "\n",
      "   20201106_byte_in  20201106_record_num  20201106_byte_out  \\\n",
      "0       164591374.0                 26.0         13379333.0   \n",
      "1           10920.0                  1.0                0.0   \n",
      "2         3633534.0                  8.0           695260.0   \n",
      "3               0.0                  0.0                0.0   \n",
      "4            3684.0                  1.0             2978.0   \n",
      "5               0.0                  0.0                0.0   \n",
      "6               0.0                  0.0                0.0   \n",
      "7               0.0                  0.0                0.0   \n",
      "8       562640887.0                 75.0         21264841.0   \n",
      "9               0.0                  0.0                0.0   \n",
      "\n",
      "   20201106_duration  20201107_byte_in  20201107_record_num  \\\n",
      "0            50567.0       229911607.0                 27.0   \n",
      "1               14.0               0.0                  0.0   \n",
      "2             9154.0        13779543.0                  6.0   \n",
      "3                0.0               0.0                  0.0   \n",
      "4              320.0            3653.0                  1.0   \n",
      "5                0.0               0.0                  0.0   \n",
      "6                0.0               0.0                  0.0   \n",
      "7                0.0               0.0                  0.0   \n",
      "8           177339.0       756854462.0                 80.0   \n",
      "9                0.0               0.0                  0.0   \n",
      "\n",
      "   20201107_byte_out  20201107_duration  20201108_byte_in  \\\n",
      "0         20875284.0            34525.0       119863968.0   \n",
      "1                0.0                0.0               0.0   \n",
      "2          1158756.0             7649.0        25687839.0   \n",
      "3                0.0                0.0               0.0   \n",
      "4             2978.0              300.0            3684.0   \n",
      "5                0.0                0.0               0.0   \n",
      "6                0.0                0.0               0.0   \n",
      "7                0.0                0.0               0.0   \n",
      "8         27275892.0           181038.0       340403047.0   \n",
      "9                0.0                0.0               0.0   \n",
      "\n",
      "   20201108_record_num  20201108_byte_out  20201108_duration  \\\n",
      "0                 20.0          9551848.0            22174.0   \n",
      "1                  0.0                0.0                0.0   \n",
      "2                 28.0          4761940.0            43395.0   \n",
      "3                  0.0                0.0                0.0   \n",
      "4                  1.0             2978.0              299.0   \n",
      "5                  0.0                0.0                0.0   \n",
      "6                  0.0                0.0                0.0   \n",
      "7                  0.0                0.0                0.0   \n",
      "8                 73.0         19625641.0           173797.0   \n",
      "9                  0.0                0.0                0.0   \n",
      "\n",
      "   20201109_byte_in  20201109_record_num  20201109_byte_out  \\\n",
      "0       164074519.0                 22.0         12567763.0   \n",
      "1               0.0                  0.0                0.0   \n",
      "2        10060581.0                 14.0          3480943.0   \n",
      "3        16863855.0                  9.0          1563241.0   \n",
      "4            3384.0                  1.0             2978.0   \n",
      "5               0.0                  0.0                0.0   \n",
      "6               0.0                  0.0                0.0   \n",
      "7       596549019.0                 28.0         22299370.0   \n",
      "8       537082404.0                 94.0         19674885.0   \n",
      "9               0.0                  0.0                0.0   \n",
      "\n",
      "   20201109_duration  20201110_byte_in  20201110_record_num  \\\n",
      "0            22315.0       147363343.0                 27.0   \n",
      "1                0.0        37544415.0                 38.0   \n",
      "2            20359.0        40925431.0                 15.0   \n",
      "3            20588.0          137896.0                  4.0   \n",
      "4              298.0            4264.0                  1.0   \n",
      "5                0.0               0.0                  0.0   \n",
      "6                0.0               0.0                  0.0   \n",
      "7            37867.0               0.0                  0.0   \n",
      "8           171255.0       713631028.0                 72.0   \n",
      "9                0.0               0.0                  0.0   \n",
      "\n",
      "   20201110_byte_out  20201110_duration  20201111_byte_in  \\\n",
      "0         10701539.0            23830.0         4184218.0   \n",
      "1          8278185.0            27459.0        40200534.0   \n",
      "2         12015003.0            29657.0        16418795.0   \n",
      "3           106206.0             7104.0               0.0   \n",
      "4             3559.0              298.0            3684.0   \n",
      "5                0.0                0.0               0.0   \n",
      "6                0.0                0.0               0.0   \n",
      "7                0.0                0.0               0.0   \n",
      "8         24183754.0           164517.0       343995257.0   \n",
      "9                0.0                0.0               0.0   \n",
      "\n",
      "   20201111_record_num  20201111_byte_out  20201111_duration  \\\n",
      "0                  9.0          1050753.0             9576.0   \n",
      "1                 69.0         14615263.0            38733.0   \n",
      "2                 19.0          5297905.0            41609.0   \n",
      "3                  0.0                0.0                0.0   \n",
      "4                  1.0             2978.0              304.0   \n",
      "5                  0.0                0.0                0.0   \n",
      "6                  0.0                0.0                0.0   \n",
      "7                  0.0                0.0                0.0   \n",
      "8                 62.0         15346591.0           143976.0   \n",
      "9                  0.0                0.0                0.0   \n",
      "\n",
      "   20201112_byte_in  20201112_record_num  20201112_byte_out  \\\n",
      "0        36334731.0                 13.0          3019101.0   \n",
      "1       307328817.0                105.0         33903726.0   \n",
      "2        59708137.0                 21.0         11872595.0   \n",
      "3          118664.0                  1.0            70938.0   \n",
      "4            3644.0                  1.0             2978.0   \n",
      "5               0.0                  0.0                0.0   \n",
      "6               0.0                  0.0                0.0   \n",
      "7               0.0                  0.0                0.0   \n",
      "8       198208858.0                 58.0         12815587.0   \n",
      "9        35718818.0                  6.0           529478.0   \n",
      "\n",
      "   20201112_duration  20201113_byte_in  20201113_record_num  \\\n",
      "0            14495.0        51480926.0                 11.0   \n",
      "1           146476.0       121043680.0                 46.0   \n",
      "2            41315.0        45630332.0                 24.0   \n",
      "3             1712.0          147194.0                  1.0   \n",
      "4              299.0            3684.0                  1.0   \n",
      "5                0.0               0.0                  0.0   \n",
      "6                0.0               0.0                  0.0   \n",
      "7                0.0               0.0                  0.0   \n",
      "8           133502.0       389018981.0                 73.0   \n",
      "9             3054.0       181959341.0                 31.0   \n",
      "\n",
      "   20201113_byte_out  20201113_duration  20201114_byte_in  \\\n",
      "0          4182869.0            13305.0      3.871232e+08   \n",
      "1         32994434.0            87987.0      0.000000e+00   \n",
      "2         15209270.0            43841.0      1.222927e+08   \n",
      "3            96266.0             1057.0      0.000000e+00   \n",
      "4             3154.0              307.0      3.644000e+03   \n",
      "5                0.0                0.0      0.000000e+00   \n",
      "6                0.0                0.0      0.000000e+00   \n",
      "7                0.0                0.0      0.000000e+00   \n",
      "8         13583412.0           139922.0      1.205400e+09   \n",
      "9         10171169.0            45822.0      4.046768e+08   \n",
      "\n",
      "   20201114_record_num  20201114_byte_out  20201114_duration  \\\n",
      "0                 19.0         12942779.0            19000.0   \n",
      "1                  0.0                0.0                0.0   \n",
      "2                 19.0          8851435.0            33081.0   \n",
      "3                  0.0                0.0                0.0   \n",
      "4                  1.0             3022.0              303.0   \n",
      "5                  0.0                0.0                0.0   \n",
      "6                  0.0                0.0                0.0   \n",
      "7                  0.0                0.0                0.0   \n",
      "8                 89.0         44617305.0           158032.0   \n",
      "9                 37.0         25703715.0            40220.0   \n",
      "\n",
      "   20201115_byte_in  20201115_record_num  20201115_byte_out  \\\n",
      "0       605125463.0                 33.0         22871065.0   \n",
      "1               0.0                  0.0                0.0   \n",
      "2        38756435.0                 26.0          7485016.0   \n",
      "3               0.0                  0.0                0.0   \n",
      "4            3393.0                  1.0             2978.0   \n",
      "5               0.0                  0.0                0.0   \n",
      "6               0.0                  0.0                0.0   \n",
      "7               0.0                  0.0                0.0   \n",
      "8         2677984.0                 28.0          1833037.0   \n",
      "9        29964173.0                 15.0          3358307.0   \n",
      "\n",
      "   20201115_duration  20201116_byte_in  20201116_record_num  \\\n",
      "0            46971.0        14851224.0                 15.0   \n",
      "1                0.0               0.0                  0.0   \n",
      "2            44877.0        84606797.0                 26.0   \n",
      "3                0.0               0.0                  0.0   \n",
      "4              298.0            3514.0                  1.0   \n",
      "5                0.0               0.0                  0.0   \n",
      "6                0.0               0.0                  0.0   \n",
      "7                0.0               0.0                  0.0   \n",
      "8            44197.0         4164177.0                 22.0   \n",
      "9            30210.0        67031155.0                 21.0   \n",
      "\n",
      "   20201116_byte_out  20201116_duration  20201117_byte_in  \\\n",
      "0          3924128.0            23855.0       190365666.0   \n",
      "1                0.0                0.0         4759510.0   \n",
      "2         15819532.0            36339.0        73664386.0   \n",
      "3                0.0                0.0               0.0   \n",
      "4             3022.0              305.0            3684.0   \n",
      "5                0.0                0.0               0.0   \n",
      "6                0.0                0.0               0.0   \n",
      "7                0.0                0.0               0.0   \n",
      "8          1166995.0            22376.0        91945011.0   \n",
      "9          7061484.0            22376.0        68512985.0   \n",
      "\n",
      "   20201117_record_num  20201117_byte_out  20201117_duration  \\\n",
      "0                 15.0          6514992.0            25895.0   \n",
      "1                  4.0          2672691.0             2544.0   \n",
      "2                 23.0         12764200.0            23729.0   \n",
      "3                  0.0                0.0                0.0   \n",
      "4                  1.0             2978.0              299.0   \n",
      "5                  0.0                0.0                0.0   \n",
      "6                  0.0                0.0                0.0   \n",
      "7                  0.0                0.0                0.0   \n",
      "8                 51.0          6940018.0            56117.0   \n",
      "9                 20.0         10522135.0            35635.0   \n",
      "\n",
      "   20201118_byte_in  20201118_record_num  20201118_byte_out  \\\n",
      "0       249182473.0                 27.0         14856829.0   \n",
      "1               0.0                  0.0                0.0   \n",
      "2        19479618.0                 10.0          3899985.0   \n",
      "3               0.0                  0.0                0.0   \n",
      "4            3644.0                  1.0             2978.0   \n",
      "5               0.0                  0.0                0.0   \n",
      "6               0.0                  0.0                0.0   \n",
      "7               0.0                  0.0                0.0   \n",
      "8         1830810.0                 18.0           952381.0   \n",
      "9       159621733.0                 36.0         12713941.0   \n",
      "\n",
      "   20201118_duration  20201119_byte_in  20201119_record_num  \\\n",
      "0            31865.0       128793595.0                 18.0   \n",
      "1                0.0               0.0                  0.0   \n",
      "2             6590.0        31010442.0                 15.0   \n",
      "3                0.0               0.0                  0.0   \n",
      "4              301.0            3718.0                  1.0   \n",
      "5                0.0               0.0                  0.0   \n",
      "6                0.0               0.0                  0.0   \n",
      "7                0.0               0.0                  0.0   \n",
      "8            10407.0         4611133.0                 31.0   \n",
      "9            54502.0        71037307.0                 25.0   \n",
      "\n",
      "   20201119_byte_out  20201119_duration  20201120_byte_in  \\\n",
      "0          5114516.0            31747.0         7498684.0   \n",
      "1                0.0                0.0               0.0   \n",
      "2          6753444.0            23557.0        28408183.0   \n",
      "3                0.0                0.0               0.0   \n",
      "4             3040.0              304.0            3483.0   \n",
      "5                0.0                0.0               0.0   \n",
      "6                0.0                0.0               0.0   \n",
      "7                0.0                0.0               0.0   \n",
      "8          3105790.0            45189.0         2876293.0   \n",
      "9          8651445.0            41934.0        43833472.0   \n",
      "\n",
      "   20201120_record_num  20201120_byte_out  20201120_duration  \\\n",
      "0                 10.0          1220072.0             5362.0   \n",
      "1                  0.0                0.0                0.0   \n",
      "2                 24.0          5513894.0            33486.0   \n",
      "3                  0.0                0.0                0.0   \n",
      "4                  1.0             2978.0              298.0   \n",
      "5                  0.0                0.0                0.0   \n",
      "6                  0.0                0.0                0.0   \n",
      "7                  0.0                0.0                0.0   \n",
      "8                 16.0          1453620.0            16914.0   \n",
      "9                 17.0          5594301.0            26978.0   \n",
      "\n",
      "   20201121_byte_in  20201121_record_num  20201121_byte_out  \\\n",
      "0       679145318.0                 38.0         24257509.0   \n",
      "1               0.0                  0.0                0.0   \n",
      "2        15742321.0                  9.0          2902181.0   \n",
      "3               0.0                  0.0                0.0   \n",
      "4            3554.0                  1.0             3510.0   \n",
      "5               0.0                  0.0                0.0   \n",
      "6               0.0                  0.0                0.0   \n",
      "7               0.0                  0.0                0.0   \n",
      "8         1542916.0                 16.0           740212.0   \n",
      "9       164300025.0                 34.0         14685991.0   \n",
      "\n",
      "   20201121_duration  20201122_byte_in  20201122_record_num  \\\n",
      "0            29966.0       380150752.0                 21.0   \n",
      "1                0.0               0.0                  0.0   \n",
      "2             8223.0        20682333.0                 10.0   \n",
      "3                0.0               0.0                  0.0   \n",
      "4              301.0            3523.0                  1.0   \n",
      "5                0.0               0.0                  0.0   \n",
      "6                0.0               0.0                  0.0   \n",
      "7                0.0               0.0                  0.0   \n",
      "8            13241.0        16785656.0                 20.0   \n",
      "9            42236.0        50112867.0                 13.0   \n",
      "\n",
      "   20201122_byte_out  20201122_duration  20201123_byte_in  \\\n",
      "0         21450961.0            18441.0       209095702.0   \n",
      "1                0.0                0.0        23692808.0   \n",
      "2          1995998.0             6020.0        32395593.0   \n",
      "3                0.0                0.0               0.0   \n",
      "4             2978.0              298.0               0.0   \n",
      "5                0.0                0.0               0.0   \n",
      "6                0.0                0.0               0.0   \n",
      "7                0.0                0.0       360597037.0   \n",
      "8          1678251.0            14943.0         3111431.0   \n",
      "9          4124936.0            23706.0         6414122.0   \n",
      "\n",
      "   20201123_record_num  20201123_byte_out  20201123_duration  \\\n",
      "0                 16.0          8257954.0            24814.0   \n",
      "1                 11.0          3199935.0            11420.0   \n",
      "2                  8.0          5718014.0             8942.0   \n",
      "3                  0.0                0.0                0.0   \n",
      "4                  0.0                0.0                0.0   \n",
      "5                  0.0                0.0                0.0   \n",
      "6                  0.0                0.0                0.0   \n",
      "7                 27.0         28980851.0            50746.0   \n",
      "8                 10.0          1889413.0            24419.0   \n",
      "9                 11.0          3442860.0            20395.0   \n",
      "\n",
      "   20201124_byte_in  20201124_record_num  20201124_byte_out  \\\n",
      "0       451712417.0                 24.0         14793413.0   \n",
      "1         2069463.0                  9.0          1013600.0   \n",
      "2       311523931.0                 16.0         13144532.0   \n",
      "3               0.0                  0.0                0.0   \n",
      "4            3424.0                  1.0             2978.0   \n",
      "5               0.0                  0.0                0.0   \n",
      "6               0.0                  0.0                0.0   \n",
      "7               0.0                  0.0                0.0   \n",
      "8         5801745.0                 26.0          3897139.0   \n",
      "9        74380467.0                 21.0         10778671.0   \n",
      "\n",
      "   20201124_duration  20201125_byte_in  20201125_record_num  \\\n",
      "0            19419.0       223257307.0                 17.0   \n",
      "1             7651.0               0.0                  0.0   \n",
      "2            14323.0               0.0                  0.0   \n",
      "3                0.0               0.0                  0.0   \n",
      "4              304.0            2364.0                  1.0   \n",
      "5                0.0               0.0                  0.0   \n",
      "6                0.0               0.0                  0.0   \n",
      "7                0.0               0.0                  0.0   \n",
      "8            21412.0       129542972.0                 33.0   \n",
      "9            40569.0        10219815.0                 16.0   \n",
      "\n",
      "   20201125_byte_out  20201125_duration  20201126_byte_in  \\\n",
      "0          9983053.0            23872.0        84636902.0   \n",
      "1                0.0                0.0               0.0   \n",
      "2                0.0                0.0        40083649.0   \n",
      "3                0.0                0.0               0.0   \n",
      "4             1426.0              173.0            3684.0   \n",
      "5                0.0                0.0               0.0   \n",
      "6                0.0                0.0               0.0   \n",
      "7                0.0                0.0               0.0   \n",
      "8         20419021.0            49506.0       246906223.0   \n",
      "9          2108117.0            24629.0        60733185.0   \n",
      "\n",
      "   20201126_record_num  20201126_byte_out  20201126_duration  \\\n",
      "0                 10.0          5482071.0            11768.0   \n",
      "1                  0.0                0.0                0.0   \n",
      "2                  7.0          1981963.0             3331.0   \n",
      "3                  0.0                0.0                0.0   \n",
      "4                  1.0             3403.0              298.0   \n",
      "5                  0.0                0.0                0.0   \n",
      "6                  0.0                0.0                0.0   \n",
      "7                  0.0                0.0                0.0   \n",
      "8                 29.0          6942610.0            41645.0   \n",
      "9                 17.0          6552395.0            43226.0   \n",
      "\n",
      "   20201127_byte_in  20201127_record_num  20201127_byte_out  \\\n",
      "0        41662224.0                 12.0          4460705.0   \n",
      "1           97198.0                  2.0                0.0   \n",
      "2        15579726.0                  7.0          2011797.0   \n",
      "3               0.0                  0.0                0.0   \n",
      "4            3582.0                  1.0             2978.0   \n",
      "5               0.0                  0.0                0.0   \n",
      "6               0.0                  0.0                0.0   \n",
      "7               0.0                  0.0                0.0   \n",
      "8       325426014.0                 40.0         10490261.0   \n",
      "9        55257883.0                 15.0          5364037.0   \n",
      "\n",
      "   20201127_duration  20201128_byte_in  20201128_record_num  \\\n",
      "0            12536.0       204591438.0                 11.0   \n",
      "1             2975.0               0.0                  0.0   \n",
      "2             7072.0         9332750.0                  3.0   \n",
      "3                0.0               0.0                  0.0   \n",
      "4              300.0            3254.0                  1.0   \n",
      "5                0.0               0.0                  0.0   \n",
      "6                0.0               0.0                  0.0   \n",
      "7                0.0       187448469.0                 23.0   \n",
      "8            45473.0        24223843.0                 20.0   \n",
      "9            34032.0       376175802.0                 41.0   \n",
      "\n",
      "   20201128_byte_out  20201128_duration  20201129_byte_in  \\\n",
      "0          7835888.0            10543.0        41434830.0   \n",
      "1                0.0                0.0               0.0   \n",
      "2          3509401.0             5254.0        25677715.0   \n",
      "3                0.0                0.0               0.0   \n",
      "4             2978.0              299.0            3223.0   \n",
      "5                0.0                0.0               0.0   \n",
      "6                0.0                0.0               0.0   \n",
      "7         11126620.0            48633.0       659077522.0   \n",
      "8          2046289.0            18473.0         6796491.0   \n",
      "9         23991526.0            67628.0       116138133.0   \n",
      "\n",
      "   20201129_record_num  20201129_byte_out  20201129_duration  \\\n",
      "0                 13.0          2589122.0            12901.0   \n",
      "1                  0.0                0.0                0.0   \n",
      "2                  9.0          6057393.0            16257.0   \n",
      "3                  0.0                0.0                0.0   \n",
      "4                  1.0             3470.0              303.0   \n",
      "5                  0.0                0.0                0.0   \n",
      "6                  0.0                0.0                0.0   \n",
      "7                 35.0         23486996.0            48009.0   \n",
      "8                 27.0          3344097.0            78172.0   \n",
      "9                 19.0          6651333.0            39399.0   \n",
      "\n",
      "   20201130_byte_in  20201130_record_num  20201130_byte_out  \\\n",
      "0        48150942.0                  8.0          4110155.0   \n",
      "1               0.0                  0.0                0.0   \n",
      "2        14009182.0                  9.0          2142145.0   \n",
      "3               0.0                  0.0                0.0   \n",
      "4            3684.0                  1.0             2978.0   \n",
      "5               0.0                  0.0                0.0   \n",
      "6               0.0                  0.0                0.0   \n",
      "7       734063590.0                 19.0         16467676.0   \n",
      "8        18468091.0                 28.0          2363809.0   \n",
      "9       155187460.0                 21.0         11720070.0   \n",
      "\n",
      "   20201130_duration  all_app_use_times  202011_byte_in_list_entropy  \\\n",
      "0            12826.0            20997.0                     2.233800   \n",
      "1                0.0             8768.0                     0.420026   \n",
      "2            17266.0            22890.0                     1.249653   \n",
      "3                0.0              951.0                     0.420026   \n",
      "4              298.0                0.0                     1.483352   \n",
      "5                0.0                0.0                     0.000000   \n",
      "6                0.0                0.0                     0.000000   \n",
      "7             8841.0             9715.0                     0.970344   \n",
      "8            31279.0            53871.0                     2.075075   \n",
      "9            39358.0            22616.0                     1.751281   \n",
      "\n",
      "   202011_byte_out_list_entropy  202011_duration_list_entropy  \\\n",
      "0                      2.527421                      2.440551   \n",
      "1                      0.833176                      0.766510   \n",
      "2                      2.481194                      2.857426   \n",
      "3                      0.627492                      0.833176   \n",
      "4                      1.249653                      0.627492   \n",
      "5                      0.000000                      0.000000   \n",
      "6                      0.000000                      0.000000   \n",
      "7                      0.970344                      0.970344   \n",
      "8                      2.021421                      2.251933   \n",
      "9                      2.011290                      2.127421   \n",
      "\n",
      "   202011_record_num_list_entropy  202011_byte_in_list_anomaly  \\\n",
      "0                        2.743762                          2.0   \n",
      "1                        0.833176                          7.0   \n",
      "2                        2.827539                          3.0   \n",
      "3                        0.833176                          6.0   \n",
      "4                        0.000000                          4.0   \n",
      "5                        0.000000                          0.0   \n",
      "6                        0.000000                          0.0   \n",
      "7                        0.970344                          5.0   \n",
      "8                        2.547861                          1.0   \n",
      "9                        2.654773                          2.0   \n",
      "\n",
      "   202011_byte_out_list_anomaly  202011_duration_list_anomaly  \\\n",
      "0                           0.0                           2.0   \n",
      "1                           7.0                           6.0   \n",
      "2                           0.0                           0.0   \n",
      "3                           6.0                           6.0   \n",
      "4                           8.0                           3.0   \n",
      "5                           0.0                           0.0   \n",
      "6                           0.0                           0.0   \n",
      "7                           5.0                           5.0   \n",
      "8                           0.0                           0.0   \n",
      "9                           1.0                           0.0   \n",
      "\n",
      "   202011_record_num_list_anomaly  \n",
      "0                             0.0  \n",
      "1                             6.0  \n",
      "2                             0.0  \n",
      "3                             6.0  \n",
      "4                             1.0  \n",
      "5                             0.0  \n",
      "6                             0.0  \n",
      "7                             5.0  \n",
      "8                             0.0  \n",
      "9                             0.0  \n",
      "**********************************\n"
     ]
    }
   ],
   "source": [
    "ic_37w = csv2df(filename=\"ic_37w_2011\", path=\"./\")\n",
    "check_format(ic_37w)\n",
    "check_detail(ic_37w)\n",
    "\n",
    "final_ic_pd_df = ic_37w.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-disorder",
   "metadata": {},
   "source": [
    "# Churn Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-laundry",
   "metadata": {},
   "source": [
    "## Ablation-Feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "loving-choir",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_ic_pd_df.shape:(379064, 337)\n"
     ]
    }
   ],
   "source": [
    "# check_all(final_ic_pd_df, 2)\n",
    "print(f\"final_ic_pd_df.shape:{final_ic_pd_df.shape}\")\n",
    "\n",
    "'''\n",
    "Ablation-Feat\n",
    "'''\n",
    "ablation_feat = []\n",
    "magic_feat = ['hlwk_offer_aim_flow_cm_round_target', 'rel_amount_round_target']\n",
    "final_ic_pd_df.drop(columns=ablation_feat, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-customer",
   "metadata": {},
   "source": [
    "## Spilt TrainSet&TestSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "detected-brick",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ic_pd_df.shape:(300000, 337)\n",
      "test_ic_pd_df.shape:(75000, 337)\n",
      "dataset:positive sample number:29329\n",
      "dataset:negative sample number:349735\n",
      "dataset:neg-pos ratio:11.9245\n",
      "train_set:positive sample number:23112\n",
      "train_set:negative sample number:276888\n",
      "train_set:neg-pos ratio:11.9803\n",
      "test_set:positive sample number:5939\n",
      "test_set:negative sample number:69061\n",
      "test_set:neg-pos ratio:11.6284\n"
     ]
    }
   ],
   "source": [
    "# sample_number = 90000\n",
    "sample_number = 100000\n",
    "sample_number = 375000\n",
    "# sample_number = 500000\n",
    "# sample_number = 900000\n",
    "# sample_number = 1000000\n",
    "# sample_number = 1900000\n",
    "# sample_number = 2000000\n",
    "if final_ic_pd_df.shape[0] < sample_number:\n",
    "    # 1' splited by city\n",
    "    train_ic_pd_df = final_ic_pd_df.loc[final_ic_pd_df['lan_id'] != 11]\n",
    "    print(f'train_ic_pd_df.shape:{train_ic_pd_df.shape}')\n",
    "    test_ic_pd_df = final_ic_pd_df.loc[final_ic_pd_df['lan_id'] == 11]\n",
    "    print(f'test_ic_pd_df.shape:{test_ic_pd_df.shape}')\n",
    "else:\n",
    "    # 2' splited by number\n",
    "    sample_ic_pd_df = final_ic_pd_df.sample(n = sample_number, random_state = 42)\n",
    "    # print('check_all(sample_ic_pd_df):')\n",
    "    # check_all(sample_ic_pd_df)\n",
    "    train_number = int(sample_number//5*4)\n",
    "    # print(type(train_number))\n",
    "    # print(train_number)\n",
    "    train_ic_pd_df  = sample_ic_pd_df.iloc[:train_number]\n",
    "    print(f'train_ic_pd_df.shape:{train_ic_pd_df.shape}')\n",
    "    test_ic_pd_df = sample_ic_pd_df.iloc[train_number:]\n",
    "    print(f'test_ic_pd_df.shape:{test_ic_pd_df.shape}')\n",
    "\n",
    "check_label_distribution(final_ic_pd_df, \"dataset\", 'halt_2012')\n",
    "check_label_distribution(train_ic_pd_df, \"train_set\", 'halt_2012')\n",
    "check_label_distribution(test_ic_pd_df, \"test_set\", 'halt_2012')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-chart",
   "metadata": {},
   "source": [
    "## Seq Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "computational-ambassador",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature.shape:(300000, 240)\n",
      "label.shape:(300000,)\n",
      "seq_label.shape:(300000,)\n",
      "        halt_2012\n",
      "239162        0.0\n",
      "130315        0.0\n",
      "208641        1.0\n",
      "114591        0.0\n",
      "274994        0.0\n",
      "81444         0.0\n",
      "46971         0.0\n",
      "196658        0.0\n",
      "55763         0.0\n",
      "36596         0.0\n",
      "**********************************\n",
      "seq_X_train.shape:(225000, 240)\n",
      "seq_y_train.shape:(225000,)\n",
      "seq_X_valid.shape:(75000, 240)\n",
      "seq_y_valid.shape:(75000,)\n",
      "*******start of function:data_transformation mode:sequence*******\n",
      "X_train_np.shape:(225000, 240)\n",
      "X_valid_np.shape:(75000, 240)\n",
      "y_train_np.shape:(225000, 1)\n",
      "y_valid_np.shape:(75000, 1)\n",
      "start transforming sequential data\n",
      "before X_train_np.shape:(225000, 240)\n",
      "before X_valid_np.shape:(75000, 240)\n",
      "np_arr.shape:(225000, 240)\n",
      "timesteps:30\n",
      "np_arr.shape[1]//timesteps:8\n",
      "np_arr.shape:(75000, 240)\n",
      "timesteps:30\n",
      "np_arr.shape[1]//timesteps:8\n",
      "after X_train_np.shape:(225000, 30, 8)\n",
      "after X_valid_np.shape:(75000, 30, 8)\n",
      "X_train_np[:,:,0]:[[ 0.05798922 -0.31872205  0.28082014 ... -0.28736908 -0.28765986\n",
      "  -0.26776956]\n",
      " [ 0.49489402  1.66137802  0.58057758 ...  0.41862808  0.41095064\n",
      "  -0.26776956]\n",
      " [-0.68964251 -0.65396829  0.24264017 ... -0.28736908 -0.28765986\n",
      "   1.19199546]\n",
      " ...\n",
      " [ 1.16776894  1.57310333 -0.66639815 ... -0.28736908 -0.28765986\n",
      "  -0.26776956]\n",
      " [-0.46895361 -0.43428234 -0.0484077  ... -0.28736908 -0.28765986\n",
      "   0.46211295]\n",
      " [-0.2354797  -0.43019059 -0.2803118  ... -0.28736908 -0.28765986\n",
      "  -0.26776956]]\n",
      "*******end of function:data_transformation mode:sequence*******\n",
      "feature.shape:(75000, 240)\n",
      "label.shape:(75000,)\n",
      "test_seq_label.shape:(75000,)\n",
      "        halt_2012\n",
      "247088        0.0\n",
      "89599         0.0\n",
      "187001        0.0\n",
      "4312          0.0\n",
      "10897         0.0\n",
      "314391        1.0\n",
      "72442         0.0\n",
      "280629        0.0\n",
      "7257          0.0\n",
      "78565         0.0\n",
      "**********************************\n",
      "seq_X_test.shape:(75000, 240)\n",
      "seq_y_test.shape:(75000,)\n",
      "*******start of function:data_transformation mode:sequence*******\n",
      "X_train_np.shape:(225000, 240)\n",
      "X_valid_np.shape:(75000, 240)\n",
      "y_train_np.shape:(225000, 1)\n",
      "y_valid_np.shape:(75000, 1)\n",
      "start transforming sequential data\n",
      "before X_train_np.shape:(225000, 240)\n",
      "before X_valid_np.shape:(75000, 240)\n",
      "np_arr.shape:(225000, 240)\n",
      "timesteps:30\n",
      "np_arr.shape[1]//timesteps:8\n",
      "np_arr.shape:(75000, 240)\n",
      "timesteps:30\n",
      "np_arr.shape[1]//timesteps:8\n",
      "after X_train_np.shape:(225000, 30, 8)\n",
      "after X_valid_np.shape:(75000, 30, 8)\n",
      "X_train_np[:,:,0]:[[ 0.05798922 -0.31872205  0.28082014 ... -0.28736908 -0.28765986\n",
      "  -0.26776956]\n",
      " [ 0.49489402  1.66137802  0.58057758 ...  0.41862808  0.41095064\n",
      "  -0.26776956]\n",
      " [-0.68964251 -0.65396829  0.24264017 ... -0.28736908 -0.28765986\n",
      "   1.19199546]\n",
      " ...\n",
      " [ 1.16776894  1.57310333 -0.66639815 ... -0.28736908 -0.28765986\n",
      "  -0.26776956]\n",
      " [-0.46895361 -0.43428234 -0.0484077  ... -0.28736908 -0.28765986\n",
      "   0.46211295]\n",
      " [-0.2354797  -0.43019059 -0.2803118  ... -0.28736908 -0.28765986\n",
      "  -0.26776956]]\n",
      "*******end of function:data_transformation mode:sequence*******\n"
     ]
    }
   ],
   "source": [
    "month = '202011'\n",
    "month_days = 30\n",
    "traffic_feat_list = period_feat('byte_in',month,month_days)\n",
    "traffic_feat_list.extend(period_feat('byte_out',month,month_days))\n",
    "traffic_feat_list.extend(period_feat('duration',month,month_days))\n",
    "traffic_feat_list.extend(period_feat('record_num',month,month_days))\n",
    "\n",
    "anomaly_feat_list = period_feat('byte_in_anomaly',month,month_days)\n",
    "\n",
    "cdr_feat_list = period_feat('calling_duration',month,month_days)\n",
    "cdr_feat_list.extend(period_feat('calling_record_num',month,month_days))\n",
    "cdr_feat_list.extend(period_feat('called_duration',month,month_days))\n",
    "cdr_feat_list.extend(period_feat('called_record_num',month,month_days))\n",
    "\n",
    "sequence_feat_list = []\n",
    "sequence_feat_list.extend(traffic_feat_list)\n",
    "sequence_feat_list.extend(cdr_feat_list)\n",
    "# sequence_feat_list_1.extend(anomaly_feat_list)\n",
    "\n",
    "# for feat in sequence_feat_list_1:\n",
    "#     sequence_feat_list.append(\"1_\"+feat)\n",
    "    \n",
    "seq_feature,seq_label = data_feat_selecter(train_ic_pd_df.copy(),sequence_feat_list.copy(), \"halt_2012\")\n",
    "print(f'seq_label.shape:{seq_label.shape}')\n",
    "check_detail(seq_label)\n",
    "\n",
    "seq_X_train, seq_X_valid, seq_y_train, seq_y_valid = train_test_split(seq_feature,seq_label,random_state=random_state,test_size=0.25)\n",
    "print(f'seq_X_train.shape:{seq_X_train.shape}')\n",
    "print(f'seq_y_train.shape:{seq_y_train.shape}')\n",
    "print(f'seq_X_valid.shape:{seq_X_valid.shape}')\n",
    "print(f'seq_y_valid.shape:{seq_y_valid.shape}')\n",
    "# check_detail(seq_y_train)\n",
    "seq_dataset = (seq_X_train, seq_y_train, seq_X_valid, seq_y_valid)\n",
    "seq_dataset_df, seq_dataset_np, seq_dataset_tensor = data_transformation(seq_dataset,'sequence')\n",
    "\n",
    "test_seq_feature,test_seq_label = data_feat_selecter(test_ic_pd_df.copy(),sequence_feat_list.copy(), \"halt_2012\")\n",
    "print(f'test_seq_label.shape:{test_seq_label.shape}')\n",
    "check_detail(test_seq_label)\n",
    "\n",
    "seq_X_test, seq_y_test = test_seq_feature,test_seq_label\n",
    "# seq_X_test, _, seq_y_test, _ = train_test_split(seq_feature,seq_label,random_state=random_state,test_size=0)\n",
    "\n",
    "print(f'seq_X_test.shape:{seq_X_test.shape}')\n",
    "print(f'seq_y_test.shape:{seq_y_test.shape}')\n",
    "# check_detail(seq_y_test)\n",
    "test_seq_dataset = (seq_X_train, seq_y_train, seq_X_test, seq_y_test)\n",
    "test_seq_dataset_df, test_seq_dataset_np, test_seq_dataset_tensor = data_transformation(test_seq_dataset,'sequence')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-singing",
   "metadata": {},
   "source": [
    "## Data Spilt&Transmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fluid-seating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_feat_filter:\n",
      "label_feat:halt_2012\n",
      "feature.shape:(300000, 336)\n",
      "label.shape:(300000,)\n",
      "        halt_2012\n",
      "239162        0.0\n",
      "130315        0.0\n",
      "208641        1.0\n",
      "114591        0.0\n",
      "274994        0.0\n",
      "81444         0.0\n",
      "46971         0.0\n",
      "196658        0.0\n",
      "55763         0.0\n",
      "36596         0.0\n",
      "**********************************\n",
      "X_train.shape:(225000, 336)\n",
      "y_train.shape:(225000,)\n",
      "X_valid.shape:(75000, 336)\n",
      "y_valid.shape:(75000,)\n",
      "*******start of function:data_transformation mode:plain*******\n",
      "X_train_np.shape:(225000, 336)\n",
      "X_valid_np.shape:(75000, 336)\n",
      "y_train_np.shape:(225000, 1)\n",
      "y_valid_np.shape:(75000, 1)\n",
      "scaling:\n",
      "X_train_df.shape:(225000, 336)\n",
      "X_valid_df.shape:(75000, 336)\n",
      "*******end of function:data_transformation mode:plain*******\n",
      "data_feat_filter:\n",
      "label_feat:halt_2012\n",
      "feature.shape:(75000, 336)\n",
      "label.shape:(75000,)\n",
      "        halt_2012\n",
      "247088        0.0\n",
      "89599         0.0\n",
      "187001        0.0\n",
      "4312          0.0\n",
      "10897         0.0\n",
      "314391        1.0\n",
      "72442         0.0\n",
      "280629        0.0\n",
      "7257          0.0\n",
      "78565         0.0\n",
      "**********************************\n",
      "X_test.shape:(75000, 336)\n",
      "y_test.shape:(75000,)\n",
      "*******start of function:data_transformation mode:plain*******\n",
      "X_train_np.shape:(225000, 336)\n",
      "X_valid_np.shape:(75000, 336)\n",
      "y_train_np.shape:(225000, 1)\n",
      "y_valid_np.shape:(75000, 1)\n",
      "scaling:\n",
      "X_train_df.shape:(225000, 336)\n",
      "X_valid_df.shape:(75000, 336)\n",
      "*******end of function:data_transformation mode:plain*******\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Get TrainSet\n",
    "'''\n",
    "\n",
    "feature,label = data_feat_filter(train_ic_pd_df.copy(),[], \"halt_2012\")\n",
    "# X_train,X_valid,y_train,y_valid = train_test_split(feature,label,random_state=random_state,test_size=0.2)\n",
    "X_train,X_valid,y_train,y_valid = train_test_split(feature,label,random_state=random_state,test_size=0.25)\n",
    "# X_train,X_valid,y_train,y_valid = train_test_split(feature,label,random_state=1997,test_size=0.2)\n",
    "# X_train,X_valid,y_train,y_valid = train_test_split(feature,label,random_state=2022,test_size=0.2)\n",
    "# X_train,X_valid,y_train,y_valid = train_test_split(feature,label,random_state=25,test_size=0.2)\n",
    "# X_train,X_valid,y_train,y_valid = train_test_split(feature,label,random_state=100,test_size=0.2)\n",
    "print(f'X_train.shape:{X_train.shape}')\n",
    "print(f'y_train.shape:{y_train.shape}')\n",
    "print(f'X_valid.shape:{X_valid.shape}')\n",
    "print(f'y_valid.shape:{y_valid.shape}')\n",
    "dataset = (X_train,y_train,X_valid,y_valid)\n",
    "# dataset_df, dataset_np, dataset_tensor = data_transformation(dataset,'all')\n",
    "dataset_df, dataset_np, dataset_tensor = data_transformation(dataset,'plain')\n",
    "# dataset_df, dataset_np, dataset_tensor = data_transformation(dataset,'fig')\n",
    "\n",
    "'''\n",
    "Get TestSet\n",
    "'''\n",
    "# X_test,y_test = data_feat_filter(test_part_ic_pd_df.copy(),[], \"halt_2101\")\n",
    "X_test,y_test = data_feat_filter(test_ic_pd_df.copy(),[], \"halt_2012\")\n",
    "print(f'X_test.shape:{X_test.shape}')\n",
    "print(f'y_test.shape:{y_test.shape}')\n",
    "test_dataset = (X_train,y_train,X_test,y_test)\n",
    "# test_dataset_df, test_dataset_np, test_dataset_tensor = data_transformation(dataset,'all')\n",
    "test_dataset_df, test_dataset_np, test_dataset_tensor = data_transformation(test_dataset,'plain')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-atlas",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-shelter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-directory",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-forge",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-details",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-identity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-cartridge",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-fighter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-southwest",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-feeding",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-morris",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-stream",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-rainbow",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-neighbor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-speaker",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-sculpture",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-heating",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-vector",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decent-tracy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-independence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-delta",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-wedding",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-nothing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-index",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-calendar",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-converter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "right-bradford",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-selection",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-spectacular",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-party",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-emphasis",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-property",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-bread",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-romantic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recreational-increase",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-smith",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-roulette",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-duration",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-found",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-ability",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-hundred",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-earth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-switch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-deviation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-gateway",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-longitude",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-destruction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-iceland",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-foundation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-flour",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-halifax",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
